<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>如何新建一篇文章</title>
    <url>/2020/10/24/article-writing/</url>
    <content><![CDATA[<p>1.在博客文件夹目录下鼠标右键，点击Git Bash Here</p>
<p>2.输入代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new article</span><br></pre></td></tr></table></figure>
<p>回车新建文章，生成的.md文件在/source/_post/目录下</p>
<p>3.</p>
<p>清除之前的页面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure>
<p>生成新的页面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo generate</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure>
<p>部署新的页面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo deploy</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure>
<p>打开本地服务器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure>
<p>4.文章Front Matter</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>title</td>
<td>文章的标题</td>
</tr>
<tr>
<td>date</td>
<td>文章的日期</td>
</tr>
<tr>
<td>categories</td>
<td>文章的所属类别，只有一个，填写多个时默认第一个</td>
</tr>
<tr>
<td>tags</td>
<td>标签，可以有多个</td>
</tr>
<tr>
<td>description</td>
<td>首页缩略显示内容</td>
</tr>
<tr>
<td>comments</td>
<td>是否支持评论</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
  <entry>
    <title>Tesorflow实战：简单mnist分类任务</title>
    <url>/2020/10/30/tensorflow-coding-mnist-simple-classfication/</url>
    <content><![CDATA[<h1 id="Original-Codes"><a href="#Original-Codes" class="headerlink" title="Original Codes"></a>Original Codes</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist &#x3D; input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot&#x3D;True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size &#x3D; 100</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch &#x3D; mnist.train.num_examples &#x2F;&#x2F; batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义两个placeholder</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32,[None,10])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#创建一个简单的神经网络</span><br><span class="line">w &#x3D; tf.Variable(tf.zeros([784,10]))</span><br><span class="line">b &#x3D; tf.Variable(tf.zeros([10]))</span><br><span class="line">prediction &#x3D; tf.nn.softmax(tf.matmul(x,w) + b)</span><br><span class="line"></span><br><span class="line">#二次代价函数</span><br><span class="line">loss &#x3D; tf.reduce_mean(tf.square(y - prediction))</span><br><span class="line"></span><br><span class="line">#使用梯度下降法</span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#初始化变量</span><br><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#定义求准确率的方法,结果存在一个布尔型列表中</span><br><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))</span><br><span class="line">#求准确率</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    for epoch in range(21):</span><br><span class="line">        for batch in range(n_batch):</span><br><span class="line">            batch_xs,batch_ys &#x3D; mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict&#x3D;&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">            </span><br><span class="line">        acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line">        print(&quot;Iter&quot; + str(epoch) + &quot;,Testing Accuracy&quot; + str(acc))</span><br></pre></td></tr></table></figure>
<p>执行结果：20次迭代后，准确率为0.9138</p>
<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><h2 id="Note1-tf-argmax"><a href="#Note1-tf-argmax" class="headerlink" title="Note1    tf.argmax()"></a>Note1    tf.argmax()</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.argmax(input,axis)</span><br></pre></td></tr></table></figure>
<p>含义：根据axis取值的不同返回每行或者每列最大值的索引。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>input</td>
<td>输入的array</td>
</tr>
<tr>
<td>axis</td>
<td>axis=0，将每一列最大元素的所在索引记录下来，最后输出每一列最大元素所在的索引数组。axis=1，将每一行最大元素所在的索引记录下来，最后返回每一行最大元素所在的索引数组。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Note2-tf-cast"><a href="#Note2-tf-cast" class="headerlink" title="Note2    tf.cast()"></a>Note2    tf.cast()</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.cast(x, dtype, name&#x3D;None)</span><br></pre></td></tr></table></figure>
<p>含义：tf.cast()函数的作用是执行 tensorflow 中张量数据类型转换。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>x</td>
<td>待转换的数据（张量）。</td>
</tr>
<tr>
<td>dtype</td>
<td>目标数据类型。</td>
</tr>
<tr>
<td>name</td>
<td>可选参数，定义操作的名称。</td>
</tr>
</tbody>
</table>
</div>
<p>Tensorflow中的数据类型列表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据类型</th>
<th>Python类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>DT_FLOAT</td>
<td>tf.float32</td>
<td>32位浮点数</td>
</tr>
<tr>
<td>DT_DOUBLE</td>
<td>tf.float64</td>
<td>64位浮点数</td>
</tr>
<tr>
<td>DT_INT64</td>
<td>tf.int64</td>
<td>64位有符号整型</td>
</tr>
<tr>
<td>DT_INT32</td>
<td>tf.int32</td>
<td>32位有符号整型</td>
</tr>
<tr>
<td>DT_INT16</td>
<td>tf.int16</td>
<td>16位有符号整型</td>
</tr>
<tr>
<td>DT_INT8</td>
<td>tf.int8</td>
<td>8位有符号整型</td>
</tr>
<tr>
<td>DT_UINT8</td>
<td>tf.uint8</td>
<td>8位无符号整型</td>
</tr>
<tr>
<td>DT_STRING</td>
<td>tf.string</td>
<td>可变长度的字节数组.每一个张量元素都是一个字节数组.</td>
</tr>
<tr>
<td>DT_BOOL</td>
<td>tf.bool</td>
<td>布尔型</td>
</tr>
<tr>
<td>DT_COMPLEX64</td>
<td>tf.complex64</td>
<td>由两个32位浮点数组成的复数:实数和虚数.</td>
</tr>
<tr>
<td>DT_QINT32</td>
<td>tf.qint32</td>
<td>用于量化Ops的32位有符号整型.</td>
</tr>
<tr>
<td>DT_QINT8</td>
<td>tf.qint8</td>
<td>用于量化Ops的8位有符号整型.</td>
</tr>
<tr>
<td>DT_QUINT8</td>
<td>tf.quint8</td>
<td>用于量化Ops的8位无符号整型.</td>
</tr>
</tbody>
</table>
</div>
<h1 id="超参数调整"><a href="#超参数调整" class="headerlink" title="超参数调整"></a>超参数调整</h1><h2 id="只有输入层和输出层"><a href="#只有输入层和输出层" class="headerlink" title="只有输入层和输出层"></a>只有输入层和输出层</h2><div class="table-container">
<table>
<thead>
<tr>
<th>batch_size</th>
<th>learning_rate</th>
<th>accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>32</td>
<td>0.2</td>
<td>0.9236</td>
</tr>
<tr>
<td>64</td>
<td>0.2</td>
<td>0.9185</td>
</tr>
<tr>
<td>128</td>
<td>0.2</td>
<td>0.9107</td>
</tr>
<tr>
<td>16</td>
<td>0.2</td>
<td>0.9272</td>
</tr>
<tr>
<td>8</td>
<td>0.2</td>
<td>0.9310</td>
</tr>
<tr>
<td>4</td>
<td>0.2</td>
<td>0.9308</td>
</tr>
<tr>
<td>8</td>
<td>0.1</td>
<td>0.9301</td>
</tr>
</tbody>
</table>
</div>
<h2 id="增加隐藏层"><a href="#增加隐藏层" class="headerlink" title="增加隐藏层"></a>增加隐藏层</h2><div class="table-container">
<table>
<thead>
<tr>
<th>隐藏层数</th>
<th>隐藏层神经元数</th>
<th>激活函数</th>
<th>batch_size</th>
<th>w</th>
<th>learning_rate</th>
<th>accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>10</td>
<td>tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.937</td>
</tr>
<tr>
<td>1</td>
<td>20</td>
<td>tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.853</td>
</tr>
<tr>
<td>1</td>
<td>30</td>
<td>tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.9638</td>
</tr>
<tr>
<td>1</td>
<td>30</td>
<td>tanh</td>
<td>32</td>
<td>0</td>
<td>0.3</td>
<td>0.961</td>
</tr>
<tr>
<td>1</td>
<td>30</td>
<td>tanh</td>
<td>48</td>
<td>0</td>
<td>0.3</td>
<td>0.962</td>
</tr>
<tr>
<td>1</td>
<td>30</td>
<td>tanh</td>
<td>128</td>
<td>0</td>
<td>0.3</td>
<td>0.962</td>
</tr>
<tr>
<td>1</td>
<td>40</td>
<td>tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.967</td>
</tr>
<tr>
<td>1</td>
<td>50</td>
<td>tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.798</td>
</tr>
<tr>
<td>1</td>
<td>32</td>
<td>tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.962</td>
</tr>
<tr>
<td>2</td>
<td>10 4</td>
<td>tanh tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.868</td>
</tr>
<tr>
<td>2</td>
<td>20 10</td>
<td>tanh tanh</td>
<td>64</td>
<td>0</td>
<td>0.6</td>
<td>0.905</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Q1:weight初试化为0，网络由于对称性实际上可以再压缩</strong></p>
]]></content>
      <categories>
        <category>Tensorflow-Coding-Notes</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow实战：非线性逻辑回归</title>
    <url>/2020/10/28/tensorflow-coding-nonlinear-logic-regression/</url>
    <content><![CDATA[<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><h2 id="Note1-linspace-函数功能"><a href="#Note1-linspace-函数功能" class="headerlink" title="Note1    linspace()函数功能"></a>Note1    linspace()函数功能</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x_data &#x3D; np.linspace(-0.5,0.5,10)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">np.linspace(start, stop, num&#x3D;50, endpoint&#x3D;True, retstep&#x3D;False, dtype&#x3D;None)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>start</td>
<td>队列的开始值</td>
</tr>
<tr>
<td>stop</td>
<td>队列的结束值</td>
</tr>
<tr>
<td>num</td>
<td>要生成的样本数，非负数，默认是50</td>
</tr>
<tr>
<td>endpoint</td>
<td>若为True，“stop”是最后的样本；否则“stop”将不会被包含。默认为True</td>
</tr>
<tr>
<td>retstop</td>
<td>若为False，返回等差数列；否则返回array([samples, step])。默认为False</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Note2-newaxis-功能"><a href="#Note2-newaxis-功能" class="headerlink" title="Note2    [:,newaxis]功能"></a>Note2    [:,newaxis]功能</h2><p>执行代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x_data &#x3D; np.linspace(-0.5,0.5,10)</span><br><span class="line">print(x_data.shape)</span><br><span class="line">x_data &#x3D; x_data[:,newaxis]</span><br><span class="line">print(x_data.shape)</span><br></pre></td></tr></table></figure>
<p>输出结果为</p>
<p>(10,)</p>
<p>(10,1)</p>
<p>x_data[:,newaxis]能将原来的数据扩充一个维度。</p>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2><h2 id="Note3-正态分布生成函数"><a href="#Note3-正态分布生成函数" class="headerlink" title="Note3    正态分布生成函数"></a>Note3    正态分布生成函数</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">np.random.normal(loc&#x3D;0.0,scale&#x3D;1.0,size&#x3D;None)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>loc</td>
<td>float</td>
<td>正态分布的均值</td>
</tr>
<tr>
<td>scale</td>
<td>float</td>
<td>正态分布的标准差，对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高</td>
</tr>
<tr>
<td>size</td>
<td>int or tuple of ints</td>
<td>输出的shape，默认为None，只输出一个值</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Note4-y-np-square-x"><a href="#Note4-y-np-square-x" class="headerlink" title="Note4    y = np.square(x)"></a>Note4    y = np.square(x)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y &#x3D; np.square(x)</span><br></pre></td></tr></table></figure>
<p>返回的y与x有相同的shape，其中y的每个元素是x每个元素的平方。</p>
<h2 id="Note5-Tensorflow的占位符"><a href="#Note5-Tensorflow的占位符" class="headerlink" title="Note5 Tensorflow的占位符"></a>Note5 Tensorflow的占位符</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,1])</span><br></pre></td></tr></table></figure>
<p>定义了一个32位浮点型的占位符，shape为[None,1]，行数没有定义，列数定义为1</p>
<p>执行时使用feed_dict{}以字典的形式传入数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sess.run(train_step,feed_dict&#x3D;&#123;x:x_data,y:y_data&#125;)</span><br></pre></td></tr></table></figure>
<p>前面定义了operation，我们运行的时候seess.run()最后一个operation就可以，前面的会自动执行（我的推断）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.placeholder(dtype,shape&#x3D;None,name&#x3D;None)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>dtype</td>
<td>数据类型。常用的是tf.float32,tf.float64等数值类型</td>
</tr>
<tr>
<td>shape</td>
<td>数据形状。默认是None，就是一维值，也可以是多维（比如[2,3], [None, 3]表示列是3，行不定）</td>
</tr>
<tr>
<td>name</td>
<td>名称</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Codes"><a href="#Codes" class="headerlink" title="Codes"></a>Codes</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#使用numpy生成200个随机点</span><br><span class="line">x_data &#x3D; np.linspace(-0.5,0.5,200)[:,np.newaxis]</span><br><span class="line">noise &#x3D; np.random.normal(0,0.02,x_data.shape)</span><br><span class="line">y_data &#x3D; np.square(x_data) + noise</span><br><span class="line"></span><br><span class="line">#定义两个placeholder</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,1])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32,[None,1])</span><br><span class="line"></span><br><span class="line">#定义神经网络中间层</span><br><span class="line">Weights_L1 &#x3D; tf.Variable(tf.random_normal([1,10]))</span><br><span class="line">biases_L1 &#x3D; tf.Variable(tf.zeros([1,10]))</span><br><span class="line">Wx_plus_b_L1 &#x3D; tf.matmul(x,Weights_L1) + biases_L1</span><br><span class="line">L1 &#x3D; tf.nn.tanh(Wx_plus_b_L1)</span><br><span class="line"></span><br><span class="line">#定义神经网络输出层</span><br><span class="line">Weights_L2 &#x3D; tf.Variable(tf.random_normal([10,1]))</span><br><span class="line">biases_L2 &#x3D; tf.Variable(tf.zeros([1,1]))</span><br><span class="line">Wx_plus_b_L2 &#x3D; tf.matmul(L1,Weights_L2) + biases_L2</span><br><span class="line">prediction &#x3D; tf.nn.tanh(Wx_plus_b_L2)</span><br><span class="line"></span><br><span class="line">#二次代价函数</span><br><span class="line">loss &#x3D; tf.reduce_mean(tf.square(y-prediction))</span><br><span class="line">#使用梯度下降法训练</span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.1).minimize(loss)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    #变量初始化</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    for _ in range(2000):</span><br><span class="line">        sess.run(train_step,feed_dict&#x3D;&#123;x:x_data,y:y_data&#125;)</span><br><span class="line">        </span><br><span class="line">    #获得预测值</span><br><span class="line">    prediction_value &#x3D; sess.run(prediction,feed_dict&#x3D;&#123;x:x_data&#125;)</span><br><span class="line">    #画图</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(x_data,y_data)</span><br><span class="line">    plt.plot(x_data,prediction_value,&#39;r-&#39;,lw&#x3D;5)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Tensorflow-Coding-Notes</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title>关于激活函数前使用Batch Normalization的思考</title>
    <url>/2020/10/28/deeplearning-batch-normalization/</url>
    <content><![CDATA[<h1 id="没有BN时的网络流程"><a href="#没有BN时的网络流程" class="headerlink" title="没有BN时的网络流程"></a>没有BN时的网络流程</h1><p>在没有Batch Normalization时，$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$，$a^{[l]}=g^{[l]}(z^{[l]})$</p>
<p>注：$[l]$表示第$l$层网络的数据，$g^{[l]}(z)$为第$l$层的激活函数</p>
<p>注：Batch Normalization的目的是使参数搜索问题变得更容易，使神经网络对于超参数的选择更稳定</p>
<h1 id="在激活函数前Batch-Normalization"><a href="#在激活函数前Batch-Normalization" class="headerlink" title="在激活函数前Batch Normalization"></a>在激活函数前Batch Normalization</h1><p>①    $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$</p>
<p>②    $z_{BN}^{[l]}=\frac{z^{[l]}-u^{[l]}}{\sigma^{[l]}}=\frac{w^{[l]}a^{[l-1]}+b^{[l]}-u^{[l]}}{\sigma^{[l]}}$     则①可变为    $z^{[l]}=w^{[l]}a^{[l-1]}$，参数$b^{[l]}$失去了意义</p>
<p>③    由新加入的参数$\beta^{[l]}$和$\gamma^{[l]}$重新缩放    $\rightarrow$    $\overset{~}{z}^{[l]}=\beta^{[l]}z_{BN}^{[l]}+\gamma^{[l]}$</p>
<p>④    $\overset{~}{z}^{[l]}=\beta^{[l]}z_{BN}^{[l]}+\gamma^{[l]}=\beta^{[l]}\frac{z^{[l]}-u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}=\frac{\beta^{[l]}z^{[l]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}=\frac{\beta^{[l]}w^{[l]}a^{[l-1]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}$</p>
<h1 id="我的思考"><a href="#我的思考" class="headerlink" title="我的思考"></a>我的思考</h1><ol>
<li>​    $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$ 和 $z^{[l]}=\frac{\beta^{[l]}w^{[l]}a^{[l-1]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}$ 有多大的区别，后者是否同样是一个与前者一样的线性函数，如果是线性函数的话，这么做的意义何在？</li>
<li>​    可能的解释：$\sigma^{[l]}$ 和 $u^{[l]}$ 都是 $z^{[l]}$ 或者说 $a^{[l-1]}$ 的函数，并不能与①等同看成一个线性函数。</li>
<li>​    想法：$z^{[l]}=\frac{\beta^{[l]}w^{[l]}a^{[l-1]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}$ 是否是一个参数更多的神经元，是否可以用更多的参数来构建次数更高的多项式然后来进行模拟？</li>
</ol>
]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
  </entry>
  <entry>
    <title>代价函数</title>
    <url>/2020/10/30/deeplearning-cost-function/</url>
    <content><![CDATA[<h1 id="交叉熵代价函数"><a href="#交叉熵代价函数" class="headerlink" title="交叉熵代价函数"></a>交叉熵代价函数</h1><h2 id="交叉熵代价函数定义"><a href="#交叉熵代价函数定义" class="headerlink" title="交叉熵代价函数定义"></a>交叉熵代价函数定义</h2><script type="math/tex; mode=display">
C=-\frac{1}{n} \sum_{x}[y \ln a+(1-y) \ln (1-a)]</script><p>有</p>
<script type="math/tex; mode=display">
\begin{array}{l}
a=\sigma(z), \quad z=\sum W_{j}^{\star} X_{j}+b \\
\sigma^{\prime}(z)=\sigma(z)(1-\sigma(z))
\end{array}</script><h2 id="w的导数推导"><a href="#w的导数推导" class="headerlink" title="w的导数推导"></a>w的导数推导</h2><script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial C}{\partial w_{j}} &=-\frac{1}{n} \sum_{x}\left(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)}\right) \frac{\partial \sigma}{\partial w_{j}} \\
&=-\frac{1}{n} \sum_{x}\left(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)}\right) \sigma^{\prime}(z) x_{j} \\
&=\frac{1}{n} \sum_{x} \frac{\sigma^{\prime}(z) x_{j}}{\sigma(z)(1-\sigma(z))}(\sigma(z)-y) \\
&=\frac{1}{n} \sum_{x} x_{j}(\sigma(z)-y)
\end{aligned}</script><h2 id="b的导数推导"><a href="#b的导数推导" class="headerlink" title="b的导数推导"></a>b的导数推导</h2><script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial C}{\partial b_{j}} &=-\frac{1}{n} \sum_{x}\left(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)}\right) \frac{\partial \sigma}{\partial b_{j}} \\
&=-\frac{1}{n} \sum_{x}\left(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)}\right) \sigma^{\prime}(z)  \\
&=\frac{1}{n} \sum_{x} \frac{\sigma^{\prime}(z) }{\sigma(z)(1-\sigma(z))}(\sigma(z)-y) \\
&=\frac{1}{n} \sum_{x} (\sigma(z)-y)
\end{aligned}</script><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><script type="math/tex; mode=display">
\frac{\partial C}{\partial w_{j}}=\frac{1}{n} \sum_{x} x_{j}(\sigma(z)-y)</script><script type="math/tex; mode=display">
\frac{\partial C}{\partial b}=\frac{1}{n} \sum_{x}(\sigma(z)-y)</script><ul>
<li>权值和偏置值的调整与 $\sigma^{\prime}(z) $ 无关, 另外，梯度公式中的 $\sigma(z)-y$ 表示输出值与实际值的误差。所以当误差越大时，梯度就越大，参数w和b的调整就越快，训练的速度也就越快。</li>
<li>如果输出神经元是线性的，那么二次代价函数就是一种合适的选择。如果输出神经元是S型函数，<br>那么比较适合用交叉熵代价函数。</li>
<li>对数似然函数常用来作为softmax回归的代价函数，如果输出层神经元是sigmoid函数，可以采用<br>交叉熵代价函数。而深度学习中更普遍的做法是将softmax作为最后一层，此时常用的代价函数是<br>对数似然代价函数。</li>
<li>对数似然代价函数与softmax的组合和交叉嫡与sigmoid函数的组合非常相似。对数释然代价函数<br>在二分类时可以化简为交叉熵代价函数的形式。</li>
</ul>
<h2 id="在Tensorflow中使用"><a href="#在Tensorflow中使用" class="headerlink" title="在Tensorflow中使用:"></a>在Tensorflow中使用:</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits()  </span><br><span class="line">tf.nn.sparse_softmax_cross_entropy_with_logits()   </span><br><span class="line">tf.nn.sigmoid_cross_entropy_with_logits()  </span><br><span class="line">tf.nn.weighted_cross_entropy_with_logits()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits(_sentinel&#x3D;None, labels&#x3D;None, logits&#x3D;None, dim&#x3D;-1, name&#x3D;None)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.nn.sparse_softmax_cross_entropy_with_logits(_sentinel&#x3D;None,labels&#x3D;None,logits&#x3D;None, name&#x3D;None)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.nn.sigmoid_cross_entropy_with_logits(_sentinel&#x3D;None,labels&#x3D;None, logits&#x3D;None, name&#x3D;None)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.nn.weighted_cross_entropy_with_logits(labels,logits, pos_weight, name&#x3D;None)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Deeplearning</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title>过拟合</title>
    <url>/2020/10/30/deeplearning-over-fitting/</url>
    <content><![CDATA[<h1 id="什么是过拟合？"><a href="#什么是过拟合？" class="headerlink" title="什么是过拟合？"></a>什么是过拟合？</h1><h1 id="通过偏差和方差来判断过拟合-欠拟合"><a href="#通过偏差和方差来判断过拟合-欠拟合" class="headerlink" title="通过偏差和方差来判断过拟合/欠拟合"></a>通过偏差和方差来判断过拟合/欠拟合</h1><h1 id="过拟合解决方案"><a href="#过拟合解决方案" class="headerlink" title="过拟合解决方案"></a>过拟合解决方案</h1><h2 id="增加数据集"><a href="#增加数据集" class="headerlink" title="增加数据集"></a>增加数据集</h2><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2>]]></content>
      <categories>
        <category>Deeplearning</category>
      </categories>
  </entry>
  <entry>
    <title>2019刘晓燕救命班-comment</title>
    <url>/2020/10/25/postgraduate-exam-english/</url>
    <content><![CDATA[<center>`记笔记！！！然后学以致用！！！！`</center>

<p>（未完待续。。。2020/10/25 9:33）</p>
<h1 id="完型"><a href="#完型" class="headerlink" title="完型"></a>完型</h1><p>前面教你猜选项那个我觉得不靠谱，尽量还是自己做，但是最后真的考场上只剩一两分钟但是你还没做，那就大胆按照她的方法来蒙答案。</p>
<p>这个课我觉得还是值得看的，她里面讲到了按照文章的中心来做题，我觉得这个非常有用。</p>
<p>我以前看过王晟的课，他讲了逻辑关系词，我觉得那个对于完型、阅读、新题型都有好处，能够帮助理解文章（对于完型来说有利于解题，大概能够完型提升1-2分，对于阅读，文章理解好了提升也很大）</p>
<h1 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h1><p>翻译按照 <code>正确、通顺、完整</code> 进行，刘晓燕在她的课里面讲了“做一个勇敢的中国人”，她的意思是告诉你为了使翻译出来的中文要通顺，大胆对单词意思进行延伸。只要翻译的意思准确，信息传达完整，句子通顺就能得全分。（就算对单词逐个翻译，但是不通顺也不会得全分）。</p>
<p>另外刘晓燕还介绍了几种方法，我觉得都是能够比较快掌握又比较实用的。</p>
<h1 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h1><p>讲了一篇模板作文应该怎么写，也告诉你了如果你的英语能力好怎么做可以拿更高的分。在看视频的时候做好笔记，然后按照笔记来进行练习，学会写这个模板作文30分里面就已经可以稳稳拿到20分，对于自身的英语基础要求也不高，只需要会写简单句和几个句型就可以。</p>
<p>看完视频课后一定要自己写几篇，可以每天晚上写一篇然后持续一段时间。然后还有时间的话再看看其他的作文资料，没有时间的话这个已经足够了。考研英语作文要拿到25分我觉得需要一个比较好的英语基础，现在英语基础没那么好的话就把时间花在其他更容易提升的地方。</p>
<p>我英语76分，但是作文也是20分左右这一档（阅读+新题型得了大概40分，完型+翻译大概13分，完型翻译作文大家差距都不大，差距主要在阅读和新题型这里拉开，要考80分以上的话可能每个部分都得细致认真的复习）。</p>
<h1 id="阅读"><a href="#阅读" class="headerlink" title="阅读"></a>阅读</h1><p>思路比较重要。</p>
<p>一句话一句话的翻译一遍没有很大的作用，可能对翻译有点好处，但是对于阅读的提升并不大。阅读需要的基础积累是长期的，现在要找到正确的方法来利用已有的基础去更快更好的理解文章，然后更快更准确的做题。（应试比的不仅仅是基础）</p>
<h1 id="新题型"><a href="#新题型" class="headerlink" title="新题型"></a>新题型</h1><h1 id="Last-but-not-least"><a href="#Last-but-not-least" class="headerlink" title="Last but not least"></a>Last but not least</h1><p>英语这门课时间比较紧，经常会有同学在考场上做不完题，为了避免这种情况，需要在复习的时候就规划好每部分的做题时间，然后在每部分练习的时候就计时来做。</p>
<p>考试时间180分钟</p>
<p>完型-10分——10分钟</p>
<p>阅读-40分——70~80分钟</p>
<p>新题型-10分——20分钟</p>
<p>翻译-10分——20分钟</p>
<p>小作文-10分——10~20分钟</p>
<p>大作文-20分——30分钟</p>
<p>上边是各部分对应的分值和需要的大致时间，你应该按照自己的情况进行具体调整，在给自己设置计划的时候，把总时间当成170分钟来设置计划，留10分钟裕量确保考试的时候能够稳稳把题做完。</p>
<p>阅读大概每篇需要20分钟左右，但是每年真题阅读每篇文章的难度不一，有的只需要15分钟左右，有的需要可能20分钟多一点，我之前是每篇设置的20分钟，尤其是对于较难的文章，如果较难的文章都能在20分钟内完成好，那4篇文章70分钟应该不困难。</p>
<p>翻译自己先试一下翻译完5道题自己大概需要多长时间，做的时候不要拖拖拉拉死抠细解，因为要写的中文有点多，所以写字就得要很多时间。</p>
]]></content>
      <categories>
        <category>考研</category>
      </categories>
  </entry>
  <entry>
    <title>Tensorflow问题合集</title>
    <url>/2020/10/29/tensorflow-questions/</url>
    <content><![CDATA[<h6 id="Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。"><a href="#Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。" class="headerlink" title="Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。"></a>Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。</h6><p><img src="/.ink//tensorflow-questions-20201029.assets/image-20201029195107148.png" alt="image-20201029195107148" style="zoom:50%;"></p>
<p><img src="/.ink//tensorflow-questions-20201029.assets/image-20201029195649017.png" alt="image-20201029195649017" style="zoom:50%;"></p>
<h6 id="A1："><a href="#A1：" class="headerlink" title="A1："></a>A1：</h6><h6 id="Q2：随着迭代次数的增加，准确率在下降（来源于Tensorflow实战：mnist简单分类任务）"><a href="#Q2：随着迭代次数的增加，准确率在下降（来源于Tensorflow实战：mnist简单分类任务）" class="headerlink" title="Q2：随着迭代次数的增加，准确率在下降（来源于Tensorflow实战：mnist简单分类任务）"></a>Q2：随着迭代次数的增加，准确率在下降（来源于Tensorflow实战：mnist简单分类任务）</h6><p><img src="/.ink//image-20201030115021987.png" alt="image-20201030115021987" style="zoom:50%;"></p>
<p>参数：     batch_size = 8    learning_rate=0.2 w=0 b=0</p>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title>weekly-report-20201030</title>
    <url>/2020/10/30/weekly-report-20201030/</url>
    <content><![CDATA[<h1 id="DeepLearning"><a href="#DeepLearning" class="headerlink" title="DeepLearning"></a>DeepLearning</h1><h2 id="吴承恩的深度学习课程"><a href="#吴承恩的深度学习课程" class="headerlink" title="吴承恩的深度学习课程"></a>吴承恩的深度学习课程</h2><h2 id="第一门课-神经网络和深度学习-Neural-Networks-and-Deep-Learning"><a href="#第一门课-神经网络和深度学习-Neural-Networks-and-Deep-Learning" class="headerlink" title="第一门课 神经网络和深度学习(Neural Networks and Deep Learning)"></a>第一门课 神经网络和深度学习(Neural Networks and Deep Learning)</h2><p><a href="http://www.ai-start.com/dl2017/html/lesson1-week1.html">第一周：深度学习引言(Introduction to Deep Learning)</a></p>
<p>1.1 欢迎(Welcome) 1</p>
<p>1.2 什么是神经网络？(What is a Neural Network)</p>
<p>1.3 神经网络的监督学习(Supervised Learning with Neural Networks)</p>
<p>1.4 为什么神经网络会流行？(Why is Deep Learning taking off?)</p>
<p>1.5 关于本课程(About this Course)</p>
<p>1.6 课程资源(Course Resources)</p>
<p>1.7 Geoffery Hinton 专访(Geoffery Hinton interview)</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson1-week2.html">第二周：神经网络的编程基础(Basics of Neural Network programming)</a></p>
<p>2.1 二分类(Binary Classification)</p>
<p>2.2 逻辑回归(Logistic Regression)</p>
<p>2.3 逻辑回归的代价函数（Logistic Regression Cost Function）</p>
<p>2.4 梯度下降（Gradient Descent）</p>
<p>2.5 导数（Derivatives）</p>
<p>2.6 更多的导数例子（More Derivative Examples）</p>
<p>2.7 计算图（Computation Graph）</p>
<p>2.8 计算图导数（Derivatives with a Computation Graph）</p>
<p>2.9 逻辑回归的梯度下降（Logistic Regression Gradient Descent）</p>
<p>2.10 梯度下降的例子(Gradient Descent on m Examples)</p>
<p>2.11 向量化(Vectorization)</p>
<p>2.12 更多的向量化例子（More Examples of Vectorization）</p>
<p>2.13 向量化逻辑回归(Vectorizing Logistic Regression)</p>
<p>2.14 向量化逻辑回归的梯度计算（Vectorizing Logistic Regression’s Gradient）</p>
<p>2.15 Python中的广播机制（Broadcasting in Python）</p>
<p>2.16 关于 Python与numpy向量的使用（A note on python or numpy vectors）</p>
<p>2.17 Jupyter/iPython Notebooks快速入门（Quick tour of Jupyter/iPython Notebooks）</p>
<p>2.18 逻辑回归损失函数详解（Explanation of logistic regression cost function）</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson1-week3.html">第三周：浅层神经网络(Shallow neural networks)</a></p>
<p>3.1 神经网络概述（Neural Network Overview）</p>
<p>3.2 神经网络的表示（Neural Network Representation）</p>
<p>3.3 计算一个神经网络的输出（Computing a Neural Network’s output）</p>
<p>3.4 多样本向量化（Vectorizing across multiple examples）</p>
<p>3.5 向量化实现的解释（Justification for vectorized implementation）</p>
<p>3.6 激活函数（Activation functions）</p>
<p>3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）</p>
<p>3.8 激活函数的导数（Derivatives of activation functions）</p>
<p>3.9 神经网络的梯度下降（Gradient descent for neural networks）</p>
<p>3.10（选修）直观理解反向传播（Backpropagation intuition）</p>
<p>3.11 随机初始化（Random+Initialization）</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson1-week4.html">第四周：深层神经网络(Deep Neural Networks)</a></p>
<p>4.1 深层神经网络（Deep L-layer neural network）</p>
<p>4.2 前向传播和反向传播（Forward and backward propagation）</p>
<p>4.3 深层网络中的前向和反向传播（Forward propagation in a Deep Network）</p>
<p>4.4 核对矩阵的维数（Getting your matrix dimensions right）</p>
<p>4.5 为什么使用深层表示？（Why deep representations?）</p>
<p>4.6 搭建神经网络块（Building blocks of deep neural networks）</p>
<p>4.7 参数VS超参数（Parameters vs Hyperparameters）</p>
<p>4.8 深度学习和大脑的关联性（What does this have to do with the brain?）</p>
<h2 id="第二门课-改善深层神经网络：超参数调试、正则化以及优化-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization"><a href="#第二门课-改善深层神经网络：超参数调试、正则化以及优化-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization" class="headerlink" title="第二门课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)"></a>第二门课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)</h2><p><a href="http://www.ai-start.com/dl2017/html/lesson2-week1.html">第一周：深度学习的实用层面(Practical aspects of Deep Learning)</a></p>
<p>1.1 训练，验证，测试集（Train / Dev / Test sets）</p>
<p>1.2 偏差，方差（Bias /Variance）</p>
<p>1.3 机器学习基础（Basic Recipe for Machine Learning）</p>
<p>1.4 正则化（Regularization）</p>
<p>1.5 为什么正则化有利于预防过拟合呢？（Why regularization reduces overfitting?）</p>
<p>1.6 dropout 正则化（Dropout Regularization）</p>
<p>1.7 理解 dropout（Understanding Dropout）</p>
<p>1.8 其他正则化方法（Other regularization methods）</p>
<p>1.9 标准化输入（Normalizing inputs）</p>
<p>1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）</p>
<p>1.11 神经网络的权重初始化（Weight Initialization for Deep NetworksVanishing /Exploding gradients）</p>
<p>1.12 梯度的数值逼近（Numerical approximation of gradients）</p>
<p>1.13 梯度检验（Gradient checking）</p>
<p>1.14 梯度检验应用的注意事项（Gradient Checking Implementation Notes）</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson2-week2.html">第二周：优化算法 (Optimization algorithms)</a></p>
<p>2.1 Mini-batch 梯度下降（Mini-batch gradient descent）</p>
<p>2.2 理解Mini-batch 梯度下降（Understanding Mini-batch gradient descent）</p>
<p>2.3 指数加权平均（Exponentially weighted averages）</p>
<p>2.4 理解指数加权平均（Understanding Exponentially weighted averages）</p>
<p>2.5 指数加权平均的偏差修正（Bias correction in exponentially weighted averages）</p>
<p>2.6 momentum梯度下降（Gradient descent with momentum）</p>
<p>2.7 RMSprop——root mean square prop（RMSprop）</p>
<p>2.8 Adam优化算法（Adam optimization algorithm）</p>
<p>2.9 学习率衰减（Learning rate decay）</p>
<p>2.10 局部最优问题（The problem of local optima）</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson2-week3.html">第三周超参数调试，batch正则化和程序框架（Hyperparameter tuning, Batch Normalization and Programming Frameworks)</a></p>
<p>3.1 调试处理（Tuning process）</p>
<p>3.2 为超参数选择和适合范围（Using an appropriate scale to pick hyperparameters）</p>
<p>3.3 超参数训练的实践：Pandas vs. Caviar（Hyperparameters tuning in practice: Pandas vs. Caviar）</p>
<p>3.4 网络中的正则化激活函数（Normalizing activations in a network）</p>
<p>3.5 将 Batch Norm拟合进神经网络（Fitting Batch Norm into a neural network）</p>
<p>3.6 为什么Batch Norm奏效？（Why does Batch Norm work?）</p>
<p>3.7 测试时的Batch Norm（Batch Norm at test time）</p>
<p>3.8 Softmax 回归（Softmax Regression）</p>
<p>3.9 训练一个Softmax 分类器（Training a softmax classifier）</p>
<p>3.10 深度学习框架（Deep learning frameworks）</p>
<p>3.11 TensorFlow（TensorFlow）</p>
<h2 id="Tensorflow实战"><a href="#Tensorflow实战" class="headerlink" title="Tensorflow实战"></a>Tensorflow实战</h2><h3 id="非线性回归模型"><a href="#非线性回归模型" class="headerlink" title="非线性回归模型"></a>非线性回归模型</h3><h3 id="mnist手写数字识别"><a href="#mnist手写数字识别" class="headerlink" title="mnist手写数字识别"></a>mnist手写数字识别</h3><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><p>本周没有看论文。</p>
]]></content>
      <categories>
        <category>Study-Report</category>
      </categories>
      <tags>
        <tag>study-report</tag>
      </tags>
  </entry>
</search>
