<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>如何新建一篇文章</title>
    <url>/2020/10/24/article-writing/</url>
    <content><![CDATA[<p>1.在博客文件夹目录下鼠标右键，点击Git Bash Here</p>
<p>2.输入代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new article</span><br></pre></td></tr></table></figure>
<p>回车新建文章，生成的.md文件在/source/_post/目录下</p>
<p>3.</p>
<p>清除之前的页面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure>
<p>生成新的页面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo generate</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure>
<p>部署新的页面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo deploy</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure>
<p>打开本地服务器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure>
<p>4.文章Front Matter</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>title</td>
<td>文章的标题</td>
</tr>
<tr>
<td>date</td>
<td>文章的日期</td>
</tr>
<tr>
<td>categories</td>
<td>文章的所属类别，只有一个，填写多个时默认第一个</td>
</tr>
<tr>
<td>tags</td>
<td>标签，可以有多个</td>
</tr>
<tr>
<td>description</td>
<td>首页缩略显示内容</td>
</tr>
<tr>
<td>comments</td>
<td>是否支持评论</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
  <entry>
    <title>关于激活函数前使用Batch Normalization的思考</title>
    <url>/2020/10/28/batch-normalization-20201028/</url>
    <content><![CDATA[<h1 id="没有BN时的网络流程"><a href="#没有BN时的网络流程" class="headerlink" title="没有BN时的网络流程"></a>没有BN时的网络流程</h1><p>在没有Batch Normalization时，$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$，$a^{[l]}=g^{[l]}(z^{[l]})$</p>
<p>注：$[l]$表示第$l$层网络的数据，$g^{[l]}(z)$为第$l$层的激活函数</p>
<p>注：Batch Normalization的目的是使参数搜索问题变得更容易，使神经网络对于超参数的选择更稳定</p>
<h1 id="在激活函数前Batch-Normalization"><a href="#在激活函数前Batch-Normalization" class="headerlink" title="在激活函数前Batch Normalization"></a>在激活函数前Batch Normalization</h1><p>①    $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$</p>
<p>②    $z_{BN}^{[l]}=\frac{z^{[l]}-u^{[l]}}{\sigma^{[l]}}=\frac{w^{[l]}a^{[l-1]}+b^{[l]}-u^{[l]}}{\sigma^{[l]}}$     则①可变为    $z^{[l]}=w^{[l]}a^{[l-1]}$，参数$b^{[l]}$失去了意义</p>
<p>③    由新加入的参数$\beta^{[l]}$和$\gamma^{[l]}$重新缩放    $\rightarrow$    $\overset{~}{z}^{[l]}=\beta^{[l]}z_{BN}^{[l]}+\gamma^{[l]}$</p>
<p>④    $\overset{~}{z}^{[l]}=\beta^{[l]}z_{BN}^{[l]}+\gamma^{[l]}=\beta^{[l]}\frac{z^{[l]}-u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}=\frac{\beta^{[l]}z^{[l]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}=\frac{\beta^{[l]}w^{[l]}a^{[l-1]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}$</p>
<h1 id="我的思考"><a href="#我的思考" class="headerlink" title="我的思考"></a>我的思考</h1><ol>
<li>​    $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$ 和 $z^{[l]}=\frac{\beta^{[l]}w^{[l]}a^{[l-1]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}$ 有多大的区别，后者是否同样是一个与前者一样的线性函数，如果是线性函数的话，这么做的意义何在？</li>
<li>​    可能的解释：$\sigma^{[l]}$ 和 $u^{[l]}$ 都是 $z^{[l]}$ 或者说 $a^{[l-1]}$ 的函数，并不能与①等同看成一个线性函数。</li>
<li>​    想法：$z^{[l]}=\frac{\beta^{[l]}w^{[l]}a^{[l-1]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}$ 是否是一个参数更多的神经元，是否可以用更多的参数来构建次数更高的多项式然后来进行模拟？</li>
</ol>
]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
  </entry>
  <entry>
    <title>2019刘晓燕救命班-comment</title>
    <url>/2020/10/25/english-20201025/</url>
    <content><![CDATA[<center>`记笔记！！！然后学以致用！！！！`</center>

<p>（未完待续。。。2020/10/25 9:33）</p>
<h1 id="完型"><a href="#完型" class="headerlink" title="完型"></a>完型</h1><p>前面教你猜选项那个我觉得不靠谱，尽量还是自己做，但是最后真的考场上只剩一两分钟但是你还没做，那就大胆按照她的方法来蒙答案。</p>
<p>这个课我觉得还是值得看的，她里面讲到了按照文章的中心来做题，我觉得这个非常有用。</p>
<p>我以前看过王晟的课，他讲了逻辑关系词，我觉得那个对于完型、阅读、新题型都有好处，能够帮助理解文章（对于完型来说有利于解题，大概能够完型提升1-2分，对于阅读，文章理解好了提升也很大）</p>
<h1 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h1><p>翻译按照 <code>正确、通顺、完整</code> 进行，刘晓燕在她的课里面讲了“做一个勇敢的中国人”，她的意思是告诉你为了使翻译出来的中文要通顺，大胆对单词意思进行延伸。只要翻译的意思准确，信息传达完整，句子通顺就能得全分。（就算对单词逐个翻译，但是不通顺也不会得全分）。</p>
<p>另外刘晓燕还介绍了几种方法，我觉得都是能够比较快掌握又比较实用的。</p>
<h1 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h1><p>讲了一篇模板作文应该怎么写，也告诉你了如果你的英语能力好怎么做可以拿更高的分。在看视频的时候做好笔记，然后按照笔记来进行练习，学会写这个模板作文30分里面就已经可以稳稳拿到20分，对于自身的英语基础要求也不高，只需要会写简单句和几个句型就可以。</p>
<p>看完视频课后一定要自己写几篇，可以每天晚上写一篇然后持续一段时间。然后还有时间的话再看看其他的作文资料，没有时间的话这个已经足够了。考研英语作文要拿到25分我觉得需要一个比较好的英语基础，现在英语基础没那么好的话就把时间花在其他更容易提升的地方。</p>
<p>我英语76分，但是作文也是20分左右这一档（阅读+新题型得了大概40分，完型+翻译大概13分，完型翻译作文大家差距都不大，差距主要在阅读和新题型这里拉开，要考80分以上的话可能每个部分都得细致认真的复习）。</p>
<h1 id="阅读"><a href="#阅读" class="headerlink" title="阅读"></a>阅读</h1><p>思路比较重要。</p>
<p>一句话一句话的翻译一遍没有很大的作用，可能对翻译有点好处，但是对于阅读的提升并不大。阅读需要的基础积累是长期的，现在要找到正确的方法来利用已有的基础去更快更好的理解文章，然后更快更准确的做题。（应试比的不仅仅是基础）</p>
<h1 id="新题型"><a href="#新题型" class="headerlink" title="新题型"></a>新题型</h1><h1 id="Last-but-not-least"><a href="#Last-but-not-least" class="headerlink" title="Last but not least"></a>Last but not least</h1><p>英语这门课时间比较紧，经常会有同学在考场上做不完题，为了避免这种情况，需要在复习的时候就规划好每部分的做题时间，然后在每部分练习的时候就计时来做。</p>
<p>考试时间180分钟</p>
<p>完型-10分——10分钟</p>
<p>阅读-40分——70~80分钟</p>
<p>新题型-10分——20分钟</p>
<p>翻译-10分——20分钟</p>
<p>小作文-10分——10~20分钟</p>
<p>大作文-20分——30分钟</p>
<p>上边是各部分对应的分值和需要的大致时间，你应该按照自己的情况进行具体调整，在给自己设置计划的时候，把总时间当成170分钟来设置计划，留10分钟裕量确保考试的时候能够稳稳把题做完。</p>
<p>阅读大概每篇需要20分钟左右，但是每年真题阅读每篇文章的难度不一，有的只需要15分钟左右，有的需要可能20分钟多一点，我之前是每篇设置的20分钟，尤其是对于较难的文章，如果较难的文章都能在20分钟内完成好，那4篇文章70分钟应该不困难。</p>
<p>翻译自己先试一下翻译完5道题自己大概需要多长时间，做的时候不要拖拖拉拉死抠细解，因为要写的中文有点多，所以写字就得要很多时间。</p>
]]></content>
      <categories>
        <category>考研</category>
      </categories>
  </entry>
  <entry>
    <title>Tensorflow问题合集</title>
    <url>/2020/10/29/tensorflow-questions-20201029/</url>
    <content><![CDATA[<h6 id="Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。"><a href="#Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。" class="headerlink" title="Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。"></a>Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。</h6><p><img src="/.ink//tensorflow-questions-20201029.assets/image-20201029195107148.png" alt="image-20201029195107148" style="zoom:50%;"></p>
<p><img src="/.ink//tensorflow-questions-20201029.assets/image-20201029195649017.png" alt="image-20201029195649017" style="zoom:50%;"></p>
<h6 id="A1："><a href="#A1：" class="headerlink" title="A1："></a>A1：</h6>]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Deeplearning</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Tesorflow实战：简单mnist分类任务</title>
    <url>/2020/10/30/tesorflow-coding-mnist-simple-classfication/</url>
    <content><![CDATA[<h1 id="Original-Codes"><a href="#Original-Codes" class="headerlink" title="Original Codes"></a>Original Codes</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist &#x3D; input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot&#x3D;True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size &#x3D; 100</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch &#x3D; mnist.train.num_examples &#x2F;&#x2F; batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义两个placeholder</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32,[None,10])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#创建一个简单的神经网络</span><br><span class="line">w &#x3D; tf.Variable(tf.zeros([784,10]))</span><br><span class="line">b &#x3D; tf.Variable(tf.zeros([10]))</span><br><span class="line">prediction &#x3D; tf.nn.softmax(tf.matmul(x,w) + b)</span><br><span class="line"></span><br><span class="line">#二次代价函数</span><br><span class="line">loss &#x3D; tf.reduce_mean(tf.square(y - prediction))</span><br><span class="line"></span><br><span class="line">#使用梯度下降法</span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#初始化变量</span><br><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#定义求准确率的方法,结果存在一个布尔型列表中</span><br><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))</span><br><span class="line">#求准确率</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    for epoch in range(21):</span><br><span class="line">        for batch in range(n_batch):</span><br><span class="line">            batch_xs,batch_ys &#x3D; mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict&#x3D;&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">            </span><br><span class="line">        acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line">        print(&quot;Iter&quot; + str(epoch) + &quot;,Testing Accuracy&quot; + str(acc))</span><br></pre></td></tr></table></figure>
<p>执行结果：20次迭代后，准确率为0.9138</p>
<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><h2 id="Note1-tf-argmax"><a href="#Note1-tf-argmax" class="headerlink" title="Note1    tf.argmax()"></a>Note1    tf.argmax()</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.argmax(input,axis)</span><br></pre></td></tr></table></figure>
<p>含义：根据axis取值的不同返回每行或者每列最大值的索引。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>input</td>
<td>输入的array</td>
</tr>
<tr>
<td>axis</td>
<td>axis=0，将每一列最大元素的所在索引记录下来，最后输出每一列最大元素所在的索引数组。axis=1，将每一行最大元素所在的索引记录下来，最后返回每一行最大元素所在的索引数组。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Note2-tf-cast"><a href="#Note2-tf-cast" class="headerlink" title="Note2    tf.cast()"></a>Note2    tf.cast()</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.cast(x, dtype, name&#x3D;None)</span><br></pre></td></tr></table></figure>
<p>含义：tf.cast()函数的作用是执行 tensorflow 中张量数据类型转换。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>x</td>
<td>待转换的数据（张量）。</td>
</tr>
<tr>
<td>dtype</td>
<td>目标数据类型。</td>
</tr>
<tr>
<td>name</td>
<td>可选参数，定义操作的名称。</td>
</tr>
</tbody>
</table>
</div>
<p>Tensorflow中的数据类型列表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据类型</th>
<th>Python类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>DT_FLOAT</td>
<td>tf.float32</td>
<td>32位浮点数</td>
</tr>
<tr>
<td>DT_DOUBLE</td>
<td>tf.float64</td>
<td>64位浮点数</td>
</tr>
<tr>
<td>DT_INT64</td>
<td>tf.int64</td>
<td>64位有符号整型</td>
</tr>
<tr>
<td>DT_INT32</td>
<td>tf.int32</td>
<td>32位有符号整型</td>
</tr>
<tr>
<td>DT_INT16</td>
<td>tf.int16</td>
<td>16位有符号整型</td>
</tr>
<tr>
<td>DT_INT8</td>
<td>tf.int8</td>
<td>8位有符号整型</td>
</tr>
<tr>
<td>DT_UINT8</td>
<td>tf.uint8</td>
<td>8位无符号整型</td>
</tr>
<tr>
<td>DT_STRING</td>
<td>tf.string</td>
<td>可变长度的字节数组.每一个张量元素都是一个字节数组.</td>
</tr>
<tr>
<td>DT_BOOL</td>
<td>tf.bool</td>
<td>布尔型</td>
</tr>
<tr>
<td>DT_COMPLEX64</td>
<td>tf.complex64</td>
<td>由两个32位浮点数组成的复数:实数和虚数.</td>
</tr>
<tr>
<td>DT_QINT32</td>
<td>tf.qint32</td>
<td>用于量化Ops的32位有符号整型.</td>
</tr>
<tr>
<td>DT_QINT8</td>
<td>tf.qint8</td>
<td>用于量化Ops的8位有符号整型.</td>
</tr>
<tr>
<td>DT_QUINT8</td>
<td>tf.quint8</td>
<td>用于量化Ops的8位无符号整型.</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>Tensorflow-Coding-Notes</category>
      </categories>
      <tags>
        <tag>Deeplearning</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow实战：非线性逻辑回归</title>
    <url>/2020/10/28/tensorflow-coding-nonlinear-logic-regression/</url>
    <content><![CDATA[<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><h2 id="Note1-linspace-函数功能"><a href="#Note1-linspace-函数功能" class="headerlink" title="Note1    linspace()函数功能"></a>Note1    linspace()函数功能</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x_data &#x3D; np.linspace(-0.5,0.5,10)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">np.linspace(start, stop, num&#x3D;50, endpoint&#x3D;True, retstep&#x3D;False, dtype&#x3D;None)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>start</td>
<td>队列的开始值</td>
</tr>
<tr>
<td>stop</td>
<td>队列的结束值</td>
</tr>
<tr>
<td>num</td>
<td>要生成的样本数，非负数，默认是50</td>
</tr>
<tr>
<td>endpoint</td>
<td>若为True，“stop”是最后的样本；否则“stop”将不会被包含。默认为True</td>
</tr>
<tr>
<td>retstop</td>
<td>若为False，返回等差数列；否则返回array([samples, step])。默认为False</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Note2-newaxis-功能"><a href="#Note2-newaxis-功能" class="headerlink" title="Note2    [:,newaxis]功能"></a>Note2    [:,newaxis]功能</h2><p>执行代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x_data &#x3D; np.linspace(-0.5,0.5,10)</span><br><span class="line">print(x_data.shape)</span><br><span class="line">x_data &#x3D; x_data[:,newaxis]</span><br><span class="line">print(x_data.shape)</span><br></pre></td></tr></table></figure>
<p>输出结果为</p>
<p>(10,)</p>
<p>(10,1)</p>
<p>x_data[:,newaxis]能将原来的数据扩充一个维度。</p>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2><h2 id="Note3-正态分布生成函数"><a href="#Note3-正态分布生成函数" class="headerlink" title="Note3    正态分布生成函数"></a>Note3    正态分布生成函数</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">np.random.normal(loc&#x3D;0.0,scale&#x3D;1.0,size&#x3D;None)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>loc</td>
<td>float</td>
<td>正态分布的均值</td>
</tr>
<tr>
<td>scale</td>
<td>float</td>
<td>正态分布的标准差，对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高</td>
</tr>
<tr>
<td>size</td>
<td>int or tuple of ints</td>
<td>输出的shape，默认为None，只输出一个值</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Note4-y-np-square-x"><a href="#Note4-y-np-square-x" class="headerlink" title="Note4    y = np.square(x)"></a>Note4    y = np.square(x)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y &#x3D; np.square(x)</span><br></pre></td></tr></table></figure>
<p>返回的y与x有相同的shape，其中y的每个元素是x每个元素的平方。</p>
<h2 id="Note5-Tensorflow的占位符"><a href="#Note5-Tensorflow的占位符" class="headerlink" title="Note5 Tensorflow的占位符"></a>Note5 Tensorflow的占位符</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,1])</span><br></pre></td></tr></table></figure>
<p>定义了一个32位浮点型的占位符，shape为[None,1]，行数没有定义，列数定义为1</p>
<p>执行时使用feed_dict{}以字典的形式传入数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sess.run(train_step,feed_dict&#x3D;&#123;x:x_data,y:y_data&#125;)</span><br></pre></td></tr></table></figure>
<p>前面定义了operation，我们运行的时候seess.run()最后一个operation就可以，前面的会自动执行（我的推断）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.placeholder(dtype,shape&#x3D;None,name&#x3D;None)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>dtype</td>
<td>数据类型。常用的是tf.float32,tf.float64等数值类型</td>
</tr>
<tr>
<td>shape</td>
<td>数据形状。默认是None，就是一维值，也可以是多维（比如[2,3], [None, 3]表示列是3，行不定）</td>
</tr>
<tr>
<td>name</td>
<td>名称</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Codes"><a href="#Codes" class="headerlink" title="Codes"></a>Codes</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#使用numpy生成200个随机点</span><br><span class="line">x_data &#x3D; np.linspace(-0.5,0.5,200)[:,np.newaxis]</span><br><span class="line">noise &#x3D; np.random.normal(0,0.02,x_data.shape)</span><br><span class="line">y_data &#x3D; np.square(x_data) + noise</span><br><span class="line"></span><br><span class="line">#定义两个placeholder</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,1])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32,[None,1])</span><br><span class="line"></span><br><span class="line">#定义神经网络中间层</span><br><span class="line">Weights_L1 &#x3D; tf.Variable(tf.random_normal([1,10]))</span><br><span class="line">biases_L1 &#x3D; tf.Variable(tf.zeros([1,10]))</span><br><span class="line">Wx_plus_b_L1 &#x3D; tf.matmul(x,Weights_L1) + biases_L1</span><br><span class="line">L1 &#x3D; tf.nn.tanh(Wx_plus_b_L1)</span><br><span class="line"></span><br><span class="line">#定义神经网络输出层</span><br><span class="line">Weights_L2 &#x3D; tf.Variable(tf.random_normal([10,1]))</span><br><span class="line">biases_L2 &#x3D; tf.Variable(tf.zeros([1,1]))</span><br><span class="line">Wx_plus_b_L2 &#x3D; tf.matmul(L1,Weights_L2) + biases_L2</span><br><span class="line">prediction &#x3D; tf.nn.tanh(Wx_plus_b_L2)</span><br><span class="line"></span><br><span class="line">#二次代价函数</span><br><span class="line">loss &#x3D; tf.reduce_mean(tf.square(y-prediction))</span><br><span class="line">#使用梯度下降法训练</span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.1).minimize(loss)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    #变量初始化</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    for _ in range(2000):</span><br><span class="line">        sess.run(train_step,feed_dict&#x3D;&#123;x:x_data,y:y_data&#125;)</span><br><span class="line">        </span><br><span class="line">    #获得预测值</span><br><span class="line">    prediction_value &#x3D; sess.run(prediction,feed_dict&#x3D;&#123;x:x_data&#125;)</span><br><span class="line">    #画图</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(x_data,y_data)</span><br><span class="line">    plt.plot(x_data,prediction_value,&#39;r-&#39;,lw&#x3D;5)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Tensorflow-Coding-Notes</category>
      </categories>
      <tags>
        <tag>Deeplearning</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
</search>
