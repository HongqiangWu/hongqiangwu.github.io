<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>代价函数</title>
    <url>/2020/10/30/deeplearning-cost-function/</url>
    <content><![CDATA[<h1 id="交叉熵代价函数"><a href="#交叉熵代价函数" class="headerlink" title="交叉熵代价函数"></a>交叉熵代价函数</h1><h2 id="交叉熵代价函数定义"><a href="#交叉熵代价函数定义" class="headerlink" title="交叉熵代价函数定义"></a>交叉熵代价函数定义</h2><script type="math/tex; mode=display">
C=-\frac{1}{n} \sum_{x}[y \ln a+(1-y) \ln (1-a)]</script><p>有</p>
<script type="math/tex; mode=display">
\begin{array}{l}
a=\sigma(z), \quad z=\sum W_{j}^{\star} X_{j}+b \\
\sigma^{\prime}(z)=\sigma(z)(1-\sigma(z))
\end{array}</script><h2 id="w的导数推导"><a href="#w的导数推导" class="headerlink" title="w的导数推导"></a>w的导数推导</h2><script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial C}{\partial w_{j}} &=-\frac{1}{n} \sum_{x}\left(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)}\right) \frac{\partial \sigma}{\partial w_{j}} \\
&=-\frac{1}{n} \sum_{x}\left(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)}\right) \sigma^{\prime}(z) x_{j} \\
&=\frac{1}{n} \sum_{x} \frac{\sigma^{\prime}(z) x_{j}}{\sigma(z)(1-\sigma(z))}(\sigma(z)-y) \\
&=\frac{1}{n} \sum_{x} x_{j}(\sigma(z)-y)
\end{aligned}</script><h2 id="b的导数推导"><a href="#b的导数推导" class="headerlink" title="b的导数推导"></a>b的导数推导</h2><script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial C}{\partial b_{j}} &=-\frac{1}{n} \sum_{x}\left(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)}\right) \frac{\partial \sigma}{\partial b_{j}} \\
&=-\frac{1}{n} \sum_{x}\left(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)}\right) \sigma^{\prime}(z)  \\
&=\frac{1}{n} \sum_{x} \frac{\sigma^{\prime}(z) }{\sigma(z)(1-\sigma(z))}(\sigma(z)-y) \\
&=\frac{1}{n} \sum_{x} (\sigma(z)-y)
\end{aligned}</script><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><script type="math/tex; mode=display">
\frac{\partial C}{\partial w_{j}}=\frac{1}{n} \sum_{x} x_{j}(\sigma(z)-y)</script><script type="math/tex; mode=display">
\frac{\partial C}{\partial b}=\frac{1}{n} \sum_{x}(\sigma(z)-y)</script><ul>
<li>权值和偏置值的调整与 $\sigma^{\prime}(z) $ 无关, 另外，梯度公式中的 $\sigma(z)-y$ 表示输出值与实际值的误差。所以当误差越大时，梯度就越大，参数w和b的调整就越快，训练的速度也就越快。</li>
<li>如果输出神经元是线性的，那么二次代价函数就是一种合适的选择。如果输出神经元是S型函数，<br>那么比较适合用交叉熵代价函数。</li>
<li>对数似然函数常用来作为softmax回归的代价函数，如果输出层神经元是sigmoid函数，可以采用<br>交叉熵代价函数。而深度学习中更普遍的做法是将softmax作为最后一层，此时常用的代价函数是<br>对数似然代价函数。</li>
<li>对数似然代价函数与softmax的组合和交叉嫡与sigmoid函数的组合非常相似。对数释然代价函数<br>在二分类时可以化简为交叉熵代价函数的形式。</li>
</ul>
<h2 id="在Tensorflow中使用"><a href="#在Tensorflow中使用" class="headerlink" title="在Tensorflow中使用:"></a>在Tensorflow中使用:</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits()  </span><br><span class="line">tf.nn.sparse_softmax_cross_entropy_with_logits()   </span><br><span class="line">tf.nn.sigmoid_cross_entropy_with_logits()  </span><br><span class="line">tf.nn.weighted_cross_entropy_with_logits()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits(_sentinel&#x3D;None, labels&#x3D;None, logits&#x3D;None, dim&#x3D;-1, name&#x3D;None)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.nn.sparse_softmax_cross_entropy_with_logits(_sentinel&#x3D;None,labels&#x3D;None,logits&#x3D;None, name&#x3D;None)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.nn.sigmoid_cross_entropy_with_logits(_sentinel&#x3D;None,labels&#x3D;None, logits&#x3D;None, name&#x3D;None)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.nn.weighted_cross_entropy_with_logits(labels,logits, pos_weight, name&#x3D;None)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Deeplearning</category>
      </categories>
      <tags>
        <tag>Deeplearning</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>关于激活函数前使用Batch Normalization的思考</title>
    <url>/2020/10/28/deeplearning-batch-normalization/</url>
    <content><![CDATA[<h1 id="没有BN时的网络流程"><a href="#没有BN时的网络流程" class="headerlink" title="没有BN时的网络流程"></a>没有BN时的网络流程</h1><p>在没有Batch Normalization时，$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$，$a^{[l]}=g^{[l]}(z^{[l]})$</p>
<p>注：$[l]$表示第$l$层网络的数据，$g^{[l]}(z)$为第$l$层的激活函数</p>
<p>注：Batch Normalization的目的是使参数搜索问题变得更容易，使神经网络对于超参数的选择更稳定</p>
<h1 id="在激活函数前Batch-Normalization"><a href="#在激活函数前Batch-Normalization" class="headerlink" title="在激活函数前Batch Normalization"></a>在激活函数前Batch Normalization</h1><p>①    $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$</p>
<p>②    $z_{BN}^{[l]}=\frac{z^{[l]}-u^{[l]}}{\sigma^{[l]}}=\frac{w^{[l]}a^{[l-1]}+b^{[l]}-u^{[l]}}{\sigma^{[l]}}$     则①可变为    $z^{[l]}=w^{[l]}a^{[l-1]}$，参数$b^{[l]}$失去了意义</p>
<p>③    由新加入的参数$\beta^{[l]}$和$\gamma^{[l]}$重新缩放    $\rightarrow$    $\overset{~}{z}^{[l]}=\beta^{[l]}z_{BN}^{[l]}+\gamma^{[l]}$</p>
<p>④    $\overset{~}{z}^{[l]}=\beta^{[l]}z_{BN}^{[l]}+\gamma^{[l]}=\beta^{[l]}\frac{z^{[l]}-u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}=\frac{\beta^{[l]}z^{[l]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}=\frac{\beta^{[l]}w^{[l]}a^{[l-1]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}$</p>
<h1 id="我的思考"><a href="#我的思考" class="headerlink" title="我的思考"></a>我的思考</h1><ol>
<li>​    $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$ 和 $z^{[l]}=\frac{\beta^{[l]}w^{[l]}a^{[l-1]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}$ 有多大的区别，后者是否同样是一个与前者一样的线性函数，如果是线性函数的话，这么做的意义何在？</li>
<li>​    可能的解释：$\sigma^{[l]}$ 和 $u^{[l]}$ 都是 $z^{[l]}$ 或者说 $a^{[l-1]}$ 的函数，并不能与①等同看成一个线性函数。</li>
<li>​    想法：$z^{[l]}=\frac{\beta^{[l]}w^{[l]}a^{[l-1]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}$ 是否是一个参数更多的神经元，是否可以用更多的参数来构建次数更高的多项式然后来进行模拟？</li>
</ol>
]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
  </entry>
  <entry>
    <title>Optimizer优化器</title>
    <url>/2020/10/31/deeplearning-optimizer/</url>
    <content><![CDATA[<h1 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.train.GradientDescentOptimizer</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.train.AdadeltaOptimizer</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.train.AdagradOptimizer</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.train.AdagradDAOptimizer</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.train.MomentumOptimizer</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.train.AdamOptimizer</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.train.FtrlOptimizer</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.train.ProximalGradientDescentOptimizer</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.train.ProximalAdagradOptimizer</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.train.RMSPropOptimizer</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
\begin{aligned}
&\mathrm{J}(\mathrm{W}): \text { 代价函教 }\\
&\nabla \mathrm{w} \mathrm{J}(\mathrm{W}): \text { 代价函数的梯度 }\\
&\eta \text {：学习率 }
\end{aligned}</script><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><script type="math/tex; mode=display">
W=W-\eta \cdot \nabla w J\left(W ; x^{(i)} ; y^{(i)}\right)</script><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><script type="math/tex; mode=display">
\begin{aligned}
&\mathrm{Y}: \text { 动力，通常设置为0.9 }\\
&v_{t}=y v_{t-1}+\eta \nabla_{w} j(W)\\
&W=W-v_{t}
\end{aligned}</script><p>当前权值的改变会受到上一次权值改变的影响，类似于小球向下滚动的时候带上了惯性。这样可以加快小球的向下的速度。</p>
<h2 id="NAG（Nesterov-accelerated-gradient）"><a href="#NAG（Nesterov-accelerated-gradient）" class="headerlink" title="NAG（Nesterov accelerated gradient）"></a>NAG（Nesterov accelerated gradient）</h2><script type="math/tex; mode=display">
\begin{array}{l}
v_{t}=\gamma v_{t-1}+\eta \nabla w J\left(W-\gamma v_{t-1}\right) \\
W=W-v_{t}
\end{array}</script><p>NAG在TF中跟Momentum合并在同一个函数tf.train.MomentumOptimizer中，可以通过参数配置启用。在Momentum中小球会盲目地跟从下坡的梯度，容易发生错误，所以我们需要一个更聪明的小球，这个小球提计算$W-\gamma v_{t-1}$可以表示小球下一个位置大概在哪里。从而我们可以提前计算下一个位置的梯度，然后使用到当前位置。</p>
<h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><script type="math/tex; mode=display">
\begin{array}
&&i：代表第i个分类\\
&t：代表出现次数\\
&\epsilon：的作用是避免分母为0，取值一般为1e-8\\
&\eta ：取值一般为0.01 \\
&\mathrm{g}_{\mathrm{t},\mathrm{i}}=\nabla \mathrm{w}^{\mathrm{J}}\left(\mathrm{W}_{\mathrm{i}}\right)\\
&W_{t+1}=W_{t}-\frac{\eta}{\sqrt{\sum_{t^{\prime}=1}^{t}\left(g_{t^{\prime}, i}\right)^{2}+\epsilon}} \odot g_{t}
\end{array}</script><p>它是基于SGD的一种算法，它的核心思想是对比较常见的数据给予它比较小的学习率去调整参数，对于比较罕见的数据给予它比较大的学习率去调整参数。它很适合应用于数据稀疏的数据集（比如一个图片数据集，有10000张狗的昭片，10000张猫的照片，只有100张大象的昭片）。</p>
<p>Adagrad主要的优势在于不需要人为的调节学习率，它可以自动调节。它的缺点在于，随着迭代次数的增多，学习率也会越来越低，最终会趋向于0。</p>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>RMS（Root Mean Square）是均方根的缩写。</p>
<script type="math/tex; mode=display">
\begin{array}
&&\gamma：动力，通常设置为0.9\\
&n：取值一般为0.001\\
&E[g^2]_t：表示前t次的梯度平方的平均值\\
&\mathrm{g}_{\mathrm{t}}=\nabla \mathrm{w} \mathrm{J}(\mathrm{W}) \\

&E\left[g^{2}\right]_{t}=y E\left[g^{2}\right]_{t-1}+(1-\gamma) g^{2} t \\
&W_{t+1}=W_{t}-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} \odot g_{t}
\end{array}</script><p>RMSprop借鉴了一些Adagrad的思想，不过这里RMSprop只用到了前$t-1$次梯度平方的平均值加上当前梯度的平方的和的开平方作为学习率的分母。这样RMSprop不会出现学习率越来越低的问题，而且也能自己调节学习率，并且可以有一个比较好的效果。</p>
<h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><script type="math/tex; mode=display">
\begin{array}{l}
&\mathrm{g}_{\mathrm{t}}=\nabla \mathrm{w}{\mathrm{J}}(\mathrm{W}) \\
&\Delta \mathrm{W}_{\mathrm{t}}=-\frac{\mathrm{\eta}}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} \odot g_{t} \\
&\Delta \mathrm{W}_{\mathrm{t}}=-\frac{\mathrm{\eta}}{R M S[\mathrm{g}]_{t}} \odot g_{t} \\
&\mathrm{W}_{\mathrm{t}+1}=\mathrm{W}_{\mathrm{t}}-\frac{\mathrm{RMS}[\Delta \mathrm{W}]_{t-1}}{\mathrm{RMS}[\mathrm{g}]_{t}}
\end{array}</script><p>使用Adadelta我们甚至不需要设置一个默认学习率，在Adadelta不需要使用学习率也可以达到一个非常好的效果。</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><script type="math/tex; mode=display">
\begin{array}
&&\beta_{1}: 一般取值 0.9 \\
&\beta_{2}: 一般取值0.999 \\
&\varepsilon: 避免分母为 0, 一般取值 10^{-8} \\ &\mathrm{m}_{\mathrm{t}}=\beta_{1}  \mathrm{m}_{\mathrm{t}-1}+\left(1-\beta_{1}\right) \mathrm{g}_{\mathrm{t}} \\
&v_{t}=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g^{2}_ t \\
&\hat{\mathrm{m}}_{\mathrm{t}}=\frac{\mathrm{m}_{\mathrm{t}}}{1-\beta_{1}^{\mathrm{t}}} \\
&\hat{v}_{\mathrm{t}}=\frac{v_{\mathrm{t}}}{1-\beta_{2}^{\mathrm{t}}} \\
&\mathrm{W}_{\mathrm{t}+1}=\mathrm{W}_{\mathrm{t}}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\varepsilon} \hat{m}_{t}
\end{array}</script><p>就像Adadelta和RMSprop一样Adam会存储之前衰减的平方梯度，同时它也会保存之前衰减的梯度。经过一些处理之后再使用类似Adadelta和RMSprop的方式更新参数。</p>
<h1 id="各种优化器对比"><a href="#各种优化器对比" class="headerlink" title="各种优化器对比"></a>各种优化器对比</h1><h2 id="标准梯度下降法"><a href="#标准梯度下降法" class="headerlink" title="标准梯度下降法"></a>标准梯度下降法</h2><p>标准梯度下降先计算所有样本汇总误差，然后根据总误差来更新权值。</p>
<p>缺点：大样本时每次更新取值都要很长时间。</p>
<h2 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h2><p>随机梯度下降随机抽取一个样本来计算误差，然后更新权值。</p>
<p>缺点：取值更新快，但是不一定都是往正确的方向更新，会产生比较多的噪点。</p>
<h2 id="批量梯度下降法（常用）"><a href="#批量梯度下降法（常用）" class="headerlink" title="批量梯度下降法（常用）"></a>批量梯度下降法（常用）</h2><p>批量梯度下降算是一种折中的方案，从总样本中选取一个批次（比如一共有10000个样本，随机选取100个样本作为一个batch），然后计算这个batch的总误差，根据总误差来更新权值。</p>
]]></content>
      <categories>
        <category>Deeplearning</category>
      </categories>
      <tags>
        <tag>Deeplearning</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>过拟合</title>
    <url>/2020/10/30/deeplearning-over-fitting/</url>
    <content><![CDATA[<h1 id="什么是过拟合？"><a href="#什么是过拟合？" class="headerlink" title="什么是过拟合？"></a>什么是过拟合？</h1><h1 id="通过偏差和方差来判断过拟合-欠拟合"><a href="#通过偏差和方差来判断过拟合-欠拟合" class="headerlink" title="通过偏差和方差来判断过拟合/欠拟合"></a>通过偏差和方差来判断过拟合/欠拟合</h1><h1 id="过拟合解决方案"><a href="#过拟合解决方案" class="headerlink" title="过拟合解决方案"></a>过拟合解决方案</h1><h2 id="增加数据集"><a href="#增加数据集" class="headerlink" title="增加数据集"></a>增加数据集</h2><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2>]]></content>
      <categories>
        <category>Deeplearning</category>
      </categories>
  </entry>
  <entry>
    <title>如何新建一篇文章</title>
    <url>/2020/10/24/hexo-article-writing/</url>
    <content><![CDATA[<p>1.在博客文件夹目录下鼠标右键，点击Git Bash Here</p>
<p>2.输入代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new article</span><br></pre></td></tr></table></figure>
<p>回车新建文章，生成的.md文件在/source/_post/目录下</p>
<p>3.</p>
<p>清除之前的页面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure>
<p>生成新的页面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo generate</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure>
<p>部署新的页面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo deploy</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure>
<p>打开本地服务器</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure>
<p>4.文章Front Matter</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>title</td>
<td>文章的标题</td>
</tr>
<tr>
<td>date</td>
<td>文章的日期</td>
</tr>
<tr>
<td>categories</td>
<td>文章的所属类别，只有一个，填写多个时默认第一个</td>
</tr>
<tr>
<td>tags</td>
<td>标签，可以有多个</td>
</tr>
<tr>
<td>description</td>
<td>首页缩略显示内容</td>
</tr>
<tr>
<td>comments</td>
<td>是否支持评论</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
  <entry>
    <title>考研：刘晓燕英语保命班</title>
    <url>/2020/10/25/postgraduate-qualifying-examination-english/</url>
    <content><![CDATA[<center>`记笔记！！！然后学以致用！！！！`</center>

<p>（未完待续。。。2020/10/25 9:33）</p>
<h1 id="完型"><a href="#完型" class="headerlink" title="完型"></a>完型</h1><p>前面教你猜选项那个我觉得不靠谱，尽量还是自己做，但是最后真的考场上只剩一两分钟但是你还没做，那就大胆按照她的方法来蒙答案。</p>
<p>这个课我觉得还是值得看的，她里面讲到了按照文章的中心来做题，我觉得这个非常有用。</p>
<p>我以前看过王晟的课，他讲了逻辑关系词，我觉得那个对于完型、阅读、新题型都有好处，能够帮助理解文章（对于完型来说有利于解题，大概能够完型提升1-2分，对于阅读，文章理解好了提升也很大）</p>
<h1 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h1><p>翻译按照 <code>正确、通顺、完整</code> 进行，刘晓燕在她的课里面讲了“做一个勇敢的中国人”，她的意思是告诉你为了使翻译出来的中文要通顺，大胆对单词意思进行延伸。只要翻译的意思准确，信息传达完整，句子通顺就能得全分。（就算对单词逐个翻译，但是不通顺也不会得全分）。</p>
<p>另外刘晓燕还介绍了几种方法，我觉得都是能够比较快掌握又比较实用的。</p>
<h1 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h1><p>讲了一篇模板作文应该怎么写，也告诉你了如果你的英语能力好怎么做可以拿更高的分。在看视频的时候做好笔记，然后按照笔记来进行练习，学会写这个模板作文30分里面就已经可以稳稳拿到20分，对于自身的英语基础要求也不高，只需要会写简单句和几个句型就可以。</p>
<p>看完视频课后一定要自己写几篇，可以每天晚上写一篇然后持续一段时间。然后还有时间的话再看看其他的作文资料，没有时间的话这个已经足够了。考研英语作文要拿到25分我觉得需要一个比较好的英语基础，现在英语基础没那么好的话就把时间花在其他更容易提升的地方。</p>
<p>我英语76分，但是作文也是20分左右这一档（阅读+新题型得了大概40分，完型+翻译大概13分，完型翻译作文大家差距都不大，差距主要在阅读和新题型这里拉开，要考80分以上的话可能每个部分都得细致认真的复习）。</p>
<h1 id="阅读"><a href="#阅读" class="headerlink" title="阅读"></a>阅读</h1><p>思路比较重要。</p>
<p>一句话一句话的翻译一遍没有很大的作用，可能对翻译有点好处，但是对于阅读的提升并不大。阅读需要的基础积累是长期的，现在要找到正确的方法来利用已有的基础去更快更好的理解文章，然后更快更准确的做题。（应试比的不仅仅是基础）</p>
<h1 id="新题型"><a href="#新题型" class="headerlink" title="新题型"></a>新题型</h1><h1 id="Last-but-not-least"><a href="#Last-but-not-least" class="headerlink" title="Last but not least"></a>Last but not least</h1><p>英语这门课时间比较紧，经常会有同学在考场上做不完题，为了避免这种情况，需要在复习的时候就规划好每部分的做题时间，然后在每部分练习的时候就计时来做。</p>
<p>考试时间180分钟</p>
<p>完型-10分——10分钟</p>
<p>阅读-40分——70~80分钟</p>
<p>新题型-10分——20分钟</p>
<p>翻译-10分——20分钟</p>
<p>小作文-10分——10~20分钟</p>
<p>大作文-20分——30分钟</p>
<p>上边是各部分对应的分值和需要的大致时间，你应该按照自己的情况进行具体调整，在给自己设置计划的时候，把总时间当成170分钟来设置计划，留10分钟裕量确保考试的时候能够稳稳把题做完。</p>
<p>阅读大概每篇需要20分钟左右，但是每年真题阅读每篇文章的难度不一，有的只需要15分钟左右，有的需要可能20分钟多一点，我之前是每篇设置的20分钟，尤其是对于较难的文章，如果较难的文章都能在20分钟内完成好，那4篇文章70分钟应该不困难。</p>
<p>翻译自己先试一下翻译完5道题自己大概需要多长时间，做的时候不要拖拖拉拉死抠细解，因为要写的中文有点多，所以写字就得要很多时间。</p>
]]></content>
      <categories>
        <category>Postgraduate-Qualifying-Examination</category>
      </categories>
  </entry>
  <entry>
    <title>Tensorflow-Coding-Notes:dropout</title>
    <url>/2020/11/01/tensorflow-coding-notes-dropout/</url>
    <content><![CDATA[<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><h2 id="Note1-tf-nn-dropout"><a href="#Note1-tf-nn-dropout" class="headerlink" title="Note1 tf.nn.dropout()"></a>Note1 tf.nn.dropout()</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.nn.dropout(x, keep_prob, noise_shape&#x3D;None, seed&#x3D;None,name&#x3D;None)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">keep_prob &#x3D; tf.placeholder(tf.float32)</span><br></pre></td></tr></table></figure>
<h1 id="Codes"><a href="#Codes" class="headerlink" title="Codes"></a>Codes</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist &#x3D; input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot&#x3D;True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size &#x3D; 64</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch &#x3D; mnist.train.num_examples &#x2F;&#x2F; batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义placeholder，x为输入，y为label</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32,[None,10])</span><br><span class="line"></span><br><span class="line">#定义占位符，dropout的比例，神经元工作的比例</span><br><span class="line">keep_prob &#x3D; tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">#神经网络参数</span><br><span class="line">L1_size &#x3D; 200</span><br><span class="line">L2_size &#x3D; 200</span><br><span class="line">L3_size &#x3D; 200</span><br><span class="line"></span><br><span class="line">#定义神经网络</span><br><span class="line">W1 &#x3D; tf.Variable(tf.truncated_normal([784,L1_size],stddev &#x3D; 0.1))</span><br><span class="line">b1 &#x3D; tf.Variable(tf.zeros([1,L1_size]) + 0.01)</span><br><span class="line">z1 &#x3D; tf.matmul(x,W1) + b1</span><br><span class="line">a1 &#x3D; tf.nn.tanh(z1)</span><br><span class="line">L1_drop &#x3D; tf.nn.dropout(a1,keep_prob)</span><br><span class="line"></span><br><span class="line">W2 &#x3D; tf.Variable(tf.truncated_normal([L1_size,L2_size],stddev&#x3D;0.1))</span><br><span class="line">b2 &#x3D; tf.Variable(tf.zeros([1,L2_size]) + 0.01)</span><br><span class="line">z2 &#x3D; tf.matmul(L1_drop,W2) + b2</span><br><span class="line">a2 &#x3D; tf.nn.tanh(z2)</span><br><span class="line">L2_drop &#x3D; tf.nn.dropout(a2,keep_prob)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W3 &#x3D; tf.Variable(tf.truncated_normal([L2_size,L3_size],stddev&#x3D;0.1))</span><br><span class="line">b3 &#x3D; tf.Variable(tf.zeros([1,L3_size]) + 0.01)</span><br><span class="line">z3 &#x3D; tf.matmul(L2_drop,W3) + b3</span><br><span class="line">a3 &#x3D; tf.nn.tanh(z3)</span><br><span class="line">L3_drop &#x3D; tf.nn.dropout(a3,keep_prob)</span><br><span class="line"></span><br><span class="line">Wout &#x3D; tf.Variable(tf.truncated_normal([L3_size,10],stddev&#x3D;0.1))</span><br><span class="line">bout &#x3D; tf.Variable(tf.zeros([1,10]) + 0.01)</span><br><span class="line">zout &#x3D; tf.matmul(L3_drop,Wout) + bout</span><br><span class="line">prediction &#x3D; tf.nn.softmax(zout)</span><br><span class="line"></span><br><span class="line">#代价函数</span><br><span class="line">loss &#x3D; tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels &#x3D; y,logits &#x3D; prediction))</span><br><span class="line"></span><br><span class="line">#梯度下降法的优化器</span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#变量初试化</span><br><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#求准确度</span><br><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    </span><br><span class="line">    for epoch in range(100):</span><br><span class="line">        for batch in range(n_batch):</span><br><span class="line">            batch_xs,batch_ys &#x3D; mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict&#x3D;&#123;x:batch_xs,y:batch_ys,keep_prob:0.7&#125;)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"> </span><br><span class="line">            </span><br><span class="line">        test_acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;)</span><br><span class="line">        train_acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0&#125;)</span><br><span class="line">        print(&quot;Iter&quot; + str(epoch) + &quot;,Testing Accuracy:&quot; + str(test_acc) + &quot; |  Trainung Accuracy:&quot; + str(train_acc))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Tensorflow-Coding-Notes</category>
      </categories>
      <tags>
        <tag>Deeplearning</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow-Coding-Notes:CNN</title>
    <url>/2020/11/02/tensorflow-coding-notes-cnn/</url>
    <content><![CDATA[<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><h2 id="Note1-conv2d"><a href="#Note1-conv2d" class="headerlink" title="Note1    conv2d()"></a>Note1    conv2d()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span>(<span class="params">x,W</span>):</span></span><br><span class="line">    <span class="comment">#x input tensor of shape &#x27;[batch,in_height,in_width,in_channels]&#x27;</span></span><br><span class="line">    <span class="comment">#W filter/kernel tensor of shape [filter_height,filter_weight,in_channels,out_channels]</span></span><br><span class="line">    <span class="comment">#stride[0] = stride[3] = 1 ,stride[1]代表x方向上的步长，stride[2]代表y方向上的步长</span></span><br><span class="line">    <span class="comment">#padding: a string :&#x27;SAME&#x27; / &#x27;VALID&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x,W,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Note2-max-pool"><a href="#Note2-max-pool" class="headerlink" title="Note2    max_pool()"></a>Note2    max_pool()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="comment">#ksize [1,x,y,1]</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Codes"><a href="#Codes" class="headerlink" title="Codes"></a>Codes</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&#x27;MNIST_data&#x27;</span>,one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个批次的大小</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line"><span class="comment">#计算一共有多少个批次</span></span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line"><span class="comment">#参数概要</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span>(<span class="params">var</span>):</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;summaries&#x27;</span>):</span><br><span class="line">        mean = tf.reduce_mean(var)</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;mean&#x27;</span>, mean)<span class="comment">#平均值</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;stddev&#x27;</span>):</span><br><span class="line">            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;stddev&#x27;</span>, stddev)<span class="comment">#标准差</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;max&#x27;</span>, tf.reduce_max(var))<span class="comment">#最大值</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;min&#x27;</span>, tf.reduce_min(var))<span class="comment">#最小值</span></span><br><span class="line">        tf.summary.histogram(<span class="string">&#x27;histogram&#x27;</span>, var)<span class="comment">#直方图</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化权值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span>(<span class="params">shape,name</span>):</span></span><br><span class="line">    initial = tf.truncated_normal(shape,stddev=<span class="number">0.1</span>)<span class="comment">#生成一个截断的正态分布</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial,name=name)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化偏置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span>(<span class="params">shape,name</span>):</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>,shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial,name=name)</span><br><span class="line"></span><br><span class="line"><span class="comment">#卷积层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span>(<span class="params">x,W</span>):</span></span><br><span class="line">    <span class="comment">#x input tensor of shape `[batch, in_height, in_width, in_channels]`</span></span><br><span class="line">    <span class="comment">#W filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]</span></span><br><span class="line">    <span class="comment">#`strides[0] = strides[3] = 1`. strides[1]代表x方向的步长，strides[2]代表y方向的步长</span></span><br><span class="line">    <span class="comment">#padding: A `string` from: `&quot;SAME&quot;, &quot;VALID&quot;`</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x,W,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#池化层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="comment">#ksize [1,x,y,1]</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#命名空间</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;input&#x27;</span>):</span><br><span class="line">    <span class="comment">#定义两个placeholder</span></span><br><span class="line">    x = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>],name=<span class="string">&#x27;x-input&#x27;</span>)</span><br><span class="line">    y = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>],name=<span class="string">&#x27;y-input&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;x_image&#x27;</span>):</span><br><span class="line">        <span class="comment">#改变x的格式转为4D的向量[batch, in_height, in_width, in_channels]`</span></span><br><span class="line">        x_image = tf.reshape(x,[<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>],name=<span class="string">&#x27;x_image&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;Conv1&#x27;</span>):</span><br><span class="line">    <span class="comment">#初始化第一个卷积层的权值和偏置</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;W_conv1&#x27;</span>):</span><br><span class="line">        W_conv1 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>],name=<span class="string">&#x27;W_conv1&#x27;</span>)<span class="comment">#5*5的采样窗口，32个卷积核从1个平面抽取特征</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;b_conv1&#x27;</span>):  </span><br><span class="line">        b_conv1 = bias_variable([<span class="number">32</span>],name=<span class="string">&#x27;b_conv1&#x27;</span>)<span class="comment">#每一个卷积核一个偏置值</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;conv2d_1&#x27;</span>):</span><br><span class="line">        conv2d_1 = conv2d(x_image,W_conv1) + b_conv1</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;relu&#x27;</span>):</span><br><span class="line">        h_conv1 = tf.nn.relu(conv2d_1)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;h_pool1&#x27;</span>):</span><br><span class="line">        h_pool1 = max_pool_2x2(h_conv1)<span class="comment">#进行max-pooling</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;Conv2&#x27;</span>):</span><br><span class="line">    <span class="comment">#初始化第二个卷积层的权值和偏置</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;W_conv2&#x27;</span>):</span><br><span class="line">        W_conv2 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>],name=<span class="string">&#x27;W_conv2&#x27;</span>)<span class="comment">#5*5的采样窗口，64个卷积核从32个平面抽取特征</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;b_conv2&#x27;</span>):  </span><br><span class="line">        b_conv2 = bias_variable([<span class="number">64</span>],name=<span class="string">&#x27;b_conv2&#x27;</span>)<span class="comment">#每一个卷积核一个偏置值</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;conv2d_2&#x27;</span>):</span><br><span class="line">        conv2d_2 = conv2d(h_pool1,W_conv2) + b_conv2</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;relu&#x27;</span>):</span><br><span class="line">        h_conv2 = tf.nn.relu(conv2d_2)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;h_pool2&#x27;</span>):</span><br><span class="line">        h_pool2 = max_pool_2x2(h_conv2)<span class="comment">#进行max-pooling</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#28*28的图片第一次卷积后还是28*28，第一次池化后变为14*14</span></span><br><span class="line"><span class="comment">#第二次卷积后为14*14，第二次池化后变为了7*7</span></span><br><span class="line"><span class="comment">#进过上面操作后得到64张7*7的平面</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;fc1&#x27;</span>):</span><br><span class="line">    <span class="comment">#初始化第一个全连接层的权值</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;W_fc1&#x27;</span>):</span><br><span class="line">        W_fc1 = weight_variable([<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>,<span class="number">1024</span>],name=<span class="string">&#x27;W_fc1&#x27;</span>)<span class="comment">#上一场有7*7*64个神经元，全连接层有1024个神经元</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;b_fc1&#x27;</span>):</span><br><span class="line">        b_fc1 = bias_variable([<span class="number">1024</span>],name=<span class="string">&#x27;b_fc1&#x27;</span>)<span class="comment">#1024个节点</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#把池化层2的输出扁平化为1维</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;h_pool2_flat&#x27;</span>):</span><br><span class="line">        h_pool2_flat = tf.reshape(h_pool2,[<span class="number">-1</span>,<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>],name=<span class="string">&#x27;h_pool2_flat&#x27;</span>)</span><br><span class="line">    <span class="comment">#求第一个全连接层的输出</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;wx_plus_b1&#x27;</span>):</span><br><span class="line">        wx_plus_b1 = tf.matmul(h_pool2_flat,W_fc1) + b_fc1</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;relu&#x27;</span>):</span><br><span class="line">        h_fc1 = tf.nn.relu(wx_plus_b1)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#keep_prob用来表示神经元的输出概率</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;keep_prob&#x27;</span>):</span><br><span class="line">        keep_prob = tf.placeholder(tf.float32,name=<span class="string">&#x27;keep_prob&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;h_fc1_drop&#x27;</span>):</span><br><span class="line">        h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob,name=<span class="string">&#x27;h_fc1_drop&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;fc2&#x27;</span>):</span><br><span class="line">    <span class="comment">#初始化第二个全连接层</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;W_fc2&#x27;</span>):</span><br><span class="line">        W_fc2 = weight_variable([<span class="number">1024</span>,<span class="number">10</span>],name=<span class="string">&#x27;W_fc2&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;b_fc2&#x27;</span>):    </span><br><span class="line">        b_fc2 = bias_variable([<span class="number">10</span>],name=<span class="string">&#x27;b_fc2&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;wx_plus_b2&#x27;</span>):</span><br><span class="line">        wx_plus_b2 = tf.matmul(h_fc1_drop,W_fc2) + b_fc2</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;softmax&#x27;</span>):</span><br><span class="line">        <span class="comment">#计算输出</span></span><br><span class="line">        prediction = tf.nn.softmax(wx_plus_b2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#交叉熵代价函数</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;cross_entropy&#x27;</span>):</span><br><span class="line">    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction),name=<span class="string">&#x27;cross_entropy&#x27;</span>)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;cross_entropy&#x27;</span>,cross_entropy)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#使用AdamOptimizer进行优化</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">    train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment">#求准确率</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;accuracy&#x27;</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;correct_prediction&#x27;</span>):</span><br><span class="line">        <span class="comment">#结果存放在一个布尔列表中</span></span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(prediction,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>))<span class="comment">#argmax返回一维张量中最大的值所在的位置</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;accuracy&#x27;</span>):</span><br><span class="line">        <span class="comment">#求准确率</span></span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;accuracy&#x27;</span>,accuracy)</span><br><span class="line">        </span><br><span class="line"><span class="comment">#合并所有的summary</span></span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    train_writer = tf.summary.FileWriter(<span class="string">&#x27;logs/train&#x27;</span>,sess.graph)</span><br><span class="line">    test_writer = tf.summary.FileWriter(<span class="string">&#x27;logs/test&#x27;</span>,sess.graph)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1001</span>):</span><br><span class="line">        <span class="comment">#训练模型</span></span><br><span class="line">        batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:<span class="number">0.5</span>&#125;)</span><br><span class="line">        <span class="comment">#记录训练集计算的参数</span></span><br><span class="line">        summary = sess.run(merged,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        train_writer.add_summary(summary,i)</span><br><span class="line">        <span class="comment">#记录测试集计算的参数</span></span><br><span class="line">        batch_xs,batch_ys =  mnist.test.next_batch(batch_size)</span><br><span class="line">        summary = sess.run(merged,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        test_writer.add_summary(summary,i)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            test_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">            train_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.train.images[:<span class="number">10000</span>],y:mnist.train.labels[:<span class="number">10000</span>],keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;Iter &quot;</span> + <span class="built_in">str</span>(i) + <span class="string">&quot;, Testing Accuracy= &quot;</span> + <span class="built_in">str</span>(test_acc) + <span class="string">&quot;, Training Accuracy= &quot;</span> + <span class="built_in">str</span>(train_acc))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Tensorflow-Coding-Notes</category>
      </categories>
      <tags>
        <tag>Deeplearning</tag>
        <tag>Tensorflow</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Tesorflow-Coding-Notes：简单mnist分类任务</title>
    <url>/2020/10/30/tensorflow-coding-notes-mnist-simple-classfication/</url>
    <content><![CDATA[<h1 id="Original-Codes"><a href="#Original-Codes" class="headerlink" title="Original Codes"></a>Original Codes</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist &#x3D; input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot&#x3D;True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size &#x3D; 100</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch &#x3D; mnist.train.num_examples &#x2F;&#x2F; batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义两个placeholder</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32,[None,10])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#创建一个简单的神经网络</span><br><span class="line">w &#x3D; tf.Variable(tf.zeros([784,10]))</span><br><span class="line">b &#x3D; tf.Variable(tf.zeros([10]))</span><br><span class="line">prediction &#x3D; tf.nn.softmax(tf.matmul(x,w) + b)</span><br><span class="line"></span><br><span class="line">#二次代价函数</span><br><span class="line">loss &#x3D; tf.reduce_mean(tf.square(y - prediction))</span><br><span class="line"></span><br><span class="line">#使用梯度下降法</span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#初始化变量</span><br><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#定义求准确率的方法,结果存在一个布尔型列表中</span><br><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))</span><br><span class="line">#求准确率</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    for epoch in range(21):</span><br><span class="line">        for batch in range(n_batch):</span><br><span class="line">            batch_xs,batch_ys &#x3D; mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict&#x3D;&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">            </span><br><span class="line">        acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line">        print(&quot;Iter&quot; + str(epoch) + &quot;,Testing Accuracy&quot; + str(acc))</span><br></pre></td></tr></table></figure>
<p>执行结果：20次迭代后，准确率为0.9138</p>
<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><h2 id="Note1-tf-argmax"><a href="#Note1-tf-argmax" class="headerlink" title="Note1    tf.argmax()"></a>Note1    tf.argmax()</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.argmax(input,axis)</span><br></pre></td></tr></table></figure>
<p>含义：根据axis取值的不同返回每行或者每列最大值的索引。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>input</td>
<td>输入的array</td>
</tr>
<tr>
<td>axis</td>
<td>axis=0，将每一列最大元素的所在索引记录下来，最后输出每一列最大元素所在的索引数组。axis=1，将每一行最大元素所在的索引记录下来，最后返回每一行最大元素所在的索引数组。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Note2-tf-cast"><a href="#Note2-tf-cast" class="headerlink" title="Note2    tf.cast()"></a>Note2    tf.cast()</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.cast(x, dtype, name&#x3D;None)</span><br></pre></td></tr></table></figure>
<p>含义：tf.cast()函数的作用是执行 tensorflow 中张量数据类型转换。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>x</td>
<td>待转换的数据（张量）。</td>
</tr>
<tr>
<td>dtype</td>
<td>目标数据类型。</td>
</tr>
<tr>
<td>name</td>
<td>可选参数，定义操作的名称。</td>
</tr>
</tbody>
</table>
</div>
<p>Tensorflow中的数据类型列表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据类型</th>
<th>Python类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>DT_FLOAT</td>
<td>tf.float32</td>
<td>32位浮点数</td>
</tr>
<tr>
<td>DT_DOUBLE</td>
<td>tf.float64</td>
<td>64位浮点数</td>
</tr>
<tr>
<td>DT_INT64</td>
<td>tf.int64</td>
<td>64位有符号整型</td>
</tr>
<tr>
<td>DT_INT32</td>
<td>tf.int32</td>
<td>32位有符号整型</td>
</tr>
<tr>
<td>DT_INT16</td>
<td>tf.int16</td>
<td>16位有符号整型</td>
</tr>
<tr>
<td>DT_INT8</td>
<td>tf.int8</td>
<td>8位有符号整型</td>
</tr>
<tr>
<td>DT_UINT8</td>
<td>tf.uint8</td>
<td>8位无符号整型</td>
</tr>
<tr>
<td>DT_STRING</td>
<td>tf.string</td>
<td>可变长度的字节数组.每一个张量元素都是一个字节数组.</td>
</tr>
<tr>
<td>DT_BOOL</td>
<td>tf.bool</td>
<td>布尔型</td>
</tr>
<tr>
<td>DT_COMPLEX64</td>
<td>tf.complex64</td>
<td>由两个32位浮点数组成的复数:实数和虚数.</td>
</tr>
<tr>
<td>DT_QINT32</td>
<td>tf.qint32</td>
<td>用于量化Ops的32位有符号整型.</td>
</tr>
<tr>
<td>DT_QINT8</td>
<td>tf.qint8</td>
<td>用于量化Ops的8位有符号整型.</td>
</tr>
<tr>
<td>DT_QUINT8</td>
<td>tf.quint8</td>
<td>用于量化Ops的8位无符号整型.</td>
</tr>
</tbody>
</table>
</div>
<h1 id="超参数调整"><a href="#超参数调整" class="headerlink" title="超参数调整"></a>超参数调整</h1><h2 id="只有输入层和输出层"><a href="#只有输入层和输出层" class="headerlink" title="只有输入层和输出层"></a>只有输入层和输出层</h2><div class="table-container">
<table>
<thead>
<tr>
<th>batch_size</th>
<th>learning_rate</th>
<th>accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>32</td>
<td>0.2</td>
<td>0.9236</td>
</tr>
<tr>
<td>64</td>
<td>0.2</td>
<td>0.9185</td>
</tr>
<tr>
<td>128</td>
<td>0.2</td>
<td>0.9107</td>
</tr>
<tr>
<td>16</td>
<td>0.2</td>
<td>0.9272</td>
</tr>
<tr>
<td>8</td>
<td>0.2</td>
<td>0.9310</td>
</tr>
<tr>
<td>4</td>
<td>0.2</td>
<td>0.9308</td>
</tr>
<tr>
<td>8</td>
<td>0.1</td>
<td>0.9301</td>
</tr>
</tbody>
</table>
</div>
<h2 id="增加隐藏层"><a href="#增加隐藏层" class="headerlink" title="增加隐藏层"></a>增加隐藏层</h2><div class="table-container">
<table>
<thead>
<tr>
<th>隐藏层数</th>
<th>隐藏层神经元数</th>
<th>激活函数</th>
<th>batch_size</th>
<th>w</th>
<th>learning_rate</th>
<th>accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>10</td>
<td>tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.937</td>
</tr>
<tr>
<td>1</td>
<td>20</td>
<td>tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.853</td>
</tr>
<tr>
<td>1</td>
<td>30</td>
<td>tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.9638</td>
</tr>
<tr>
<td>1</td>
<td>30</td>
<td>tanh</td>
<td>32</td>
<td>0</td>
<td>0.3</td>
<td>0.961</td>
</tr>
<tr>
<td>1</td>
<td>30</td>
<td>tanh</td>
<td>48</td>
<td>0</td>
<td>0.3</td>
<td>0.962</td>
</tr>
<tr>
<td>1</td>
<td>30</td>
<td>tanh</td>
<td>128</td>
<td>0</td>
<td>0.3</td>
<td>0.962</td>
</tr>
<tr>
<td>1</td>
<td>40</td>
<td>tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.967</td>
</tr>
<tr>
<td>1</td>
<td>50</td>
<td>tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.798</td>
</tr>
<tr>
<td>1</td>
<td>32</td>
<td>tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.962</td>
</tr>
<tr>
<td>2</td>
<td>10 4</td>
<td>tanh tanh</td>
<td>64</td>
<td>0</td>
<td>0.3</td>
<td>0.868</td>
</tr>
<tr>
<td>2</td>
<td>20 10</td>
<td>tanh tanh</td>
<td>64</td>
<td>0</td>
<td>0.6</td>
<td>0.905</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Q1:weight初试化为0，网络由于对称性实际上可以再压缩</strong></p>
]]></content>
      <categories>
        <category>Tensorflow-Coding-Notes</category>
      </categories>
      <tags>
        <tag>Deeplearning</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow-Coding-Notes：非线性逻辑回归</title>
    <url>/2020/10/28/tensorflow-coding-notes-nonlinear-logic-regression/</url>
    <content><![CDATA[<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><h2 id="Note1-linspace-函数功能"><a href="#Note1-linspace-函数功能" class="headerlink" title="Note1    linspace()函数功能"></a>Note1    linspace()函数功能</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x_data &#x3D; np.linspace(-0.5,0.5,10)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">np.linspace(start, stop, num&#x3D;50, endpoint&#x3D;True, retstep&#x3D;False, dtype&#x3D;None)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>start</td>
<td>队列的开始值</td>
</tr>
<tr>
<td>stop</td>
<td>队列的结束值</td>
</tr>
<tr>
<td>num</td>
<td>要生成的样本数，非负数，默认是50</td>
</tr>
<tr>
<td>endpoint</td>
<td>若为True，“stop”是最后的样本；否则“stop”将不会被包含。默认为True</td>
</tr>
<tr>
<td>retstop</td>
<td>若为False，返回等差数列；否则返回array([samples, step])。默认为False</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Note2-newaxis-功能"><a href="#Note2-newaxis-功能" class="headerlink" title="Note2    [:,newaxis]功能"></a>Note2    [:,newaxis]功能</h2><p>执行代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x_data &#x3D; np.linspace(-0.5,0.5,10)</span><br><span class="line">print(x_data.shape)</span><br><span class="line">x_data &#x3D; x_data[:,newaxis]</span><br><span class="line">print(x_data.shape)</span><br></pre></td></tr></table></figure>
<p>输出结果为</p>
<p>(10,)</p>
<p>(10,1)</p>
<p>x_data[:,newaxis]能将原来的数据扩充一个维度。</p>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2><h2 id="Note3-正态分布生成函数"><a href="#Note3-正态分布生成函数" class="headerlink" title="Note3    正态分布生成函数"></a>Note3    正态分布生成函数</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">np.random.normal(loc&#x3D;0.0,scale&#x3D;1.0,size&#x3D;None)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>loc</td>
<td>float</td>
<td>正态分布的均值</td>
</tr>
<tr>
<td>scale</td>
<td>float</td>
<td>正态分布的标准差，对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高</td>
</tr>
<tr>
<td>size</td>
<td>int or tuple of ints</td>
<td>输出的shape，默认为None，只输出一个值</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Note4-y-np-square-x"><a href="#Note4-y-np-square-x" class="headerlink" title="Note4    y = np.square(x)"></a>Note4    y = np.square(x)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y &#x3D; np.square(x)</span><br></pre></td></tr></table></figure>
<p>返回的y与x有相同的shape，其中y的每个元素是x每个元素的平方。</p>
<h2 id="Note5-Tensorflow的占位符"><a href="#Note5-Tensorflow的占位符" class="headerlink" title="Note5 Tensorflow的占位符"></a>Note5 Tensorflow的占位符</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,1])</span><br></pre></td></tr></table></figure>
<p>定义了一个32位浮点型的占位符，shape为[None,1]，行数没有定义，列数定义为1</p>
<p>执行时使用feed_dict{}以字典的形式传入数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sess.run(train_step,feed_dict&#x3D;&#123;x:x_data,y:y_data&#125;)</span><br></pre></td></tr></table></figure>
<p>前面定义了operation，我们运行的时候seess.run()最后一个operation就可以，前面的会自动执行（我的推断）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.placeholder(dtype,shape&#x3D;None,name&#x3D;None)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>dtype</td>
<td>数据类型。常用的是tf.float32,tf.float64等数值类型</td>
</tr>
<tr>
<td>shape</td>
<td>数据形状。默认是None，就是一维值，也可以是多维（比如[2,3], [None, 3]表示列是3，行不定）</td>
</tr>
<tr>
<td>name</td>
<td>名称</td>
</tr>
</tbody>
</table>
</div>
<h1 id="Codes"><a href="#Codes" class="headerlink" title="Codes"></a>Codes</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#使用numpy生成200个随机点</span><br><span class="line">x_data &#x3D; np.linspace(-0.5,0.5,200)[:,np.newaxis]</span><br><span class="line">noise &#x3D; np.random.normal(0,0.02,x_data.shape)</span><br><span class="line">y_data &#x3D; np.square(x_data) + noise</span><br><span class="line"></span><br><span class="line">#定义两个placeholder</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,1])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32,[None,1])</span><br><span class="line"></span><br><span class="line">#定义神经网络中间层</span><br><span class="line">Weights_L1 &#x3D; tf.Variable(tf.random_normal([1,10]))</span><br><span class="line">biases_L1 &#x3D; tf.Variable(tf.zeros([1,10]))</span><br><span class="line">Wx_plus_b_L1 &#x3D; tf.matmul(x,Weights_L1) + biases_L1</span><br><span class="line">L1 &#x3D; tf.nn.tanh(Wx_plus_b_L1)</span><br><span class="line"></span><br><span class="line">#定义神经网络输出层</span><br><span class="line">Weights_L2 &#x3D; tf.Variable(tf.random_normal([10,1]))</span><br><span class="line">biases_L2 &#x3D; tf.Variable(tf.zeros([1,1]))</span><br><span class="line">Wx_plus_b_L2 &#x3D; tf.matmul(L1,Weights_L2) + biases_L2</span><br><span class="line">prediction &#x3D; tf.nn.tanh(Wx_plus_b_L2)</span><br><span class="line"></span><br><span class="line">#二次代价函数</span><br><span class="line">loss &#x3D; tf.reduce_mean(tf.square(y-prediction))</span><br><span class="line">#使用梯度下降法训练</span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.1).minimize(loss)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    #变量初始化</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    for _ in range(2000):</span><br><span class="line">        sess.run(train_step,feed_dict&#x3D;&#123;x:x_data,y:y_data&#125;)</span><br><span class="line">        </span><br><span class="line">    #获得预测值</span><br><span class="line">    prediction_value &#x3D; sess.run(prediction,feed_dict&#x3D;&#123;x:x_data&#125;)</span><br><span class="line">    #画图</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(x_data,y_data)</span><br><span class="line">    plt.plot(x_data,prediction_value,&#39;r-&#39;,lw&#x3D;5)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Tensorflow-Coding-Notes</category>
      </categories>
      <tags>
        <tag>Deeplearning</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow-Coding-Notes:Questions</title>
    <url>/2020/10/29/tensorflow-coding-notes-questions/</url>
    <content><![CDATA[<h6 id="Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。"><a href="#Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。" class="headerlink" title="Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。"></a>Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。</h6><p><img src="/.ink//image-20201029195107148.png" alt="image-20201029195107148" style="zoom:50%;"></p>
<p><img src="/.ink//image-20201029195649017.png" alt="image-20201029195649017" style="zoom:50%;"></p>
<h6 id="A1："><a href="#A1：" class="headerlink" title="A1："></a>A1：</h6><h6 id="Q2：随着迭代次数的增加，准确率在下降（来源于Tensorflow实战：mnist简单分类任务）"><a href="#Q2：随着迭代次数的增加，准确率在下降（来源于Tensorflow实战：mnist简单分类任务）" class="headerlink" title="Q2：随着迭代次数的增加，准确率在下降（来源于Tensorflow实战：mnist简单分类任务）"></a>Q2：随着迭代次数的增加，准确率在下降（来源于Tensorflow实战：mnist简单分类任务）</h6><p><img src="/.ink//image-20201030115021987.png" alt="image-20201030115021987" style="zoom:50%;"></p>
<p>参数：     batch_size = 8    learning_rate=0.2 w=0 b=0</p>
]]></content>
      <categories>
        <category>Tensorflow-Coding-Notes</category>
      </categories>
      <tags>
        <tag>Deeplearning</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>weekly-report-20201030</title>
    <url>/2020/10/30/weekly-report-20201030/</url>
    <content><![CDATA[<h1 id="本周学习报告"><a href="#本周学习报告" class="headerlink" title="本周学习报告"></a>本周学习报告</h1><ol>
<li>这一周主要学习了吴恩达的深度学习课程，他的课程主要是讲了深度学习的一些基本原理和概念。下边的目录是我这一周所看的内容。</li>
<li>另外本周还在Tensorflow上做了一个mnist手写数字的识别练习，代码和网络参数都在后文。</li>
</ol>
<h1 id="DeepLearning"><a href="#DeepLearning" class="headerlink" title="DeepLearning"></a>DeepLearning</h1><h2 id="吴承恩的深度学习课程"><a href="#吴承恩的深度学习课程" class="headerlink" title="吴承恩的深度学习课程"></a>吴承恩的深度学习课程</h2><h3 id="第一门课-神经网络和深度学习-Neural-Networks-and-Deep-Learning"><a href="#第一门课-神经网络和深度学习-Neural-Networks-and-Deep-Learning" class="headerlink" title="第一门课 神经网络和深度学习(Neural Networks and Deep Learning)"></a>第一门课 神经网络和深度学习(Neural Networks and Deep Learning)</h3><p><a href="http://www.ai-start.com/dl2017/html/lesson1-week1.html">第一周：深度学习引言(Introduction to Deep Learning)</a></p>
<p>1.1 欢迎(Welcome) 1</p>
<p>1.2 什么是神经网络？(What is a Neural Network)</p>
<p>1.3 神经网络的监督学习(Supervised Learning with Neural Networks)</p>
<p>1.4 为什么神经网络会流行？(Why is Deep Learning taking off?)</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson1-week2.html">第二周：神经网络的编程基础(Basics of Neural Network programming)</a></p>
<p>2.1 二分类(Binary Classification)</p>
<p>2.2 逻辑回归(Logistic Regression)</p>
<p>2.3 逻辑回归的代价函数（Logistic Regression Cost Function）</p>
<p>2.4 梯度下降（Gradient Descent）</p>
<p>2.5 导数（Derivatives）</p>
<p>2.6 更多的导数例子（More Derivative Examples）</p>
<p>2.7 计算图（Computation Graph）</p>
<p>2.8 计算图导数（Derivatives with a Computation Graph）</p>
<p>2.9 逻辑回归的梯度下降（Logistic Regression Gradient Descent）</p>
<p>2.10 梯度下降的例子(Gradient Descent on m Examples)</p>
<p>2.11 向量化(Vectorization)</p>
<p>2.12 更多的向量化例子（More Examples of Vectorization）</p>
<p>2.13 向量化逻辑回归(Vectorizing Logistic Regression)</p>
<p>2.14 向量化逻辑回归的梯度计算（Vectorizing Logistic Regression’s Gradient）</p>
<p>2.15 Python中的广播机制（Broadcasting in Python）</p>
<p>2.16 关于 Python与numpy向量的使用（A note on python or numpy vectors）</p>
<p>2.17 Jupyter/iPython Notebooks快速入门（Quick tour of Jupyter/iPython Notebooks）</p>
<p>2.18 逻辑回归损失函数详解（Explanation of logistic regression cost function）</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson1-week3.html">第三周：浅层神经网络(Shallow neural networks)</a></p>
<p>3.1 神经网络概述（Neural Network Overview）</p>
<p>3.2 神经网络的表示（Neural Network Representation）</p>
<p>3.3 计算一个神经网络的输出（Computing a Neural Network’s output）</p>
<p>3.4 多样本向量化（Vectorizing across multiple examples）</p>
<p>3.5 向量化实现的解释（Justification for vectorized implementation）</p>
<p>3.6 激活函数（Activation functions）</p>
<p>3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）</p>
<p>3.8 激活函数的导数（Derivatives of activation functions）</p>
<p>3.9 神经网络的梯度下降（Gradient descent for neural networks）</p>
<p>3.10（选修）直观理解反向传播（Backpropagation intuition）</p>
<p>3.11 随机初始化（Random+Initialization）</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson1-week4.html">第四周：深层神经网络(Deep Neural Networks)</a></p>
<p>4.1 深层神经网络（Deep L-layer neural network）</p>
<p>4.2 前向传播和反向传播（Forward and backward propagation）</p>
<p>4.3 深层网络中的前向和反向传播（Forward propagation in a Deep Network）</p>
<p>4.4 核对矩阵的维数（Getting your matrix dimensions right）</p>
<p>4.5 为什么使用深层表示？（Why deep representations?）</p>
<p>4.6 搭建神经网络块（Building blocks of deep neural networks）</p>
<p>4.7 参数VS超参数（Parameters vs Hyperparameters）</p>
<p>4.8 深度学习和大脑的关联性（What does this have to do with the brain?）</p>
<h3 id="第二门课-改善深层神经网络：超参数调试、正则化以及优化-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization"><a href="#第二门课-改善深层神经网络：超参数调试、正则化以及优化-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization" class="headerlink" title="第二门课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)"></a>第二门课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)</h3><p><a href="http://www.ai-start.com/dl2017/html/lesson2-week1.html">第一周：深度学习的实用层面(Practical aspects of Deep Learning)</a></p>
<p>1.1 训练，验证，测试集（Train / Dev / Test sets）</p>
<p>1.2 偏差，方差（Bias /Variance）</p>
<p>1.3 机器学习基础（Basic Recipe for Machine Learning）</p>
<p>1.4 正则化（Regularization）</p>
<p>1.5 为什么正则化有利于预防过拟合呢？（Why regularization reduces overfitting?）</p>
<p>1.6 dropout 正则化（Dropout Regularization）</p>
<p>1.7 理解 dropout（Understanding Dropout）</p>
<p>1.8 其他正则化方法（Other regularization methods）</p>
<p>1.9 标准化输入（Normalizing inputs）</p>
<p>1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）</p>
<p>1.11 神经网络的权重初始化（Weight Initialization for Deep NetworksVanishing /Exploding gradients）</p>
<p>1.12 梯度的数值逼近（Numerical approximation of gradients）</p>
<p>1.13 梯度检验（Gradient checking）</p>
<p>1.14 梯度检验应用的注意事项（Gradient Checking Implementation Notes）</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson2-week2.html">第二周：优化算法 (Optimization algorithms)</a></p>
<p>2.1 Mini-batch 梯度下降（Mini-batch gradient descent）</p>
<p>2.2 理解Mini-batch 梯度下降（Understanding Mini-batch gradient descent）</p>
<p>2.3 指数加权平均（Exponentially weighted averages）</p>
<p>2.4 理解指数加权平均（Understanding Exponentially weighted averages）</p>
<p>2.5 指数加权平均的偏差修正（Bias correction in exponentially weighted averages）</p>
<p>2.6 momentum梯度下降（Gradient descent with momentum）</p>
<p>2.7 RMSprop——root mean square prop（RMSprop）</p>
<p>2.8 Adam优化算法（Adam optimization algorithm）</p>
<p>2.9 学习率衰减（Learning rate decay）</p>
<p>2.10 局部最优问题（The problem of local optima）</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson2-week3.html">第三周超参数调试，batch正则化和程序框架（Hyperparameter tuning, Batch Normalization and Programming Frameworks)</a></p>
<p>3.1 调试处理（Tuning process）</p>
<p>3.2 为超参数选择和适合范围（Using an appropriate scale to pick hyperparameters）</p>
<p>3.3 超参数训练的实践：Pandas vs. Caviar（Hyperparameters tuning in practice: Pandas vs. Caviar）</p>
<p>3.4 网络中的正则化激活函数（Normalizing activations in a network）</p>
<p>3.5 将 Batch Norm拟合进神经网络（Fitting Batch Norm into a neural network）</p>
<p>3.6 为什么Batch Norm奏效？（Why does Batch Norm work?）</p>
<p>3.7 测试时的Batch Norm（Batch Norm at test time）</p>
<p>3.8 Softmax 回归（Softmax Regression）</p>
<p>3.9 训练一个Softmax 分类器（Training a softmax classifier）</p>
<p>3.10 深度学习框架（Deep learning frameworks）</p>
<p>3.11 TensorFlow（TensorFlow）</p>
<h2 id="Tensorflow实战"><a href="#Tensorflow实战" class="headerlink" title="Tensorflow实战"></a>Tensorflow实战</h2><h3 id="非线性回归模型"><a href="#非线性回归模型" class="headerlink" title="非线性回归模型"></a>非线性回归模型</h3><h3 id="mnist手写数字识别"><a href="#mnist手写数字识别" class="headerlink" title="mnist手写数字识别"></a>mnist手写数字识别</h3><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>隐藏层数：                3</p>
<p>隐藏层大小：            200 200 200（3个隐藏层的神经元数目都为200）</p>
<p>droup：                    0.7 0.7 0.7（3个隐藏层的dropout参数均为0.7）</p>
<p>隐藏层激活函数：    tanh tanh tanh（隐藏层均用tanh作为激活函数）</p>
<p>输出层大小：            10</p>
<p>输出层激活函数：    softmax</p>
<p>代价函数：                交叉熵代价函数</p>
<p>梯度下降法：            mini_batch梯度下降法</p>
<p>batch_size：            64</p>
<p>实现结果：在99次迭代训练后，在训练集上识别精度为0.9885，在测试集上识别精度为0.9775</p>
<h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist &#x3D; input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot&#x3D;True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size &#x3D; 64</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch &#x3D; mnist.train.num_examples &#x2F;&#x2F; batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义placeholder，x为输入，y为label</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32,[None,10])</span><br><span class="line"></span><br><span class="line">#定义占位符，dropout的比例，神经元工作的比例</span><br><span class="line">keep_prob &#x3D; tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">#神经网络参数</span><br><span class="line">L1_size &#x3D; 200</span><br><span class="line">L2_size &#x3D; 200</span><br><span class="line">L3_size &#x3D; 200</span><br><span class="line"></span><br><span class="line">#定义神经网络</span><br><span class="line">W1 &#x3D; tf.Variable(tf.truncated_normal([784,L1_size],stddev &#x3D; 0.1))</span><br><span class="line">b1 &#x3D; tf.Variable(tf.zeros([1,L1_size]) + 0.01)</span><br><span class="line">z1 &#x3D; tf.matmul(x,W1) + b1</span><br><span class="line">a1 &#x3D; tf.nn.tanh(z1)</span><br><span class="line">L1_drop &#x3D; tf.nn.dropout(a1,keep_prob)</span><br><span class="line"></span><br><span class="line">W2 &#x3D; tf.Variable(tf.truncated_normal([L1_size,L2_size],stddev&#x3D;0.1))</span><br><span class="line">b2 &#x3D; tf.Variable(tf.zeros([1,L2_size]) + 0.01)</span><br><span class="line">z2 &#x3D; tf.matmul(L1_drop,W2) + b2</span><br><span class="line">a2 &#x3D; tf.nn.tanh(z2)</span><br><span class="line">L2_drop &#x3D; tf.nn.dropout(a2,keep_prob)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W3 &#x3D; tf.Variable(tf.truncated_normal([L2_size,L3_size],stddev&#x3D;0.1))</span><br><span class="line">b3 &#x3D; tf.Variable(tf.zeros([1,L3_size]) + 0.01)</span><br><span class="line">z3 &#x3D; tf.matmul(L2_drop,W3) + b3</span><br><span class="line">a3 &#x3D; tf.nn.tanh(z3)</span><br><span class="line">L3_drop &#x3D; tf.nn.dropout(a3,keep_prob)</span><br><span class="line"></span><br><span class="line">Wout &#x3D; tf.Variable(tf.truncated_normal([L3_size,10],stddev&#x3D;0.1))</span><br><span class="line">bout &#x3D; tf.Variable(tf.zeros([1,10]) + 0.01)</span><br><span class="line">zout &#x3D; tf.matmul(L3_drop,Wout) + bout</span><br><span class="line">prediction &#x3D; tf.nn.softmax(zout)</span><br><span class="line"></span><br><span class="line">#代价函数</span><br><span class="line">loss &#x3D; tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels &#x3D; y,logits &#x3D; prediction))</span><br><span class="line"></span><br><span class="line">#梯度下降法的优化器</span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#变量初试化</span><br><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#求准确度</span><br><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    </span><br><span class="line">    for epoch in range(100):</span><br><span class="line">        for batch in range(n_batch):</span><br><span class="line">            batch_xs,batch_ys &#x3D; mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict&#x3D;&#123;x:batch_xs,y:batch_ys,keep_prob:0.7&#125;)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"> </span><br><span class="line">            </span><br><span class="line">        test_acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;)</span><br><span class="line">        train_acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0&#125;)</span><br><span class="line">        print(&quot;Iter&quot; + str(epoch) + &quot;,Testing Accuracy:&quot; + str(test_acc) + &quot; |  Trainung Accuracy:&quot; + str(train_acc))</span><br></pre></td></tr></table></figure>
<h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><p>本周看论文的任务没有完成。</p>
<h1 id="Verilog"><a href="#Verilog" class="headerlink" title="Verilog"></a>Verilog</h1><p>仿真常用verilog example的任务没有完成。</p>
]]></content>
      <categories>
        <category>Study-Report</category>
      </categories>
      <tags>
        <tag>study-report</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow-Notes:Tensorboard</title>
    <url>/2020/11/02/tensorflow-coding-notes-tensorboard/</url>
    <content><![CDATA[<h1 id="GRAPHS"><a href="#GRAPHS" class="headerlink" title="GRAPHS"></a>GRAPHS</h1><h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><h3 id="Note1-create-namespace"><a href="#Note1-create-namespace" class="headerlink" title="Note1    create namespace"></a>Note1    create namespace</h3><p><strong>eg1.</strong>创建网络Layer1的命名空间</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;layer1&#x27;</span>):</span><br><span class="line">	W1 = tf.Variable(tf.truncated_normal([input_size,L1_size]),name=<span class="string">&#x27;weight_L1&#x27;</span>)</span><br><span class="line">	b1 = tf.Variable(tf.zeros(<span class="number">1</span>,L1_size),name=<span class="string">&#x27;bias_L1&#x27;</span>)</span><br><span class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;wx_plus_b_L1&#x27;</span>):</span><br><span class="line">		z1 = tf.matmul(x,W1) + b1</span><br><span class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;tanh_L1&#x27;</span>):</span><br><span class="line">		a1 = tf.nn.tanh(z1)</span><br><span class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;dropout_L1&#x27;</span>):</span><br><span class="line">		L1_drop = tf.nn.dropout(a1,keep_prob)</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	...</span><br><span class="line">	writer = tf.summary.FileWriter(<span class="string">&#x27;logs/&#x27;</span>,sess.graph)</span><br><span class="line">	...</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="Note2-Tensorboard打开方法"><a href="#Note2-Tensorboard打开方法" class="headerlink" title="Note2    Tensorboard打开方法"></a>Note2    Tensorboard打开方法</h3><p>1、打开命令行，进入log文件所在目录</p>
<p>2、执行代码</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Tensorboard --logdir=logs/</span><br></pre></td></tr></table></figure>
<p>3、用谷歌浏览器打开<a href="http://localhost:6006/">http://localhost:6006/</a></p>
<h1 id="SUMMARIES"><a href="#SUMMARIES" class="headerlink" title="SUMMARIES"></a>SUMMARIES</h1><h2 id="Notes-1"><a href="#Notes-1" class="headerlink" title="Notes"></a>Notes</h2><h3 id="Note1-tf-summary用法总结"><a href="#Note1-tf-summary用法总结" class="headerlink" title="Note1    tf.summary用法总结"></a>Note1    tf.summary用法总结</h3><p><strong>eg1.</strong>记录标量信息</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.summary.scalar(tags, values, collections=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>例如</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.summary.scalar(&#39;loss&#39;,loss)</span><br></pre></td></tr></table></figure>
<p><strong>eg2.</strong>记录直方图，一般用来显示训练过程中变量的分布情况</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.summary.histogram(tags, values, collections=<span class="literal">None</span>, name=<span class="literal">None</span>) </span><br></pre></td></tr></table></figure>
<p>例如</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.summary.histogram(<span class="string">&#x27;histogram&#x27;</span>, var)</span><br></pre></td></tr></table></figure>
<p><strong>eg3.</strong>分布图，一般用于weights分布</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.summary.distribution</span><br></pre></td></tr></table></figure>
<p><strong>eg4.</strong>将文本类型的数据转换为tensor写入summary中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.summary.text</span><br></pre></td></tr></table></figure>
<p>例如</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">text = <span class="string">&quot;&quot;&quot;/a/b/c\\_d/f\\_g\\_h\\_2017&quot;&quot;&quot;</span></span><br><span class="line">summary_op0 = tf.summary.text(<span class="string">&#x27;text&#x27;</span>, tf.convert_to_tensor(text))</span><br></pre></td></tr></table></figure>
<p><strong>eg5.</strong>记录图像</p>
<p>输出带图像的probuf，汇总数据的图像的的形式如下： ‘ tag /image/0’, ‘ tag /image/1’…，如：input/image/0等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.summary.image(tag, tensor, max_images=<span class="number">3</span>, collections=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p><strong>eg5.</strong>记录音频</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.summary.audio</span><br></pre></td></tr></table></figure>
<p><strong>eg6.</strong>merge_all</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.summary.merge_all</span><br></pre></td></tr></table></figure>
<p>merge_all 可以将所有summary全部保存到磁盘，以便tensorboard显示。如果没有特殊要求，一般用这一句就可一显示训练时的各种信息了。</p>
<p><strong>eg7.</strong>merge</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.summary.merge(inputs, collections=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>一般选择要保存的信息还需要用到tf.get_collection()函数</p>
<p>例如</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.summary.scalar(<span class="string">&#x27;accuracy&#x27;</span>,acc)                   <span class="comment">#生成准确率标量图  </span></span><br><span class="line">merge_summary = tf.summary.merge([tf.get_collection(tf.GraphKeys.SUMMARIES,<span class="string">&#x27;accuracy&#x27;</span>),...(其他要显示的信息)])  </span><br><span class="line">train_writer = tf.summary.FileWriter(<span class="built_in">dir</span>,sess.graph)<span class="comment">#定义一个写入summary的目标文件，dir为写入文件地址  </span></span><br><span class="line">......(交叉熵、优化器等定义)  </span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(training_step):                  <span class="comment">#训练循环  </span></span><br><span class="line">    train_summary = sess.run(merge_summary,feed_dict =  &#123;...&#125;)<span class="comment">#调用sess.run运行图，生成一步的训练过程数据  </span></span><br><span class="line">    train_writer.add_summary(train_summary,step)<span class="comment">#调用train_writer的add_summary方法将训练过程以及训练步数保存  </span></span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">acc_summary = tf.summary.scalar(<span class="string">&#x27;accuracy&#x27;</span>,acc)                   <span class="comment">#生成准确率标量图  </span></span><br><span class="line">merge_summary = tf.summary.merge([acc_summary ,...(其他要显示的信息)])  <span class="comment">#这里的[]不可省</span></span><br></pre></td></tr></table></figure>
<p><strong>eg8</strong>.保存图</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.summary.FileWriter</span><br></pre></td></tr></table></figure>
<p>可以调用其add_summary（）方法将训练过程数据保存在filewriter指定的文件中。</p>
<p>例如</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.summary.scalar(<span class="string">&#x27;accuracy&#x27;</span>,acc)                   <span class="comment">#生成准确率标量图  </span></span><br><span class="line">merge_summary = tf.summary.merge_all()  </span><br><span class="line">train_writer = tf.summary.FileWriter(<span class="built_in">dir</span>,sess.graph)<span class="comment">#定义一个写入summary的目标文件，dir为写入文件地址  </span></span><br><span class="line">......(交叉熵、优化器等定义)  </span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(training_step):                  <span class="comment">#训练循环  </span></span><br><span class="line">    train_summary = sess.run(merge_summary,feed_dict =  &#123;...&#125;)<span class="comment">#调用sess.run运行图，生成一步的训练过程数据  </span></span><br><span class="line">    train_writer.add_summary(train_summary,step)<span class="comment">#调用train_writer的add_summary方法将训练过程以及训练步数保存 </span></span><br></pre></td></tr></table></figure>
<p>如果要在tensorboard中画多个数据图，需定义多个tf.summary.FileWriter并重复上述过程。</p>
<h2 id="Codes"><a href="#Codes" class="headerlink" title="Codes"></a>Codes</h2><h3 id="Code1-定义一个函数来记录变量的suammry"><a href="#Code1-定义一个函数来记录变量的suammry" class="headerlink" title="Code1    定义一个函数来记录变量的suammry"></a>Code1    定义一个函数来记录变量的suammry</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span>(<span class="params">var</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;summaries&#x27;</span>):</span><br><span class="line">        mean = tf.reduce_mean(var)</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;mean&#x27;</span>,mean)<span class="comment">#平均值</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;stddev&#x27;</span>):</span><br><span class="line">            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;stddev&#x27;</span>,stddev)<span class="comment">#标准差</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;max&#x27;</span>,tf.reduce_max(var))<span class="comment">#最大值</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;min&#x27;</span>,tf.reduce_min(var))<span class="comment">#最小值</span></span><br><span class="line">        tf.summary.histogram(<span class="string">&#x27;histogram&#x27;</span>,var)<span class="comment">#直方图</span></span><br></pre></td></tr></table></figure>
<h3 id="Code2-用法"><a href="#Code2-用法" class="headerlink" title="Code2    用法"></a>Code2    用法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span>(<span class="params">var</span>):</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;summaries&#x27;</span>):</span><br><span class="line">        mean = tf.reduce_mean(var)</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;mean&#x27;</span>,mean)<span class="comment">#平均值</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;stddev&#x27;</span>):</span><br><span class="line">            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;stddev&#x27;</span>,stddev)<span class="comment">#标准差</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;max&#x27;</span>,tf.reduce_max(var))<span class="comment">#最大值</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;min&#x27;</span>,tf.reduce_min(var))<span class="comment">#最小值</span></span><br><span class="line">        tf.summary.histogram(<span class="string">&#x27;histogram&#x27;</span>,var)<span class="comment">#直方图</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;layer1&#x27;</span>):</span><br><span class="line">	W1 = tf.Variable(tf.truncated_normal([input_size,L1_size]),name=<span class="string">&#x27;weight_L1&#x27;</span>)</span><br><span class="line">	variable_summaries(W1)</span><br><span class="line">	b1 = tf.Variable(tf.zeros(<span class="number">1</span>,L1_size),name=<span class="string">&#x27;bias_L1&#x27;</span>)</span><br><span class="line">    variable_summaries(b1)</span><br><span class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;wx_plus_b_L1&#x27;</span>):</span><br><span class="line">		z1 = tf.matmul(x,W1) + b1</span><br><span class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;tanh_L1&#x27;</span>):</span><br><span class="line">		a1 = tf.nn.tanh(z1)</span><br><span class="line">	<span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;dropout_L1&#x27;</span>):</span><br><span class="line">		L1_drop = tf.nn.dropout(a1,keep_prob)</span><br><span class="line">...</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;loss&#x27;</span>):</span><br><span class="line">    loss = tf.nn.softmax_cross_entropy_with_logits(labels = y,logits = prediction)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;loss&#x27;</span>,loss)</span><br><span class="line">...</span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line">...</span><br><span class="line"><span class="comment">#创建计算会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">&#x27;logs/&#x27;</span>,sess.graph)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">51</span>):</span><br><span class="line">        sess.run(tf.assign(lr,lr * (<span class="number">0.95</span> ** epoch)))</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(n_batch):</span><br><span class="line">            batch_xs,batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            summary,_ = sess.run([merged,train_step],feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">            </span><br><span class="line">        writer.add_summary(summary,epoch)    </span><br><span class="line">        train_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        test_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">&quot;Iter&quot;</span> + <span class="built_in">str</span>(epoch) + <span class="string">&quot;,  |  Training Accuracy:&quot;</span> + <span class="built_in">str</span>(train_acc) + <span class="string">&quot;,  |  Testing Accuracy:&quot;</span> + <span class="built_in">str</span>(test_acc) + <span class="string">&quot;,  |  Learning Rate:&quot;</span> + <span class="built_in">str</span>(sess.run(lr)))</span><br><span class="line">        </span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<h3 id="Code3-MNIST"><a href="#Code3-MNIST" class="headerlink" title="Code3    MNIST"></a>Code3    MNIST</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&quot;MNIST_data&quot;</span>,one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span>(<span class="params">var</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;summaries&#x27;</span>):</span><br><span class="line">        mean = tf.reduce_mean(var)</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;mean&#x27;</span>,mean)<span class="comment">#平均值</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;stddev&#x27;</span>):</span><br><span class="line">            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;stddev&#x27;</span>,stddev)<span class="comment">#标准差</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;max&#x27;</span>,tf.reduce_max(var))<span class="comment">#最大值</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;min&#x27;</span>,tf.reduce_min(var))<span class="comment">#最小值</span></span><br><span class="line">        tf.summary.histogram(<span class="string">&#x27;histogram&#x27;</span>,var)<span class="comment">#直方图</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">#batch初试化</span></span><br><span class="line"><span class="comment">#每个批次的大小</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line"><span class="comment">#计算一共有多少个批次</span></span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;input&#x27;</span>):</span><br><span class="line">    <span class="comment">#定义placeholder</span></span><br><span class="line">    x = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>],name=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    y = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>],name=<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;parameter&#x27;</span>):</span><br><span class="line">    keep_prob = tf.placeholder(tf.float32,name=<span class="string">&#x27;keep_prob&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;network&#x27;</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;network_parameter&#x27;</span>):</span><br><span class="line">        <span class="comment">#定义网络</span></span><br><span class="line">        <span class="comment">#定义学习率</span></span><br><span class="line">        lr = tf.Variable(<span class="number">0.01</span>,name=<span class="string">&#x27;lr&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#网络参数</span></span><br><span class="line">        L1_size = <span class="number">500</span></span><br><span class="line">        L2_size = <span class="number">300</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;layer1&#x27;</span>):</span><br><span class="line">        <span class="comment">#L1</span></span><br><span class="line">        W1 = tf.Variable(tf.truncated_normal([<span class="number">784</span>,L1_size],stddev = <span class="number">0.1</span>),name = <span class="string">&#x27;weight_L1&#x27;</span>)</span><br><span class="line">        variable_summaries(W1)</span><br><span class="line">        b1 = tf.Variable(tf.zeros([<span class="number">1</span>,L1_size]) + <span class="number">0.1</span>,name = <span class="string">&#x27;bias_L1&#x27;</span>)</span><br><span class="line">        variable_summaries(b1)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;wx_plus_b_L1&#x27;</span>):</span><br><span class="line">            z1 = tf.matmul(x,W1) + b1</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;tanh_L1&#x27;</span>):</span><br><span class="line">            a1 = tf.nn.tanh(z1)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;dropout_L1&#x27;</span>):</span><br><span class="line">            L1_drop = tf.nn.dropout(a1,keep_prob)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;layer2&#x27;</span>):</span><br><span class="line">        <span class="comment">#L2</span></span><br><span class="line">        W2 = tf.Variable(tf.truncated_normal([L1_size,L2_size],stddev = <span class="number">0.1</span>),name=<span class="string">&#x27;weight_L2&#x27;</span>)</span><br><span class="line">        variable_summaries(W2)</span><br><span class="line">        b2 = tf.Variable(tf.zeros([<span class="number">1</span>,L2_size]) + <span class="number">0.1</span>,name=<span class="string">&#x27;bias_L2&#x27;</span>)</span><br><span class="line">        variable_summaries(b2)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;wx_plus_b_L2&#x27;</span>):</span><br><span class="line">            z2 = tf.matmul(L1_drop,W2) + b2</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;tanh_L2&#x27;</span>):</span><br><span class="line">            a2 = tf.nn.tanh(z2)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;dropout_L2&#x27;</span>):</span><br><span class="line">            L2_drop = tf.nn.dropout(a2,keep_prob)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;output_layer&#x27;</span>):</span><br><span class="line">        <span class="comment">#Lout</span></span><br><span class="line">        Wout = tf.Variable(tf.truncated_normal([L2_size,<span class="number">10</span>],stddev = <span class="number">0.1</span>),name=<span class="string">&#x27;weight_Lout&#x27;</span>)</span><br><span class="line">        variable_summaries(Wout)</span><br><span class="line">        bout = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">10</span>]) + <span class="number">0.1</span>,name=<span class="string">&#x27;bias_Lout&#x27;</span>)</span><br><span class="line">        variable_summaries(bout)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;wx_plus_b_Lout&#x27;</span>):</span><br><span class="line">            zout = tf.matmul(L2_drop,Wout) + bout</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;softmax_Lout&#x27;</span>):</span><br><span class="line">            prediction = tf.nn.softmax(zout)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;loss&#x27;</span>):</span><br><span class="line">    <span class="comment">#训练优化</span></span><br><span class="line">    <span class="comment">#代价函数</span></span><br><span class="line">    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction))</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;loss&#x27;</span>,loss)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">    <span class="comment">#训练：优化器</span></span><br><span class="line">    train_step = tf.train.AdamOptimizer(lr).minimize(loss)</span><br><span class="line">    <span class="comment"># train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)</span></span><br><span class="line">    <span class="comment">#初试化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;accuracy&#x27;</span>):</span><br><span class="line">    <span class="comment">#计算准确率</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;correct_prediction&#x27;</span>):</span><br><span class="line">        <span class="comment">#比较label和logit位置中最大值的位置是否相同，结果存在布尔型列表中</span></span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>),tf.argmax(prediction,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;accuracy_calcalate&#x27;</span>):</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;accuracy&#x27;</span>,accuracy)</span><br><span class="line"></span><br><span class="line"><span class="comment">#合并所有的summary</span></span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建计算会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">&#x27;logs/&#x27;</span>,sess.graph)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">51</span>):</span><br><span class="line">        sess.run(tf.assign(lr,lr * (<span class="number">0.95</span> ** epoch)))</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(n_batch):</span><br><span class="line">            batch_xs,batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            summary,_ = sess.run([merged,train_step],feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">            </span><br><span class="line">        writer.add_summary(summary,epoch)    </span><br><span class="line">        train_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        test_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">&quot;Iter&quot;</span> + <span class="built_in">str</span>(epoch) + <span class="string">&quot;,  |  Training Accuracy:&quot;</span> + <span class="built_in">str</span>(train_acc) + <span class="string">&quot;,  |  Testing Accuracy:&quot;</span> + <span class="built_in">str</span>(test_acc) + <span class="string">&quot;,  |  Learning Rate:&quot;</span> + <span class="built_in">str</span>(sess.run(lr)))</span><br><span class="line">        </span><br><span class="line">        </span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Tensorflow-Coding-Notes</category>
      </categories>
      <tags>
        <tag>Deeplearning</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>考研：英语阅读</title>
    <url>/2020/11/05/postgraduate-qualifying-examination-english-reading-comprehension/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1 id="考研阅读技巧"><a href="#考研阅读技巧" class="headerlink" title="考研阅读技巧"></a>考研阅读技巧</h1><h2 id="一、考研阅读的整体解题思路与步骤"><a href="#一、考研阅读的整体解题思路与步骤" class="headerlink" title="一、考研阅读的整体解题思路与步骤"></a>一、考研阅读的整体解题思路与步骤</h2><h3 id="时间分配："><a href="#时间分配：" class="headerlink" title="时间分配："></a>时间分配：</h3><p>每篇16分钟最佳，最多可延长至20分钟左右。</p>
<h3 id="第一步：快速划出段落序号以及各段首句-0-5分钟"><a href="#第一步：快速划出段落序号以及各段首句-0-5分钟" class="headerlink" title="第一步：快速划出段落序号以及各段首句                    0.5分钟"></a>第一步：快速划出段落序号以及各段首句                    0.5分钟</h3><h3 id="第二步：阅读首段，了解文章主题（Theme）-1-2分钟"><a href="#第二步：阅读首段，了解文章主题（Theme）-1-2分钟" class="headerlink" title="第二步：阅读首段，了解文章主题（Theme）                 1-2分钟"></a>第二步：阅读首段，了解文章主题（Theme）                 1-2分钟</h3><p>宏观把握，随机应变，根据第一段内容，才能更好的给定位打下基础。</p>
<p>首段一般都会给出这篇文章所要论述的事情，读懂了首段，你就知道文章要将什么事。</p>
<h3 id="第三步：扫描题干，尽量找出题干能够提供的信息（Key-Words）-1分钟"><a href="#第三步：扫描题干，尽量找出题干能够提供的信息（Key-Words）-1分钟" class="headerlink" title="第三步：扫描题干，尽量找出题干能够提供的信息（Key Words）   1分钟"></a>第三步：扫描题干，尽量找出题干能够提供的信息（Key Words）   1分钟</h3><p>定位词的优先考虑顺序: </p>
<p>1、首先标出明确告诉位置的题目所在（某段某行）</p>
<p>2、专有名词优先，包括人名、地名、书名以及带引号的词等</p>
<p>3、数字、时间、时段（包括某些介词短语）</p>
<p>4、较长、较复杂的词组（名词动词词组优先）</p>
<p>5、重要的动词、形容词或副词等实词</p>
<p>6、条件词、因果词、比较词等虚词（往往起到辅助作用）</p>
<h3 id="第四步：变速浏览原文，抓住中心-7-8分钟"><a href="#第四步：变速浏览原文，抓住中心-7-8分钟" class="headerlink" title="第四步：变速浏览原文，抓住中心        7-8分钟"></a>第四步：变速浏览原文，抓住中心        7-8分钟</h3><p>注意把握三个阅读原则：</p>
<p>原则一：首段原则：首段一般都会给出这篇文章所要论述的事情，读懂了首段，你就知道文章要讲什么事。</p>
<p>原则二：首末句原则：在段尾最后一句话不出现转折词的情况下，一般段落首句就是这个段落的中心，小标题。这整个段落几乎都是围绕着段落首句来（举例子）解释说明。</p>
<p>原则三：“路标”原则。所谓路标词，就是表示作者思想衔接和转折的功能词汇。</p>
<p>1、中心词    2、转折词    3、态度词    4、例证词    5、列举词</p>
<h3 id="第五步：仔细审题，定位原文-3-5分钟"><a href="#第五步：仔细审题，定位原文-3-5分钟" class="headerlink" title="第五步：仔细审题，定位原文      3-5分钟"></a>第五步：仔细审题，定位原文      3-5分钟</h3><p>原则一：关键词定位原则</p>
<p>原则二：自然段定位原则</p>
<p>原则三：长难句定位原则</p>
<p>注意一：关键词在原文可能是原词本身，也可能是关键词的同义词。</p>
<p>注意二：问原因的问题，一般问主要原因（major reason） </p>
<p>注意三：“邪恶的眼睛”（in the eyes of），注意问的是谁的观点和态度。</p>
<h3 id="第六步：重叠选项，斟酌答案。-3-5分钟"><a href="#第六步：重叠选项，斟酌答案。-3-5分钟" class="headerlink" title="第六步：重叠选项，斟酌答案。      3-5分钟"></a>第六步：重叠选项，斟酌答案。      3-5分钟</h3><p>原则：不能仅凭借印象做题，考研阅读的干扰项干扰性巨大，除了理解原文，分辨正确和错误的选项也是一种重要的基本功。一般对原文进行同义替换的是答案：</p>
<p>同义替换的手段有：</p>
<p>a）关键词替换 </p>
<p>b）句型替换 </p>
<p>c）正话反说 </p>
<p>d）语言简化（总之，换汤不换药！） </p>
<p>原则一：选最佳答案而不是正确答案（四个选项都要认真看，不能偏心）（whq：用排除法做题）。</p>
<p>原则二：不放过任何一个选项，仔细读每个选项，鉴于强干扰性，要求必须记住：选一个选项应有选的理由，不选一个选项也应有不选的理由。</p>
<p>原则三：每个选项都当成生命中最重要的句子，其中每个单词哪怕是时态、冠词都有可能引起错误。（whq:觉得两个选项都正确，找不到错误的点再来看这个）</p>
<h2 id="二、考研阅读的猜题技巧及救命法则"><a href="#二、考研阅读的猜题技巧及救命法则" class="headerlink" title="二、考研阅读的猜题技巧及救命法则"></a>二、考研阅读的猜题技巧及救命法则</h2><h3 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h3><p>主要掌握上边的内容，猜题还是没有自己会做靠谱，靠猜题可以解决一两个不会的。</p>
<h3 id="1、“体现中心思想的选项往往是答案”"><a href="#1、“体现中心思想的选项往往是答案”" class="headerlink" title="1、“体现中心思想的选项往往是答案”"></a>1、“体现中心思想的选项往往是答案”</h3><p>考研文章中所有的细节、例子、引语都是为了说明文章主旨、段落主旨，所以考细节的题目，虽然不是主旨题，但能体现中心思想的选项是答案的可能性要远远大于其他选项。</p>
<h3 id="2、“不看文章时，看似极其合理的选项不是答案；看似不太合理的选项往往是答案”"><a href="#2、“不看文章时，看似极其合理的选项不是答案；看似不太合理的选项往往是答案”" class="headerlink" title="2、“不看文章时，看似极其合理的选项不是答案；看似不太合理的选项往往是答案”"></a>2、“不看文章时，看似极其合理的选项不是答案；看似不太合理的选项往往是答案”</h3><p>四个选择项中若有一个对问题而言、根据常识看似不合理，而其他几项根据常识都可合理地作为问题的答案，此时若我们没看懂文中意思，我们可以大胆猜测这个看似不合理的选项是答案。作为答案的选项看起来不合理的原因可能是其中某个词不是常用意义，若根据其通常意义理解则该选项看起来当然不合理，也可能是一些特定的理由，无论是哪种情况，都有助于使试题具有难度。</p>
<h3 id="3、“完全照抄原文的选项往往不是答案；和原文作关键词同义替换的选项往往是答案”"><a href="#3、“完全照抄原文的选项往往不是答案；和原文作关键词同义替换的选项往往是答案”" class="headerlink" title="3、“完全照抄原文的选项往往不是答案；和原文作关键词同义替换的选项往往是答案”"></a>3、“完全照抄原文的选项往往不是答案；和原文作关键词同义替换的选项往往是答案”</h3><p>在设计题目时，为了迷惑考生，命题者一般都会将原文中的内容换个说法作为正确答案的选项，而将某些与题干问题不符的原文原话生拉硬扯过来作为干扰项，或将原文中的某些句子做了细微的改变引诱考生上当，所以当原文中最变态最生涩的词汇在选项中出现的时候，该选项往往是错误答案。</p>
<h3 id="4、“语气委婉不绝对的选项往往是答案”"><a href="#4、“语气委婉不绝对的选项往往是答案”" class="headerlink" title="4、“语气委婉不绝对的选项往往是答案”"></a>4、“语气委婉不绝对的选项往往是答案”</h3><p>选项中的might等词语可以表达某种委婉、中庸、不肯定的语气，为表达的观点留有余地，而含义肯定的词语则使得句意有些绝对、没有余地。凡事都不能太绝对，考研阅读中含义不肯定的标志有：can，could，probably，may，might，be likely to，most，more or less，relatively，assumably，ordinarily，presumedly，about，approximately，almost，nearly，perhaps等。含有这些词的备选项是答案的可能性很大。</p>
<h3 id="5、“语气过于绝对的选项往往不是答案”"><a href="#5、“语气过于绝对的选项往往不是答案”" class="headerlink" title="5、“语气过于绝对的选项往往不是答案”"></a>5、“语气过于绝对的选项往往不是答案”</h3><p>凡事不可太绝对，所以含义中庸、折中、不肯定、不确定的选项是答案的可能性要大于含义绝的选项。表示一种绝对含义的词语有：must，always，never, the most(最高级)，all, only，any，none，entirely，utterly，by all means，all to nothing，to a certainty，necessary，dispensable，indispensable，certainly，undoubtedly，definitely，surely等。也就是说，选项中含有上述词语时，是答案的可能性较小。 </p>
<h3 id="6、“含义具体、肤浅涉及到例子的选项往往不是答案；含义概括、抽象涉及中心大意的选项往往是答案”"><a href="#6、“含义具体、肤浅涉及到例子的选项往往不是答案；含义概括、抽象涉及中心大意的选项往往是答案”" class="headerlink" title="6、“含义具体、肤浅涉及到例子的选项往往不是答案；含义概括、抽象涉及中心大意的选项往往是答案”"></a>6、“含义具体、肤浅涉及到例子的选项往往不是答案；含义概括、抽象涉及中心大意的选项往往是答案”</h3><p>有些题目的四个备选项中，有些备选项的意思过于具体或者肤浅，往往涉及到了例子中的某些特殊词汇，那么这样的选项往往都不是答案，考研出题老师一般希望大家能够选择深刻、概括、抽象的更有份量的选项为正确答案。</p>
<h3 id="7、“含有某种、某些、某人的不确定的指代含义的选项往往是答案”"><a href="#7、“含有某种、某些、某人的不确定的指代含义的选项往往是答案”" class="headerlink" title="7、“含有某种、某些、某人的不确定的指代含义的选项往往是答案”"></a>7、“含有某种、某些、某人的不确定的指代含义的选项往往是答案”</h3><p>这类词汇例如：some，someone，somebody，something，certain，somewhat，sometime，somewhere </p>
<h3 id="8、“含义丰富的小词往往是答案”"><a href="#8、“含义丰富的小词往往是答案”" class="headerlink" title="8、“含义丰富的小词往往是答案”"></a>8、“含义丰富的小词往往是答案”</h3><p>特别是一些形容词，副词，介词。他们本身并没有什么意思，但句子中加了这些词，含义会更加丰富，更加隐蔽，对付考生非常有效，而且这些词往往作答案，可称之为“虚词型的答案”。(1)不考本身。但要考：another另一个，other剩下的，more更多的，earlier早点的，1ater晚点的，besides除此之外，：additional额外的，extra多余的，eventually最终的 (2)不考一般的，而要考：especially特别的，differently不同的，particularly特殊的 (3)不考完全的，而要考：“nearly; almost (4)不考具体的，而考概括的：either, both, also, as well考生碰到此问题要加倍小心。 </p>
<h3 id="9、“含有表示‘发展变化’含义的选项往往是答案”"><a href="#9、“含有表示‘发展变化’含义的选项往往是答案”" class="headerlink" title="9、“含有表示‘发展变化’含义的选项往往是答案”"></a>9、“含有表示‘发展变化’含义的选项往往是答案”</h3><p> 阅读理解常常以一件事发生了变化为命题对象，所以“表示变化的选项是答案”，表示变化的时间语主要有：change，delay，improve，postpone，increase，alter，decrease，decline，expand，develop，grow，evolution，progress，influence，transfer，transform等。</p>
<h3 id="10、“含有表示‘重要’含义的选项往往是答案”"><a href="#10、“含有表示‘重要’含义的选项往往是答案”" class="headerlink" title="10、“含有表示‘重要’含义的选项往往是答案”"></a>10、“含有表示‘重要’含义的选项往往是答案”</h3><p>表示“重要”的词主要有：important、necessity、 essential、necessary、crucial、critical、fatal、main、concernful、momentous、significant、staple、vital </p>
<h1 id="考研英语阅读题型总结"><a href="#考研英语阅读题型总结" class="headerlink" title="考研英语阅读题型总结"></a>考研英语阅读题型总结</h1><h2 id="一、主旨题"><a href="#一、主旨题" class="headerlink" title="一、主旨题"></a>一、主旨题</h2><p>(考察理解文中具体信息和概念性的含义的能力）</p>
<ul>
<li>识别：题干中出现：subject，summary，topic，title等表达方式的为主题句</li>
<li>实质：对论点和论题提问</li>
<li>解题方法：</li>
</ul>
<p>寻找主题句，主题句通常出现在文章首段首句，或出现在文章开头的转折处或文章开头结束处</p>
<p>主题句特征：主题句通常是一个概括总结性的结论或者判断</p>
<p>寻找主题词：主题句首段末段或全文中多次出现</p>
<p>解题原则：正确选项不能描述太细节，不能包含无依据的信息，应该包含主题词或同义替换词</p>
<p>优先考虑议论文的标题</p>
<h2 id="二、例子证明题-（主要考察区分论点和论据的能力）"><a href="#二、例子证明题-（主要考察区分论点和论据的能力）" class="headerlink" title="二、例子证明题 （主要考察区分论点和论据的能力）"></a>二、例子证明题 （主要考察区分论点和论据的能力）</h2><ul>
<li>识别：题干中出现example，case，illustrate等词</li>
<li>解题思路：例子为观点和结论服务，寻找到例子对应观点和结论，通常往上或往下寻找</li>
<li>错误选项特征：就事论事，自我总结 </li>
</ul>
<h2 id="三、推理题"><a href="#三、推理题" class="headerlink" title="三、推理题"></a>三、推理题</h2><ul>
<li><p>识别：题干中出现infer，learn,conclude等词</p>
</li>
<li><p>分类</p>
<p>a）细节性的推理题(题干中包含具体的定位信息）</p>
<p>​        理解文中具体信息和概念性的含义的能力</p>
<p>b）段落性的推理题（题干中包含具体段落）</p>
<p>​        理解文中单句之间，段落之间关系的能力，进行有关的判断，推断和引申的能力</p>
<p>c）全文性的推理题（题干中包含主体词或无定位信息）</p>
<p>​        理解文章总体结构的能力</p>
</li>
<li><p>常考出题点：段落首末段，主题句，观点句，转折处，强调或递进关系的地方</p>
</li>
<li><p>实质和解题原则：考研推理题本质上还是一种同义改写，推理通常为正反推理和归纳总结</p>
</li>
</ul>
<p>解题原则：重在推理原文依据，特别关注转折，选择答案方面，主体大于细节，观点大于论据（意思就是选择原文中对应的总结性句子，而不是论据）</p>
<h2 id="四、细节题：题干中不包含提起题型特征的题为细节题"><a href="#四、细节题：题干中不包含提起题型特征的题为细节题" class="headerlink" title="四、细节题：题干中不包含提起题型特征的题为细节题"></a>四、细节题：题干中不包含提起题型特征的题为细节题</h2><ol>
<li>（考察理解稳重的具体信息和概念性的能力，理解文章的总体解雇以及单句之间，段落之间的关系能力）<ol>
<li>事实识别：问题中出现文章相关的具体信息，可以用相对明显的本文词汇定位</li>
<li>因果关系：问题中除了有相对具体的定位信息词外，还有表示因果关系的词汇，要重点把握</li>
<li>观点识别：与观点结论有关，通常有suggest，found等引导的宾语从句</li>
<li>which题型：问题中没有具体的定位词只出现which提问</li>
</ol>
</li>
</ol>
<p>解题思路：</p>
<ol>
<li>识别题型 </li>
<li>定位：寻找题干定位词（具体的定位词，因果词，观点词，比较词，原文词汇的替换词）与包含定位词的句子。</li>
<li>读取：<ol>
<li>分析线索句主干与其他各项的对比（表达方式不同，意思最为接近的为正确选项）</li>
<li>必要时需要分析线索句的上一句和下一句（支持句）</li>
<li>当线索句为段落首末段时，支持句为段落的其他句子</li>
</ol>
</li>
<li>注意事项：顺序原则（出题顺序和行文顺序基本一致） 段落原则（一个段落通常只出现一个细节题，细节题通常不跨段（除非段落间存在指代或明显的逻辑关系）</li>
</ol>
<h2 id="五、词汇题"><a href="#五、词汇题" class="headerlink" title="五、词汇题"></a>五、词汇题</h2><p>(考察上下文推测词义的能力）</p>
<p>a.识别：要求对题干中的某个单词，词组或句子的含义进行推测</p>
<p>b.实质：通过上下文确定单词含义</p>
<p>c.解题流程：</p>
<ul>
<li>返回原文确定题干位置</li>
<li>根据上下文推测含义</li>
<li>代入原文，确定答案</li>
</ul>
<p>d.解题原则：正：并列（并列连句，语意并列），解释（定于从句，标点符号，同位语，因果），列举举例 反：转折和否定</p>
<p>e.注意事项：很多时候单词的字面意思都不是答案，一定要代入验证，尽量寻找上下文对应点</p>
<h2 id="六、态度题"><a href="#六、态度题" class="headerlink" title="六、态度题"></a>六、态度题</h2><p>（考察作者态度，意图和观点）</p>
<p>a.态度的三要素：主体，对象，表达</p>
<p>b.常见的表示态度的单词：</p>
<p>indifferent，detached，indignant,contempt,bias,</p>
<p>optimistic,pessimistic,skeptical,consent,resent,</p>
<p>concerned,reserved,opposition,suspicion,approval,</p>
<p>subjective,biased,impartial,sensitive</p>
<p>c.态度题的分类：全文态度题（主体为作者，对象为文章主题)和局部态度题</p>
<p>d.态度题的解题思路：</p>
<ul>
<li>确定题干中的态度主体和对象</li>
<li>返回原文寻找包含主体和对象的句子（优先考虑包含主体的句子）</li>
<li>想原文中的态度词语选项相对照，同义互换的为正确答案</li>
</ul>
<h1 id="考研英语做题顺序及策略"><a href="#考研英语做题顺序及策略" class="headerlink" title="考研英语做题顺序及策略"></a>考研英语做题顺序及策略</h1><p> 总3个小时180分钟</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>顺序</th>
<th>题型</th>
<th>时长</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>小作文</td>
<td>10-15min</td>
<td>严格控制字数100</td>
</tr>
<tr>
<td>2</td>
<td>大作文</td>
<td>25-30min</td>
<td>字数200注意卷面</td>
</tr>
<tr>
<td>3</td>
<td>阅读4篇</td>
<td>20*4 = 80min</td>
<td>防止连续错误</td>
</tr>
<tr>
<td>4</td>
<td>新题型1篇</td>
<td>20min</td>
<td>容易连着出错</td>
</tr>
<tr>
<td>5</td>
<td>翻译1篇</td>
<td>20min</td>
<td>最后要看看剩多少时间，然后看先做翻译还是完型</td>
</tr>
<tr>
<td>6</td>
<td>完型1篇</td>
<td>10min</td>
<td>尽量做，剩下全猜</td>
</tr>
</tbody>
</table>
</div>
<p>1、 先搞定大小作文，确保最终能够完成全部试题。</p>
<p>2、 阅读理解起伏大，能决定最后英语成绩的档次。</p>
<p>3、 新题型讲方法和技巧的做。</p>
<p>4、 翻译一定要做，但是不要求拿满分，但是要得分。</p>
<p>5、 完型快速的做一遍，不会的跳过或者蒙一个选项跳过，最后把不会的空着的蒙一个选项。</p>
]]></content>
      <categories>
        <category>Postgraduate-Qualifying-Examination</category>
      </categories>
  </entry>
  <entry>
    <title>weekly-report-20201106</title>
    <url>/2020/11/06/weekly-report-20201106/</url>
    <content><![CDATA[<h1 id="本周学习内容报告"><a href="#本周学习内容报告" class="headerlink" title="本周学习内容报告"></a>本周学习内容报告</h1><p>本周学习了吴恩达的深度学习课程，主要包含的内容有机器学习的一些方法论，卷积神经网络的详解。另外就是进行了Tensorflow的编程练习，实现了卷积神经网络来对MNIST数据集的手写数字进行识别，经过调试，测试集的识别准确率达到了98%以上。并在Tensorflow上对经典的LeNet-5网络进行了实现，没有调试出比较好的训练结果。</p>
<h1 id="Tensorflow实战"><a href="#Tensorflow实战" class="headerlink" title="Tensorflow实战"></a>Tensorflow实战</h1><p>1、学会了在Tensorflow中利用dropout来减小方差，防止过拟合。</p>
<p>2、在MNIST手写数字识别网络中对比了不同的优化器的效果：GradientDescentOptimizer和AdamOptimizer，对比了均方误差和交叉熵作为误差函数时的效果。</p>
<p>3、学会了Tensorboard的使用。</p>
<p>4、构建卷积神经网路来对MNIST手写数字进行识别，训练集准确率：0.9918，测试集准确率：0.9832。</p>
<p>5、实现经典的LeNet-5进行MNIST手写数字的识别，没有调试出比较高的准确率。</p>
<h1 id="吴恩达的深度学习理论课程——本周学习内容目录"><a href="#吴恩达的深度学习理论课程——本周学习内容目录" class="headerlink" title="吴恩达的深度学习理论课程——本周学习内容目录"></a>吴恩达的深度学习理论课程——本周学习内容目录</h1><h2 id="第三门课-结构化机器学习项目-Structuring-Machine-Learning-Projects"><a href="#第三门课-结构化机器学习项目-Structuring-Machine-Learning-Projects" class="headerlink" title="第三门课 结构化机器学习项目 (Structuring Machine Learning Projects)"></a>第三门课 结构化机器学习项目 (Structuring Machine Learning Projects)</h2><p><a href="http://www.ai-start.com/dl2017/html/lesson3-week1.html">第一周：机器学习策略（1）(ML Strategy (1))</a></p>
<p>1.1 为什么是ML策略？ (Why ML Strategy)</p>
<p>1.2 正交化(Orthogonalization)</p>
<p>1.3 单一数字评估指标(Single number evaluation metric)</p>
<p>1.4 满足和优化指标 (Satisficing and Optimizing metric)</p>
<p>1.5 训练集、开发集、测试集的划分(Train/dev/test distributions)</p>
<p>1.6 开发集和测试集的大小 (Size of the dev and test sets)</p>
<p>1.7 什么时候改变开发集/测试集和评估指标(When to change dev/test sets and metrics)</p>
<p>1.8 为什么是人的表现 (Why human-level performance?)</p>
<p>1.9 可避免偏差(Avoidable bias)</p>
<p>1.10 理解人类的表现 (Understanding human-level performance)</p>
<p>1.11 超过人类的表现(Surpassing human-level performance)</p>
<p>1.12 改善你的模型表现 (Improving your model performance)</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson3-week2.html">第二周：机器学习策略（2）(ML Strategy (2))</a></p>
<p>2.1 误差分析 (Carrying out error analysis)</p>
<p>2.2 清除标注错误的数据(Cleaning up incorrectly labeled data)</p>
<p>2.3 快速搭建你的第一个系统，并进行迭代(Build your first system quickly, then iterate)</p>
<p>2.4 在不同的分布上的训练集和测试集 (Training and testing on different distributions)</p>
<p>2.5 数据分布不匹配的偏差与方差分析 (Bias and Variance with mismatched data distributions)</p>
<p>2.6 处理数据不匹配问题(Addressing data mismatch)</p>
<p>2.7 迁移学习 (Transfer learning)</p>
<p>2.8 多任务学习(Multi-task learning)</p>
<p>2.9 什么是端到端的深度学习？ (What is end-to-end deep learning?)</p>
<p>2.10 是否使用端到端的深度学习方法 (Whether to use end-to-end deep learning)</p>
<h2 id="第四门课-卷积神经网络（Convolutional-Neural-Networks）"><a href="#第四门课-卷积神经网络（Convolutional-Neural-Networks）" class="headerlink" title="第四门课 卷积神经网络（Convolutional Neural Networks）"></a>第四门课 卷积神经网络（Convolutional Neural Networks）</h2><p><a href="http://www.ai-start.com/dl2017/html/lesson4-week1.html">第一周 卷积神经网络(Foundations of Convolutional Neural Networks)</a></p>
<p>2.1.1 计算机视觉（Computer vision）</p>
<p>1.2 边缘检测示例（Edge detection example）</p>
<p>1.3 更多边缘检测内容（More edge detection）</p>
<p>1.4 Padding</p>
<p>1.5 卷积步长（Strided convolutions）</p>
<p>1.6 三维卷积（Convolutions over volumes）</p>
<p>1.7 单层卷积网络（One layer of a convolutional network）</p>
<p>1.8 简单卷积网络示例（A simple convolution network example）</p>
<p>1.9 池化层（Pooling layers）</p>
<p>1.10 卷积神经网络示例（Convolutional neural network example）</p>
<p>1.11 为什么使用卷积？（Why convolutions?）</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson4-week2.html">第二周 深度卷积网络：实例探究(Deep convolutional models: case studies)</a></p>
<p>2.1 为什么要进行实例探究？（Why look at case studies?）</p>
<p>2.2 经典网络（Classic networks）</p>
<p>2.3 残差网络（Residual Networks (ResNets)）</p>
<p>2.4 残差网络为什么有用？（Why ResNets work?）</p>
<p>2.5 网络中的网络以及 1×1 卷积（Network in Network and 1×1 convolutions）</p>
<p>2.6 谷歌 Inception 网络简介（Inception network motivation）</p>
<p>2.7 Inception 网络（Inception network）</p>
<p>2.8 使用开源的实现方案（Using open-source implementations）</p>
<p>2.9 迁移学习（Transfer Learning）</p>
<p>2.10 数据扩充（Data augmentation）</p>
<p>2.11 计算机视觉现状（The state of computer vision）</p>
]]></content>
      <categories>
        <category>Study-Report</category>
      </categories>
  </entry>
  <entry>
    <title>weekly-report-20201113</title>
    <url>/2020/11/09/weekly-report-20201113/</url>
    <content><![CDATA[<h1 id="学习内容"><a href="#学习内容" class="headerlink" title="学习内容"></a>学习内容</h1><p>1、学习了循环神经网络模型，对其前向传播和反向传播进行了公式推导。对不同的RNN网络模型，比如一对一，一对多，多对一，多对多有了一个概念上的了解。</p>
<p>2、学习了RNN语言模型，知道语音识别系统怎样去选择一个正确的句子，从而提高语音识别的准确性。</p>
<p>3、学习了GRU单元和LSTM单元来解决长时间序列RNN网络中梯度消失的问题。GRU和LSTM相比，GRU更加简单，容易构建大规模的网络，运算速度也更快；LSTM更加强大和灵活。</p>
<p>4、学习了双向RNN网络模型，不仅能够获取之前的信息，还可以获取未来的信息进行预测。例如有这样两个句子，</p>
<p>①He said,”Teddy bears are on sale!”</p>
<p>②He said,”Teddy Roosevelt was a great Present!”</p>
<p>如果我们仅用前向的RNN网络来判断句子中出现的人名，那么在这个地方的第三个时间节点，两个句子给到前向网络的信息都是 He said Teddy，但是这个地方放一个是Teedy bear，一个是Teddy Roosevelt，只有句子②的Teddy是人民，也就是说，这个时候单向的RNN网络不能正确的预测结果。那么如果我们采用双向的RNN网络，那么我们不论在何时刻都能获取到整个句子的信息，能够更加准确的进行推断。</p>
<p>5、利用Tensorflow在MNIST手写字识别数据集上对RNN网络进行了实现，测试集上识别准确率达到0.979，代码如下：</p>
<p>该网络一共有28个lstm单元，每个28×28的图片分为28次输入网络，每次输入图片的一行。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#载入数据集</span><br><span class="line">mnist &#x3D; input_data.read_data_sets(&quot;MNIST_data&#x2F;&quot;,one_hot &#x3D; True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">n_inputs &#x3D; 28</span><br><span class="line">max_time &#x3D; 28</span><br><span class="line">lstm_size &#x3D; 100</span><br><span class="line">n_classes &#x3D; 10</span><br><span class="line">batch_size &#x3D; 50</span><br><span class="line">n_batch &#x3D; mnist.train.num_examples &#x2F;&#x2F; batch_size</span><br><span class="line"></span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32,[None,10])</span><br><span class="line"></span><br><span class="line">#RNN network</span><br><span class="line">weights &#x3D; tf.Variable(tf.truncated_normal([lstm_size,n_classes],stddev&#x3D;0.1))</span><br><span class="line">biases &#x3D; tf.Variable(tf.constant(0.1,shape&#x3D;[n_classes]))</span><br><span class="line">inputs &#x3D; tf.reshape(x,[-1,max_time,n_inputs])</span><br><span class="line">lstm_cell &#x3D; tf.nn.rnn_cell.BasicLSTMCell(lstm_size)</span><br><span class="line"># final_state[0] cell state</span><br><span class="line"># final_state[1] hidden state</span><br><span class="line">outputs,final_state &#x3D; tf.nn.dynamic_rnn(lstm_cell,inputs,dtype&#x3D;tf.float32)</span><br><span class="line">prediction &#x3D; tf.nn.softmax(tf.matmul(final_state[1],weights)+biases)</span><br><span class="line"></span><br><span class="line">cross_entropy &#x3D; tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits&#x3D;prediction,labels&#x3D;y))</span><br><span class="line">optimizer &#x3D; tf.train.AdamOptimizer(1e-4)</span><br><span class="line">train_step &#x3D; optimizer.minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line">#结果存放在一个布尔型列表中</span><br><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))</span><br><span class="line"></span><br><span class="line">#求准确率</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction,tf.float32))#把correct_prediction变为float32类型</span><br><span class="line">#初始化</span><br><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    for epoch in range(60):</span><br><span class="line">        for batch in range(n_batch):</span><br><span class="line">            batch_xs,batch_ys &#x3D;  mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict&#x3D;&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">        </span><br><span class="line">        acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line">        print (&quot;Iter &quot; + str(epoch) + &quot;, Testing Accuracy&#x3D; &quot; + str(acc))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>6、对Verilog经典例程进行了编程和仿真练习。</p>
<ol>
<li><a href="http://www.asic-world.com/examples/verilog/encoder.html#Encoders">Encoders</a><ol>
<li><a href="http://www.asic-world.com/examples/verilog/encoder.html#Encoder_-_Using_if-else_Statement">Encoder - Using if-else Statement</a></li>
<li><a href="http://www.asic-world.com/examples/verilog/encoder.html#Encoder_-_Using_case_Statement"> Encoder - Using case Statement</a></li>
</ol>
</li>
<li><a href="http://www.asic-world.com/examples/verilog/pri_encoder.html#Priority_Encoders"> Priority Encoders</a><ol>
<li><a href="http://www.asic-world.com/examples/verilog/pri_encoder.html#Pri-Encoder_-_Using_if-else_Statement">Pri-Encoder - Using if-else Statement</a></li>
<li><a href="http://www.asic-world.com/examples/verilog/pri_encoder.html#Encoder_-_Using_assign_Statement">Encoder - Using assign Statement</a></li>
</ol>
</li>
<li><a href="http://www.asic-world.com/examples/verilog/decoder.html#Decoders">Decoders</a><ol>
<li><a href="http://www.asic-world.com/examples/verilog/decoder.html#Decoder_-_Using_case_Statement"> Decoder - Using case Statement</a></li>
<li><a href="http://www.asic-world.com/examples/verilog/decoder.html#Decoder_-_Using_assign_Statement">Decoder - Using assign Statement</a></li>
</ol>
</li>
<li><a href="http://www.asic-world.com/examples/verilog/mux.html">Mux</a></li>
</ol>
]]></content>
      <categories>
        <category>Study-Report</category>
      </categories>
  </entry>
  <entry>
    <title>Verilog Coding Notes:经典例程</title>
    <url>/2020/11/09/verilog-coding-notes/</url>
    <content><![CDATA[<h1 id="Decoder-And-Encoders"><a href="#Decoder-And-Encoders" class="headerlink" title="Decoder And Encoders"></a>Decoder And Encoders</h1>]]></content>
      <tags>
        <tag>ASIC</tag>
        <tag>Verilog</tag>
      </tags>
  </entry>
  <entry>
    <title>ecg_cnn_classfication</title>
    <url>/2020/12/04/ecg-cnn-classfication/</url>
    <content><![CDATA[]]></content>
  </entry>
</search>
