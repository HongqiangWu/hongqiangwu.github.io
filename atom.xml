<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>邬</title>
  
  <subtitle>the blog of Wu</subtitle>
  <link href="http://blog.too.ink/atom.xml" rel="self"/>
  
  <link href="http://blog.too.ink/"/>
  <updated>2020-11-02T12:04:57.426Z</updated>
  <id>http://blog.too.ink/</id>
  
  <author>
    <name>邬</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>tensorflow-coding-cnn</title>
    <link href="http://blog.too.ink/2020/11/02/tensorflow-coding-cnn/"/>
    <id>http://blog.too.ink/2020/11/02/tensorflow-coding-cnn/</id>
    <published>2020-11-02T12:04:57.000Z</published>
    <updated>2020-11-02T12:04:57.426Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Tensorflow-Notes:Tensorboard</title>
    <link href="http://blog.too.ink/2020/11/02/tensorflow-coding-tensorboard/"/>
    <id>http://blog.too.ink/2020/11/02/tensorflow-coding-tensorboard/</id>
    <published>2020-11-02T01:43:31.000Z</published>
    <updated>2020-11-02T12:08:27.460Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GRAPHS"><a href="#GRAPHS" class="headerlink" title="GRAPHS"></a>GRAPHS</h1><h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><h3 id="Note1-create-namespace"><a href="#Note1-create-namespace" class="headerlink" title="Note1    create namespace"></a>Note1    create namespace</h3><p><strong>eg1.</strong>创建网络Layer1的命名空间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;layer1&#x27;</span>):</span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([input_size,L1_size]),name=<span class="string">&#x27;weight_L1&#x27;</span>)</span><br><span class="line">b1 = tf.Variable(tf.zeros(<span class="number">1</span>,L1_size),name=<span class="string">&#x27;bias_L1&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;wx_plus_b_L1&#x27;</span>):</span><br><span class="line">z1 = tf.matmul(x,W1) + b1</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;tanh_L1&#x27;</span>):</span><br><span class="line">a1 = tf.nn.tanh(z1)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;dropout_L1&#x27;</span>):</span><br><span class="line">L1_drop = tf.nn.dropout(a1,keep_prob)</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">...</span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">&#x27;logs/&#x27;</span>,sess.graph)</span><br><span class="line">...</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="Note2-Tensorboard打开方法"><a href="#Note2-Tensorboard打开方法" class="headerlink" title="Note2    Tensorboard打开方法"></a>Note2    Tensorboard打开方法</h3><p>1、打开命令行，进入log文件所在目录</p><p>2、执行代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensorboard --logdir=logs/</span><br></pre></td></tr></table></figure><p>3、用谷歌浏览器打开<a href="http://localhost:6006/">http://localhost:6006/</a></p><h1 id="SUMMARIES"><a href="#SUMMARIES" class="headerlink" title="SUMMARIES"></a>SUMMARIES</h1><h2 id="Notes-1"><a href="#Notes-1" class="headerlink" title="Notes"></a>Notes</h2><h3 id="Note1-tf-summary用法总结"><a href="#Note1-tf-summary用法总结" class="headerlink" title="Note1    tf.summary用法总结"></a>Note1    tf.summary用法总结</h3><p><strong>eg1.</strong>记录标量信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.scalar(tags, values, collections=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>例如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.scalar(&#39;loss&#39;,loss)</span><br></pre></td></tr></table></figure><p><strong>eg2.</strong>记录直方图，一般用来显示训练过程中变量的分布情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.histogram(tags, values, collections=<span class="literal">None</span>, name=<span class="literal">None</span>) </span><br></pre></td></tr></table></figure><p>例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.histogram(<span class="string">&#x27;histogram&#x27;</span>, var)</span><br></pre></td></tr></table></figure><p><strong>eg3.</strong>分布图，一般用于weights分布</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.distribution</span><br></pre></td></tr></table></figure><p><strong>eg4.</strong>将文本类型的数据转换为tensor写入summary中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.text</span><br></pre></td></tr></table></figure><p>例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">&quot;&quot;&quot;/a/b/c\\_d/f\\_g\\_h\\_2017&quot;&quot;&quot;</span></span><br><span class="line">summary_op0 = tf.summary.text(<span class="string">&#x27;text&#x27;</span>, tf.convert_to_tensor(text))</span><br></pre></td></tr></table></figure><p><strong>eg5.</strong>记录图像</p><p>输出带图像的probuf，汇总数据的图像的的形式如下： ‘ tag /image/0’, ‘ tag /image/1’…，如：input/image/0等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.image(tag, tensor, max_images=<span class="number">3</span>, collections=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p><strong>eg5.</strong>记录音频</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.audio</span><br></pre></td></tr></table></figure><p><strong>eg6.</strong>merge_all</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.merge_all</span><br></pre></td></tr></table></figure><p>merge_all 可以将所有summary全部保存到磁盘，以便tensorboard显示。如果没有特殊要求，一般用这一句就可一显示训练时的各种信息了。</p><p><strong>eg7.</strong>merge</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.merge(inputs, collections=<span class="literal">None</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>一般选择要保存的信息还需要用到tf.get_collection()函数</p><p>例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.scalar(<span class="string">&#x27;accuracy&#x27;</span>,acc)                   <span class="comment">#生成准确率标量图  </span></span><br><span class="line">merge_summary = tf.summary.merge([tf.get_collection(tf.GraphKeys.SUMMARIES,<span class="string">&#x27;accuracy&#x27;</span>),...(其他要显示的信息)])  </span><br><span class="line">train_writer = tf.summary.FileWriter(<span class="built_in">dir</span>,sess.graph)<span class="comment">#定义一个写入summary的目标文件，dir为写入文件地址  </span></span><br><span class="line">......(交叉熵、优化器等定义)  </span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(training_step):                  <span class="comment">#训练循环  </span></span><br><span class="line">    train_summary = sess.run(merge_summary,feed_dict =  &#123;...&#125;)<span class="comment">#调用sess.run运行图，生成一步的训练过程数据  </span></span><br><span class="line">    train_writer.add_summary(train_summary,step)<span class="comment">#调用train_writer的add_summary方法将训练过程以及训练步数保存  </span></span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">acc_summary = tf.summary.scalar(<span class="string">&#x27;accuracy&#x27;</span>,acc)                   <span class="comment">#生成准确率标量图  </span></span><br><span class="line">merge_summary = tf.summary.merge([acc_summary ,...(其他要显示的信息)])  <span class="comment">#这里的[]不可省</span></span><br></pre></td></tr></table></figure><p><strong>eg8</strong>.保存图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.FileWriter</span><br></pre></td></tr></table></figure><p>可以调用其add_summary（）方法将训练过程数据保存在filewriter指定的文件中。</p><p>例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.scalar(<span class="string">&#x27;accuracy&#x27;</span>,acc)                   <span class="comment">#生成准确率标量图  </span></span><br><span class="line">merge_summary = tf.summary.merge_all()  </span><br><span class="line">train_writer = tf.summary.FileWriter(<span class="built_in">dir</span>,sess.graph)<span class="comment">#定义一个写入summary的目标文件，dir为写入文件地址  </span></span><br><span class="line">......(交叉熵、优化器等定义)  </span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(training_step):                  <span class="comment">#训练循环  </span></span><br><span class="line">    train_summary = sess.run(merge_summary,feed_dict =  &#123;...&#125;)<span class="comment">#调用sess.run运行图，生成一步的训练过程数据  </span></span><br><span class="line">    train_writer.add_summary(train_summary,step)<span class="comment">#调用train_writer的add_summary方法将训练过程以及训练步数保存 </span></span><br></pre></td></tr></table></figure><p>如果要在tensorboard中画多个数据图，需定义多个tf.summary.FileWriter并重复上述过程。</p><h2 id="Codes"><a href="#Codes" class="headerlink" title="Codes"></a>Codes</h2><h3 id="Code1-定义一个函数来记录变量的suammry"><a href="#Code1-定义一个函数来记录变量的suammry" class="headerlink" title="Code1    定义一个函数来记录变量的suammry"></a>Code1    定义一个函数来记录变量的suammry</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span>(<span class="params">var</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;summaries&#x27;</span>):</span><br><span class="line">        mean = tf.reduce_mean(var)</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;mean&#x27;</span>,mean)<span class="comment">#平均值</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;stddev&#x27;</span>):</span><br><span class="line">            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;stddev&#x27;</span>,stddev)<span class="comment">#标准差</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;max&#x27;</span>,tf.reduce_max(var))<span class="comment">#最大值</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;min&#x27;</span>,tf.reduce_min(var))<span class="comment">#最小值</span></span><br><span class="line">        tf.summary.histogram(<span class="string">&#x27;histogram&#x27;</span>,var)<span class="comment">#直方图</span></span><br></pre></td></tr></table></figure><h3 id="Code2-用法"><a href="#Code2-用法" class="headerlink" title="Code2    用法"></a>Code2    用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span>(<span class="params">var</span>):</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;summaries&#x27;</span>):</span><br><span class="line">        mean = tf.reduce_mean(var)</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;mean&#x27;</span>,mean)<span class="comment">#平均值</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;stddev&#x27;</span>):</span><br><span class="line">            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;stddev&#x27;</span>,stddev)<span class="comment">#标准差</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;max&#x27;</span>,tf.reduce_max(var))<span class="comment">#最大值</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;min&#x27;</span>,tf.reduce_min(var))<span class="comment">#最小值</span></span><br><span class="line">        tf.summary.histogram(<span class="string">&#x27;histogram&#x27;</span>,var)<span class="comment">#直方图</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;layer1&#x27;</span>):</span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([input_size,L1_size]),name=<span class="string">&#x27;weight_L1&#x27;</span>)</span><br><span class="line">variable_summaries(W1)</span><br><span class="line">b1 = tf.Variable(tf.zeros(<span class="number">1</span>,L1_size),name=<span class="string">&#x27;bias_L1&#x27;</span>)</span><br><span class="line">    variable_summaries(b1)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;wx_plus_b_L1&#x27;</span>):</span><br><span class="line">z1 = tf.matmul(x,W1) + b1</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;tanh_L1&#x27;</span>):</span><br><span class="line">a1 = tf.nn.tanh(z1)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;dropout_L1&#x27;</span>):</span><br><span class="line">L1_drop = tf.nn.dropout(a1,keep_prob)</span><br><span class="line">...</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;loss&#x27;</span>):</span><br><span class="line">    loss = tf.nn.softmax_cross_entropy_with_logits(labels = y,logits = prediction)</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;loss&#x27;</span>,loss)</span><br><span class="line">...</span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line">...</span><br><span class="line"><span class="comment">#创建计算会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">&#x27;logs/&#x27;</span>,sess.graph)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">51</span>):</span><br><span class="line">        sess.run(tf.assign(lr,lr * (<span class="number">0.95</span> ** epoch)))</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(n_batch):</span><br><span class="line">            batch_xs,batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            summary,_ = sess.run([merged,train_step],feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">            </span><br><span class="line">        writer.add_summary(summary,epoch)    </span><br><span class="line">        train_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        test_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">&quot;Iter&quot;</span> + <span class="built_in">str</span>(epoch) + <span class="string">&quot;,  |  Training Accuracy:&quot;</span> + <span class="built_in">str</span>(train_acc) + <span class="string">&quot;,  |  Testing Accuracy:&quot;</span> + <span class="built_in">str</span>(test_acc) + <span class="string">&quot;,  |  Learning Rate:&quot;</span> + <span class="built_in">str</span>(sess.run(lr)))</span><br><span class="line">        </span><br><span class="line">        </span><br></pre></td></tr></table></figure><h3 id="Code3-MNIST"><a href="#Code3-MNIST" class="headerlink" title="Code3    MNIST"></a>Code3    MNIST</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&quot;MNIST_data&quot;</span>,one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_summaries</span>(<span class="params">var</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;summaries&#x27;</span>):</span><br><span class="line">        mean = tf.reduce_mean(var)</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;mean&#x27;</span>,mean)<span class="comment">#平均值</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;stddev&#x27;</span>):</span><br><span class="line">            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;stddev&#x27;</span>,stddev)<span class="comment">#标准差</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;max&#x27;</span>,tf.reduce_max(var))<span class="comment">#最大值</span></span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;min&#x27;</span>,tf.reduce_min(var))<span class="comment">#最小值</span></span><br><span class="line">        tf.summary.histogram(<span class="string">&#x27;histogram&#x27;</span>,var)<span class="comment">#直方图</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">#batch初试化</span></span><br><span class="line"><span class="comment">#每个批次的大小</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line"><span class="comment">#计算一共有多少个批次</span></span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;input&#x27;</span>):</span><br><span class="line">    <span class="comment">#定义placeholder</span></span><br><span class="line">    x = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">784</span>],name=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    y = tf.placeholder(tf.float32,[<span class="literal">None</span>,<span class="number">10</span>],name=<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;parameter&#x27;</span>):</span><br><span class="line">    keep_prob = tf.placeholder(tf.float32,name=<span class="string">&#x27;keep_prob&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;network&#x27;</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;network_parameter&#x27;</span>):</span><br><span class="line">        <span class="comment">#定义网络</span></span><br><span class="line">        <span class="comment">#定义学习率</span></span><br><span class="line">        lr = tf.Variable(<span class="number">0.01</span>,name=<span class="string">&#x27;lr&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#网络参数</span></span><br><span class="line">        L1_size = <span class="number">500</span></span><br><span class="line">        L2_size = <span class="number">300</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;layer1&#x27;</span>):</span><br><span class="line">        <span class="comment">#L1</span></span><br><span class="line">        W1 = tf.Variable(tf.truncated_normal([<span class="number">784</span>,L1_size],stddev = <span class="number">0.1</span>),name = <span class="string">&#x27;weight_L1&#x27;</span>)</span><br><span class="line">        variable_summaries(W1)</span><br><span class="line">        b1 = tf.Variable(tf.zeros([<span class="number">1</span>,L1_size]) + <span class="number">0.1</span>,name = <span class="string">&#x27;bias_L1&#x27;</span>)</span><br><span class="line">        variable_summaries(b1)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;wx_plus_b_L1&#x27;</span>):</span><br><span class="line">            z1 = tf.matmul(x,W1) + b1</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;tanh_L1&#x27;</span>):</span><br><span class="line">            a1 = tf.nn.tanh(z1)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;dropout_L1&#x27;</span>):</span><br><span class="line">            L1_drop = tf.nn.dropout(a1,keep_prob)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;layer2&#x27;</span>):</span><br><span class="line">        <span class="comment">#L2</span></span><br><span class="line">        W2 = tf.Variable(tf.truncated_normal([L1_size,L2_size],stddev = <span class="number">0.1</span>),name=<span class="string">&#x27;weight_L2&#x27;</span>)</span><br><span class="line">        variable_summaries(W2)</span><br><span class="line">        b2 = tf.Variable(tf.zeros([<span class="number">1</span>,L2_size]) + <span class="number">0.1</span>,name=<span class="string">&#x27;bias_L2&#x27;</span>)</span><br><span class="line">        variable_summaries(b2)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;wx_plus_b_L2&#x27;</span>):</span><br><span class="line">            z2 = tf.matmul(L1_drop,W2) + b2</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;tanh_L2&#x27;</span>):</span><br><span class="line">            a2 = tf.nn.tanh(z2)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;dropout_L2&#x27;</span>):</span><br><span class="line">            L2_drop = tf.nn.dropout(a2,keep_prob)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;output_layer&#x27;</span>):</span><br><span class="line">        <span class="comment">#Lout</span></span><br><span class="line">        Wout = tf.Variable(tf.truncated_normal([L2_size,<span class="number">10</span>],stddev = <span class="number">0.1</span>),name=<span class="string">&#x27;weight_Lout&#x27;</span>)</span><br><span class="line">        variable_summaries(Wout)</span><br><span class="line">        bout = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">10</span>]) + <span class="number">0.1</span>,name=<span class="string">&#x27;bias_Lout&#x27;</span>)</span><br><span class="line">        variable_summaries(bout)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;wx_plus_b_Lout&#x27;</span>):</span><br><span class="line">            zout = tf.matmul(L2_drop,Wout) + bout</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;softmax_Lout&#x27;</span>):</span><br><span class="line">            prediction = tf.nn.softmax(zout)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;loss&#x27;</span>):</span><br><span class="line">    <span class="comment">#训练优化</span></span><br><span class="line">    <span class="comment">#代价函数</span></span><br><span class="line">    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction))</span><br><span class="line">    tf.summary.scalar(<span class="string">&#x27;loss&#x27;</span>,loss)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">    <span class="comment">#训练：优化器</span></span><br><span class="line">    train_step = tf.train.AdamOptimizer(lr).minimize(loss)</span><br><span class="line">    <span class="comment"># train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)</span></span><br><span class="line">    <span class="comment">#初试化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;accuracy&#x27;</span>):</span><br><span class="line">    <span class="comment">#计算准确率</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;correct_prediction&#x27;</span>):</span><br><span class="line">        <span class="comment">#比较label和logit位置中最大值的位置是否相同，结果存在布尔型列表中</span></span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>),tf.argmax(prediction,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;accuracy_calcalate&#x27;</span>):</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;accuracy&#x27;</span>,accuracy)</span><br><span class="line"></span><br><span class="line"><span class="comment">#合并所有的summary</span></span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建计算会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">&#x27;logs/&#x27;</span>,sess.graph)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">51</span>):</span><br><span class="line">        sess.run(tf.assign(lr,lr * (<span class="number">0.95</span> ** epoch)))</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(n_batch):</span><br><span class="line">            batch_xs,batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            summary,_ = sess.run([merged,train_step],feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">            </span><br><span class="line">        writer.add_summary(summary,epoch)    </span><br><span class="line">        train_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        test_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">&quot;Iter&quot;</span> + <span class="built_in">str</span>(epoch) + <span class="string">&quot;,  |  Training Accuracy:&quot;</span> + <span class="built_in">str</span>(train_acc) + <span class="string">&quot;,  |  Testing Accuracy:&quot;</span> + <span class="built_in">str</span>(test_acc) + <span class="string">&quot;,  |  Learning Rate:&quot;</span> + <span class="built_in">str</span>(sess.run(lr)))</span><br><span class="line">        </span><br><span class="line">        </span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Tensorflow-Notes:Tensorboard</summary>
    
    
    
    <category term="Tensorflow-Notes" scheme="http://blog.too.ink/categories/Tensorflow-Notes/"/>
    
    
    <category term="Tensorflow" scheme="http://blog.too.ink/tags/Tensorflow/"/>
    
    <category term="Deeplearning" scheme="http://blog.too.ink/tags/Deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow-Coding-Notes:dropout在Tensorflow中的实现</title>
    <link href="http://blog.too.ink/2020/11/01/tensorflow-coding-dropout/"/>
    <id>http://blog.too.ink/2020/11/01/tensorflow-coding-dropout/</id>
    <published>2020-11-01T07:32:38.000Z</published>
    <updated>2020-11-01T07:50:12.006Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><h2 id="Note1-tf-nn-dropout"><a href="#Note1-tf-nn-dropout" class="headerlink" title="Note1 tf.nn.dropout()"></a>Note1 tf.nn.dropout()</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.dropout(x, keep_prob, noise_shape&#x3D;None, seed&#x3D;None,name&#x3D;None)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keep_prob &#x3D; tf.placeholder(tf.float32)</span><br></pre></td></tr></table></figure><h1 id="Codes"><a href="#Codes" class="headerlink" title="Codes"></a>Codes</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist &#x3D; input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot&#x3D;True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size &#x3D; 64</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch &#x3D; mnist.train.num_examples &#x2F;&#x2F; batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义placeholder，x为输入，y为label</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32,[None,10])</span><br><span class="line"></span><br><span class="line">#定义占位符，dropout的比例，神经元工作的比例</span><br><span class="line">keep_prob &#x3D; tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">#神经网络参数</span><br><span class="line">L1_size &#x3D; 200</span><br><span class="line">L2_size &#x3D; 200</span><br><span class="line">L3_size &#x3D; 200</span><br><span class="line"></span><br><span class="line">#定义神经网络</span><br><span class="line">W1 &#x3D; tf.Variable(tf.truncated_normal([784,L1_size],stddev &#x3D; 0.1))</span><br><span class="line">b1 &#x3D; tf.Variable(tf.zeros([1,L1_size]) + 0.01)</span><br><span class="line">z1 &#x3D; tf.matmul(x,W1) + b1</span><br><span class="line">a1 &#x3D; tf.nn.tanh(z1)</span><br><span class="line">L1_drop &#x3D; tf.nn.dropout(a1,keep_prob)</span><br><span class="line"></span><br><span class="line">W2 &#x3D; tf.Variable(tf.truncated_normal([L1_size,L2_size],stddev&#x3D;0.1))</span><br><span class="line">b2 &#x3D; tf.Variable(tf.zeros([1,L2_size]) + 0.01)</span><br><span class="line">z2 &#x3D; tf.matmul(L1_drop,W2) + b2</span><br><span class="line">a2 &#x3D; tf.nn.tanh(z2)</span><br><span class="line">L2_drop &#x3D; tf.nn.dropout(a2,keep_prob)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W3 &#x3D; tf.Variable(tf.truncated_normal([L2_size,L3_size],stddev&#x3D;0.1))</span><br><span class="line">b3 &#x3D; tf.Variable(tf.zeros([1,L3_size]) + 0.01)</span><br><span class="line">z3 &#x3D; tf.matmul(L2_drop,W3) + b3</span><br><span class="line">a3 &#x3D; tf.nn.tanh(z3)</span><br><span class="line">L3_drop &#x3D; tf.nn.dropout(a3,keep_prob)</span><br><span class="line"></span><br><span class="line">Wout &#x3D; tf.Variable(tf.truncated_normal([L3_size,10],stddev&#x3D;0.1))</span><br><span class="line">bout &#x3D; tf.Variable(tf.zeros([1,10]) + 0.01)</span><br><span class="line">zout &#x3D; tf.matmul(L3_drop,Wout) + bout</span><br><span class="line">prediction &#x3D; tf.nn.softmax(zout)</span><br><span class="line"></span><br><span class="line">#代价函数</span><br><span class="line">loss &#x3D; tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels &#x3D; y,logits &#x3D; prediction))</span><br><span class="line"></span><br><span class="line">#梯度下降法的优化器</span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#变量初试化</span><br><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#求准确度</span><br><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    </span><br><span class="line">    for epoch in range(100):</span><br><span class="line">        for batch in range(n_batch):</span><br><span class="line">            batch_xs,batch_ys &#x3D; mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict&#x3D;&#123;x:batch_xs,y:batch_ys,keep_prob:0.7&#125;)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"> </span><br><span class="line">            </span><br><span class="line">        test_acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;)</span><br><span class="line">        train_acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0&#125;)</span><br><span class="line">        print(&quot;Iter&quot; + str(epoch) + &quot;,Testing Accuracy:&quot; + str(test_acc) + &quot; |  Trainung Accuracy:&quot; + str(train_acc))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">dropout在Tensorflow中的实现方法。</summary>
    
    
    
    <category term="Tensorflow-Coding-Notes" scheme="http://blog.too.ink/categories/Tensorflow-Coding-Notes/"/>
    
    
    <category term="Tensorflow" scheme="http://blog.too.ink/tags/Tensorflow/"/>
    
    <category term="Deeplearning" scheme="http://blog.too.ink/tags/Deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>Optimizer优化器</title>
    <link href="http://blog.too.ink/2020/10/31/deeplearning-optimizer/"/>
    <id>http://blog.too.ink/2020/10/31/deeplearning-optimizer/</id>
    <published>2020-10-31T03:51:00.000Z</published>
    <updated>2020-10-31T05:22:40.652Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.GradientDescentOptimizer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.AdadeltaOptimizer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.AdagradOptimizer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.AdagradDAOptimizer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.MomentumOptimizer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.AdamOptimizer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.FtrlOptimizer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.ProximalGradientDescentOptimizer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.ProximalAdagradOptimizer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.RMSPropOptimizer</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{aligned}&\mathrm{J}(\mathrm{W}): \text { 代价函教 }\\&\nabla \mathrm{w} \mathrm{J}(\mathrm{W}): \text { 代价函数的梯度 }\\&\eta \text {：学习率 }\end{aligned}</script><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><script type="math/tex; mode=display">W=W-\eta \cdot \nabla w J\left(W ; x^{(i)} ; y^{(i)}\right)</script><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><script type="math/tex; mode=display">\begin{aligned}&\mathrm{Y}: \text { 动力，通常设置为0.9 }\\&v_{t}=y v_{t-1}+\eta \nabla_{w} j(W)\\&W=W-v_{t}\end{aligned}</script><p>当前权值的改变会受到上一次权值改变的影响，类似于小球向下滚动的时候带上了惯性。这样可以加快小球的向下的速度。</p><h2 id="NAG（Nesterov-accelerated-gradient）"><a href="#NAG（Nesterov-accelerated-gradient）" class="headerlink" title="NAG（Nesterov accelerated gradient）"></a>NAG（Nesterov accelerated gradient）</h2><script type="math/tex; mode=display">\begin{array}{l}v_{t}=\gamma v_{t-1}+\eta \nabla w J\left(W-\gamma v_{t-1}\right) \\W=W-v_{t}\end{array}</script><p>NAG在TF中跟Momentum合并在同一个函数tf.train.MomentumOptimizer中，可以通过参数配置启用。在Momentum中小球会盲目地跟从下坡的梯度，容易发生错误，所以我们需要一个更聪明的小球，这个小球提计算$W-\gamma v_{t-1}$可以表示小球下一个位置大概在哪里。从而我们可以提前计算下一个位置的梯度，然后使用到当前位置。</p><h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><script type="math/tex; mode=display">\begin{array}&&i：代表第i个分类\\&t：代表出现次数\\&\epsilon：的作用是避免分母为0，取值一般为1e-8\\&\eta ：取值一般为0.01 \\&\mathrm{g}_{\mathrm{t},\mathrm{i}}=\nabla \mathrm{w}^{\mathrm{J}}\left(\mathrm{W}_{\mathrm{i}}\right)\\&W_{t+1}=W_{t}-\frac{\eta}{\sqrt{\sum_{t^{\prime}=1}^{t}\left(g_{t^{\prime}, i}\right)^{2}+\epsilon}} \odot g_{t}\end{array}</script><p>它是基于SGD的一种算法，它的核心思想是对比较常见的数据给予它比较小的学习率去调整参数，对于比较罕见的数据给予它比较大的学习率去调整参数。它很适合应用于数据稀疏的数据集（比如一个图片数据集，有10000张狗的昭片，10000张猫的照片，只有100张大象的昭片）。</p><p>Adagrad主要的优势在于不需要人为的调节学习率，它可以自动调节。它的缺点在于，随着迭代次数的增多，学习率也会越来越低，最终会趋向于0。</p><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>RMS（Root Mean Square）是均方根的缩写。</p><script type="math/tex; mode=display">\begin{array}&&\gamma：动力，通常设置为0.9\\&n：取值一般为0.001\\&E[g^2]_t：表示前t次的梯度平方的平均值\\&\mathrm{g}_{\mathrm{t}}=\nabla \mathrm{w} \mathrm{J}(\mathrm{W}) \\&E\left[g^{2}\right]_{t}=y E\left[g^{2}\right]_{t-1}+(1-\gamma) g^{2} t \\&W_{t+1}=W_{t}-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} \odot g_{t}\end{array}</script><p>RMSprop借鉴了一些Adagrad的思想，不过这里RMSprop只用到了前$t-1$次梯度平方的平均值加上当前梯度的平方的和的开平方作为学习率的分母。这样RMSprop不会出现学习率越来越低的问题，而且也能自己调节学习率，并且可以有一个比较好的效果。</p><h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><script type="math/tex; mode=display">\begin{array}{l}&\mathrm{g}_{\mathrm{t}}=\nabla \mathrm{w}{\mathrm{J}}(\mathrm{W}) \\&\Delta \mathrm{W}_{\mathrm{t}}=-\frac{\mathrm{\eta}}{\sqrt{E\left[g^{2}\right]_{t}+\epsilon}} \odot g_{t} \\&\Delta \mathrm{W}_{\mathrm{t}}=-\frac{\mathrm{\eta}}{R M S[\mathrm{g}]_{t}} \odot g_{t} \\&\mathrm{W}_{\mathrm{t}+1}=\mathrm{W}_{\mathrm{t}}-\frac{\mathrm{RMS}[\Delta \mathrm{W}]_{t-1}}{\mathrm{RMS}[\mathrm{g}]_{t}}\end{array}</script><p>使用Adadelta我们甚至不需要设置一个默认学习率，在Adadelta不需要使用学习率也可以达到一个非常好的效果。</p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><script type="math/tex; mode=display">\begin{array}&&\beta_{1}: 一般取值 0.9 \\&\beta_{2}: 一般取值0.999 \\&\varepsilon: 避免分母为 0, 一般取值 10^{-8} \\ &\mathrm{m}_{\mathrm{t}}=\beta_{1}  \mathrm{m}_{\mathrm{t}-1}+\left(1-\beta_{1}\right) \mathrm{g}_{\mathrm{t}} \\&v_{t}=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g^{2}_ t \\&\hat{\mathrm{m}}_{\mathrm{t}}=\frac{\mathrm{m}_{\mathrm{t}}}{1-\beta_{1}^{\mathrm{t}}} \\&\hat{v}_{\mathrm{t}}=\frac{v_{\mathrm{t}}}{1-\beta_{2}^{\mathrm{t}}} \\&\mathrm{W}_{\mathrm{t}+1}=\mathrm{W}_{\mathrm{t}}-\frac{\eta}{\sqrt{\hat{v}_{t}}+\varepsilon} \hat{m}_{t}\end{array}</script><p>就像Adadelta和RMSprop一样Adam会存储之前衰减的平方梯度，同时它也会保存之前衰减的梯度。经过一些处理之后再使用类似Adadelta和RMSprop的方式更新参数。</p><h1 id="各种优化器对比"><a href="#各种优化器对比" class="headerlink" title="各种优化器对比"></a>各种优化器对比</h1><h2 id="标准梯度下降法"><a href="#标准梯度下降法" class="headerlink" title="标准梯度下降法"></a>标准梯度下降法</h2><p>标准梯度下降先计算所有样本汇总误差，然后根据总误差来更新权值。</p><p>缺点：大样本时每次更新取值都要很长时间。</p><h2 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h2><p>随机梯度下降随机抽取一个样本来计算误差，然后更新权值。</p><p>缺点：取值更新快，但是不一定都是往正确的方向更新，会产生比较多的噪点。</p><h2 id="批量梯度下降法（常用）"><a href="#批量梯度下降法（常用）" class="headerlink" title="批量梯度下降法（常用）"></a>批量梯度下降法（常用）</h2><p>批量梯度下降算是一种折中的方案，从总样本中选取一个批次（比如一共有10000个样本，随机选取100个样本作为一个batch），然后计算这个batch的总误差，根据总误差来更新权值。</p>]]></content>
    
    
    <summary type="html">1、各种优化器的对比。</summary>
    
    
    
    <category term="Deeplearning" scheme="http://blog.too.ink/categories/Deeplearning/"/>
    
    
    <category term="Tensorflow" scheme="http://blog.too.ink/tags/Tensorflow/"/>
    
    <category term="Deeplearning" scheme="http://blog.too.ink/tags/Deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>weekly-report-20201030</title>
    <link href="http://blog.too.ink/2020/10/30/weekly-report-20201030/"/>
    <id>http://blog.too.ink/2020/10/30/weekly-report-20201030/</id>
    <published>2020-10-30T09:08:08.000Z</published>
    <updated>2020-10-30T09:56:50.532Z</updated>
    
    <content type="html"><![CDATA[<h1 id="本周学习报告"><a href="#本周学习报告" class="headerlink" title="本周学习报告"></a>本周学习报告</h1><ol><li>这一周主要学习了吴恩达的深度学习课程，他的课程主要是讲了深度学习的一些基本原理和概念。下边的目录是我这一周所看的内容。</li><li>另外本周还在Tensorflow上做了一个mnist手写数字的识别练习，代码和网络参数都在后文。</li></ol><h1 id="DeepLearning"><a href="#DeepLearning" class="headerlink" title="DeepLearning"></a>DeepLearning</h1><h2 id="吴承恩的深度学习课程"><a href="#吴承恩的深度学习课程" class="headerlink" title="吴承恩的深度学习课程"></a>吴承恩的深度学习课程</h2><h3 id="第一门课-神经网络和深度学习-Neural-Networks-and-Deep-Learning"><a href="#第一门课-神经网络和深度学习-Neural-Networks-and-Deep-Learning" class="headerlink" title="第一门课 神经网络和深度学习(Neural Networks and Deep Learning)"></a>第一门课 神经网络和深度学习(Neural Networks and Deep Learning)</h3><p><a href="http://www.ai-start.com/dl2017/html/lesson1-week1.html">第一周：深度学习引言(Introduction to Deep Learning)</a></p><p>1.1 欢迎(Welcome) 1</p><p>1.2 什么是神经网络？(What is a Neural Network)</p><p>1.3 神经网络的监督学习(Supervised Learning with Neural Networks)</p><p>1.4 为什么神经网络会流行？(Why is Deep Learning taking off?)</p><p><a href="http://www.ai-start.com/dl2017/html/lesson1-week2.html">第二周：神经网络的编程基础(Basics of Neural Network programming)</a></p><p>2.1 二分类(Binary Classification)</p><p>2.2 逻辑回归(Logistic Regression)</p><p>2.3 逻辑回归的代价函数（Logistic Regression Cost Function）</p><p>2.4 梯度下降（Gradient Descent）</p><p>2.5 导数（Derivatives）</p><p>2.6 更多的导数例子（More Derivative Examples）</p><p>2.7 计算图（Computation Graph）</p><p>2.8 计算图导数（Derivatives with a Computation Graph）</p><p>2.9 逻辑回归的梯度下降（Logistic Regression Gradient Descent）</p><p>2.10 梯度下降的例子(Gradient Descent on m Examples)</p><p>2.11 向量化(Vectorization)</p><p>2.12 更多的向量化例子（More Examples of Vectorization）</p><p>2.13 向量化逻辑回归(Vectorizing Logistic Regression)</p><p>2.14 向量化逻辑回归的梯度计算（Vectorizing Logistic Regression’s Gradient）</p><p>2.15 Python中的广播机制（Broadcasting in Python）</p><p>2.16 关于 Python与numpy向量的使用（A note on python or numpy vectors）</p><p>2.17 Jupyter/iPython Notebooks快速入门（Quick tour of Jupyter/iPython Notebooks）</p><p>2.18 逻辑回归损失函数详解（Explanation of logistic regression cost function）</p><p><a href="http://www.ai-start.com/dl2017/html/lesson1-week3.html">第三周：浅层神经网络(Shallow neural networks)</a></p><p>3.1 神经网络概述（Neural Network Overview）</p><p>3.2 神经网络的表示（Neural Network Representation）</p><p>3.3 计算一个神经网络的输出（Computing a Neural Network’s output）</p><p>3.4 多样本向量化（Vectorizing across multiple examples）</p><p>3.5 向量化实现的解释（Justification for vectorized implementation）</p><p>3.6 激活函数（Activation functions）</p><p>3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）</p><p>3.8 激活函数的导数（Derivatives of activation functions）</p><p>3.9 神经网络的梯度下降（Gradient descent for neural networks）</p><p>3.10（选修）直观理解反向传播（Backpropagation intuition）</p><p>3.11 随机初始化（Random+Initialization）</p><p><a href="http://www.ai-start.com/dl2017/html/lesson1-week4.html">第四周：深层神经网络(Deep Neural Networks)</a></p><p>4.1 深层神经网络（Deep L-layer neural network）</p><p>4.2 前向传播和反向传播（Forward and backward propagation）</p><p>4.3 深层网络中的前向和反向传播（Forward propagation in a Deep Network）</p><p>4.4 核对矩阵的维数（Getting your matrix dimensions right）</p><p>4.5 为什么使用深层表示？（Why deep representations?）</p><p>4.6 搭建神经网络块（Building blocks of deep neural networks）</p><p>4.7 参数VS超参数（Parameters vs Hyperparameters）</p><p>4.8 深度学习和大脑的关联性（What does this have to do with the brain?）</p><h3 id="第二门课-改善深层神经网络：超参数调试、正则化以及优化-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization"><a href="#第二门课-改善深层神经网络：超参数调试、正则化以及优化-Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization" class="headerlink" title="第二门课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)"></a>第二门课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)</h3><p><a href="http://www.ai-start.com/dl2017/html/lesson2-week1.html">第一周：深度学习的实用层面(Practical aspects of Deep Learning)</a></p><p>1.1 训练，验证，测试集（Train / Dev / Test sets）</p><p>1.2 偏差，方差（Bias /Variance）</p><p>1.3 机器学习基础（Basic Recipe for Machine Learning）</p><p>1.4 正则化（Regularization）</p><p>1.5 为什么正则化有利于预防过拟合呢？（Why regularization reduces overfitting?）</p><p>1.6 dropout 正则化（Dropout Regularization）</p><p>1.7 理解 dropout（Understanding Dropout）</p><p>1.8 其他正则化方法（Other regularization methods）</p><p>1.9 标准化输入（Normalizing inputs）</p><p>1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）</p><p>1.11 神经网络的权重初始化（Weight Initialization for Deep NetworksVanishing /Exploding gradients）</p><p>1.12 梯度的数值逼近（Numerical approximation of gradients）</p><p>1.13 梯度检验（Gradient checking）</p><p>1.14 梯度检验应用的注意事项（Gradient Checking Implementation Notes）</p><p><a href="http://www.ai-start.com/dl2017/html/lesson2-week2.html">第二周：优化算法 (Optimization algorithms)</a></p><p>2.1 Mini-batch 梯度下降（Mini-batch gradient descent）</p><p>2.2 理解Mini-batch 梯度下降（Understanding Mini-batch gradient descent）</p><p>2.3 指数加权平均（Exponentially weighted averages）</p><p>2.4 理解指数加权平均（Understanding Exponentially weighted averages）</p><p>2.5 指数加权平均的偏差修正（Bias correction in exponentially weighted averages）</p><p>2.6 momentum梯度下降（Gradient descent with momentum）</p><p>2.7 RMSprop——root mean square prop（RMSprop）</p><p>2.8 Adam优化算法（Adam optimization algorithm）</p><p>2.9 学习率衰减（Learning rate decay）</p><p>2.10 局部最优问题（The problem of local optima）</p><p><a href="http://www.ai-start.com/dl2017/html/lesson2-week3.html">第三周超参数调试，batch正则化和程序框架（Hyperparameter tuning, Batch Normalization and Programming Frameworks)</a></p><p>3.1 调试处理（Tuning process）</p><p>3.2 为超参数选择和适合范围（Using an appropriate scale to pick hyperparameters）</p><p>3.3 超参数训练的实践：Pandas vs. Caviar（Hyperparameters tuning in practice: Pandas vs. Caviar）</p><p>3.4 网络中的正则化激活函数（Normalizing activations in a network）</p><p>3.5 将 Batch Norm拟合进神经网络（Fitting Batch Norm into a neural network）</p><p>3.6 为什么Batch Norm奏效？（Why does Batch Norm work?）</p><p>3.7 测试时的Batch Norm（Batch Norm at test time）</p><p>3.8 Softmax 回归（Softmax Regression）</p><p>3.9 训练一个Softmax 分类器（Training a softmax classifier）</p><p>3.10 深度学习框架（Deep learning frameworks）</p><p>3.11 TensorFlow（TensorFlow）</p><h2 id="Tensorflow实战"><a href="#Tensorflow实战" class="headerlink" title="Tensorflow实战"></a>Tensorflow实战</h2><h3 id="非线性回归模型"><a href="#非线性回归模型" class="headerlink" title="非线性回归模型"></a>非线性回归模型</h3><h3 id="mnist手写数字识别"><a href="#mnist手写数字识别" class="headerlink" title="mnist手写数字识别"></a>mnist手写数字识别</h3><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>隐藏层数：                3</p><p>隐藏层大小：            200 200 200（3个隐藏层的神经元数目都为200）</p><p>droup：                    0.7 0.7 0.7（3个隐藏层的dropout参数均为0.7）</p><p>隐藏层激活函数：    tanh tanh tanh（隐藏层均用tanh作为激活函数）</p><p>输出层大小：            10</p><p>输出层激活函数：    softmax</p><p>代价函数：                交叉熵代价函数</p><p>梯度下降法：            mini_batch梯度下降法</p><p>batch_size：            64</p><p>实现结果：在99次迭代训练后，在训练集上识别精度为0.9885，在测试集上识别精度为0.9775</p><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist &#x3D; input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot&#x3D;True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size &#x3D; 64</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch &#x3D; mnist.train.num_examples &#x2F;&#x2F; batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义placeholder，x为输入，y为label</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32,[None,10])</span><br><span class="line"></span><br><span class="line">#定义占位符，dropout的比例，神经元工作的比例</span><br><span class="line">keep_prob &#x3D; tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">#神经网络参数</span><br><span class="line">L1_size &#x3D; 200</span><br><span class="line">L2_size &#x3D; 200</span><br><span class="line">L3_size &#x3D; 200</span><br><span class="line"></span><br><span class="line">#定义神经网络</span><br><span class="line">W1 &#x3D; tf.Variable(tf.truncated_normal([784,L1_size],stddev &#x3D; 0.1))</span><br><span class="line">b1 &#x3D; tf.Variable(tf.zeros([1,L1_size]) + 0.01)</span><br><span class="line">z1 &#x3D; tf.matmul(x,W1) + b1</span><br><span class="line">a1 &#x3D; tf.nn.tanh(z1)</span><br><span class="line">L1_drop &#x3D; tf.nn.dropout(a1,keep_prob)</span><br><span class="line"></span><br><span class="line">W2 &#x3D; tf.Variable(tf.truncated_normal([L1_size,L2_size],stddev&#x3D;0.1))</span><br><span class="line">b2 &#x3D; tf.Variable(tf.zeros([1,L2_size]) + 0.01)</span><br><span class="line">z2 &#x3D; tf.matmul(L1_drop,W2) + b2</span><br><span class="line">a2 &#x3D; tf.nn.tanh(z2)</span><br><span class="line">L2_drop &#x3D; tf.nn.dropout(a2,keep_prob)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">W3 &#x3D; tf.Variable(tf.truncated_normal([L2_size,L3_size],stddev&#x3D;0.1))</span><br><span class="line">b3 &#x3D; tf.Variable(tf.zeros([1,L3_size]) + 0.01)</span><br><span class="line">z3 &#x3D; tf.matmul(L2_drop,W3) + b3</span><br><span class="line">a3 &#x3D; tf.nn.tanh(z3)</span><br><span class="line">L3_drop &#x3D; tf.nn.dropout(a3,keep_prob)</span><br><span class="line"></span><br><span class="line">Wout &#x3D; tf.Variable(tf.truncated_normal([L3_size,10],stddev&#x3D;0.1))</span><br><span class="line">bout &#x3D; tf.Variable(tf.zeros([1,10]) + 0.01)</span><br><span class="line">zout &#x3D; tf.matmul(L3_drop,Wout) + bout</span><br><span class="line">prediction &#x3D; tf.nn.softmax(zout)</span><br><span class="line"></span><br><span class="line">#代价函数</span><br><span class="line">loss &#x3D; tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels &#x3D; y,logits &#x3D; prediction))</span><br><span class="line"></span><br><span class="line">#梯度下降法的优化器</span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#变量初试化</span><br><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#求准确度</span><br><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    </span><br><span class="line">    for epoch in range(100):</span><br><span class="line">        for batch in range(n_batch):</span><br><span class="line">            batch_xs,batch_ys &#x3D; mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict&#x3D;&#123;x:batch_xs,y:batch_ys,keep_prob:0.7&#125;)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"> </span><br><span class="line">            </span><br><span class="line">        test_acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0&#125;)</span><br><span class="line">        train_acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0&#125;)</span><br><span class="line">        print(&quot;Iter&quot; + str(epoch) + &quot;,Testing Accuracy:&quot; + str(test_acc) + &quot; |  Trainung Accuracy:&quot; + str(train_acc))</span><br></pre></td></tr></table></figure><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><p>本周看论文的任务没有完成。</p><h1 id="Verilog"><a href="#Verilog" class="headerlink" title="Verilog"></a>Verilog</h1><p>仿真常用verilog example的任务没有完成。</p>]]></content>
    
    
    <summary type="html">2020年10月24日-2020年10月30日学习笔记。</summary>
    
    
    
    <category term="Study-Report" scheme="http://blog.too.ink/categories/Study-Report/"/>
    
    
    <category term="study-report" scheme="http://blog.too.ink/tags/study-report/"/>
    
  </entry>
  
  <entry>
    <title>过拟合</title>
    <link href="http://blog.too.ink/2020/10/30/deeplearning-over-fitting/"/>
    <id>http://blog.too.ink/2020/10/30/deeplearning-over-fitting/</id>
    <published>2020-10-30T07:35:24.000Z</published>
    <updated>2020-10-30T07:38:54.655Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是过拟合？"><a href="#什么是过拟合？" class="headerlink" title="什么是过拟合？"></a>什么是过拟合？</h1><h1 id="通过偏差和方差来判断过拟合-欠拟合"><a href="#通过偏差和方差来判断过拟合-欠拟合" class="headerlink" title="通过偏差和方差来判断过拟合/欠拟合"></a>通过偏差和方差来判断过拟合/欠拟合</h1><h1 id="过拟合解决方案"><a href="#过拟合解决方案" class="headerlink" title="过拟合解决方案"></a>过拟合解决方案</h1><h2 id="增加数据集"><a href="#增加数据集" class="headerlink" title="增加数据集"></a>增加数据集</h2><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2>]]></content>
    
    
    <summary type="html">过拟合解决方案。</summary>
    
    
    
    <category term="Deeplearning" scheme="http://blog.too.ink/categories/Deeplearning/"/>
    
    
  </entry>
  
  <entry>
    <title>代价函数</title>
    <link href="http://blog.too.ink/2020/10/30/deeplearning-cost-function/"/>
    <id>http://blog.too.ink/2020/10/30/deeplearning-cost-function/</id>
    <published>2020-10-30T07:07:39.000Z</published>
    <updated>2020-10-30T07:30:35.415Z</updated>
    
    <content type="html"><![CDATA[<h1 id="交叉熵代价函数"><a href="#交叉熵代价函数" class="headerlink" title="交叉熵代价函数"></a>交叉熵代价函数</h1><h2 id="交叉熵代价函数定义"><a href="#交叉熵代价函数定义" class="headerlink" title="交叉熵代价函数定义"></a>交叉熵代价函数定义</h2><script type="math/tex; mode=display">C=-\frac{1}{n} \sum_{x}[y \ln a+(1-y) \ln (1-a)]</script><p>有</p><script type="math/tex; mode=display">\begin{array}{l}a=\sigma(z), \quad z=\sum W_{j}^{\star} X_{j}+b \\\sigma^{\prime}(z)=\sigma(z)(1-\sigma(z))\end{array}</script><h2 id="w的导数推导"><a href="#w的导数推导" class="headerlink" title="w的导数推导"></a>w的导数推导</h2><script type="math/tex; mode=display">\begin{aligned}\frac{\partial C}{\partial w_{j}} &=-\frac{1}{n} \sum_{x}\left(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)}\right) \frac{\partial \sigma}{\partial w_{j}} \\&=-\frac{1}{n} \sum_{x}\left(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)}\right) \sigma^{\prime}(z) x_{j} \\&=\frac{1}{n} \sum_{x} \frac{\sigma^{\prime}(z) x_{j}}{\sigma(z)(1-\sigma(z))}(\sigma(z)-y) \\&=\frac{1}{n} \sum_{x} x_{j}(\sigma(z)-y)\end{aligned}</script><h2 id="b的导数推导"><a href="#b的导数推导" class="headerlink" title="b的导数推导"></a>b的导数推导</h2><script type="math/tex; mode=display">\begin{aligned}\frac{\partial C}{\partial b_{j}} &=-\frac{1}{n} \sum_{x}\left(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)}\right) \frac{\partial \sigma}{\partial b_{j}} \\&=-\frac{1}{n} \sum_{x}\left(\frac{y}{\sigma(z)}-\frac{(1-y)}{1-\sigma(z)}\right) \sigma^{\prime}(z)  \\&=\frac{1}{n} \sum_{x} \frac{\sigma^{\prime}(z) }{\sigma(z)(1-\sigma(z))}(\sigma(z)-y) \\&=\frac{1}{n} \sum_{x} (\sigma(z)-y)\end{aligned}</script><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><script type="math/tex; mode=display">\frac{\partial C}{\partial w_{j}}=\frac{1}{n} \sum_{x} x_{j}(\sigma(z)-y)</script><script type="math/tex; mode=display">\frac{\partial C}{\partial b}=\frac{1}{n} \sum_{x}(\sigma(z)-y)</script><ul><li>权值和偏置值的调整与 $\sigma^{\prime}(z) $ 无关, 另外，梯度公式中的 $\sigma(z)-y$ 表示输出值与实际值的误差。所以当误差越大时，梯度就越大，参数w和b的调整就越快，训练的速度也就越快。</li><li>如果输出神经元是线性的，那么二次代价函数就是一种合适的选择。如果输出神经元是S型函数，<br>那么比较适合用交叉熵代价函数。</li><li>对数似然函数常用来作为softmax回归的代价函数，如果输出层神经元是sigmoid函数，可以采用<br>交叉熵代价函数。而深度学习中更普遍的做法是将softmax作为最后一层，此时常用的代价函数是<br>对数似然代价函数。</li><li>对数似然代价函数与softmax的组合和交叉嫡与sigmoid函数的组合非常相似。对数释然代价函数<br>在二分类时可以化简为交叉熵代价函数的形式。</li></ul><h2 id="在Tensorflow中使用"><a href="#在Tensorflow中使用" class="headerlink" title="在Tensorflow中使用:"></a>在Tensorflow中使用:</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits()  </span><br><span class="line">tf.nn.sparse_softmax_cross_entropy_with_logits()   </span><br><span class="line">tf.nn.sigmoid_cross_entropy_with_logits()  </span><br><span class="line">tf.nn.weighted_cross_entropy_with_logits()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits(_sentinel&#x3D;None, labels&#x3D;None, logits&#x3D;None, dim&#x3D;-1, name&#x3D;None)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.sparse_softmax_cross_entropy_with_logits(_sentinel&#x3D;None,labels&#x3D;None,logits&#x3D;None, name&#x3D;None)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.sigmoid_cross_entropy_with_logits(_sentinel&#x3D;None,labels&#x3D;None, logits&#x3D;None, name&#x3D;None)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.weighted_cross_entropy_with_logits(labels,logits, pos_weight, name&#x3D;None)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">1、二次代价函数。		2、交叉熵代价函数。</summary>
    
    
    
    <category term="Deeplearning" scheme="http://blog.too.ink/categories/Deeplearning/"/>
    
    
    <category term="Tensorflow" scheme="http://blog.too.ink/tags/Tensorflow/"/>
    
    <category term="Deeplearning" scheme="http://blog.too.ink/tags/Deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>Tesorflow实战：简单mnist分类任务</title>
    <link href="http://blog.too.ink/2020/10/30/tensorflow-coding-mnist-simple-classfication/"/>
    <id>http://blog.too.ink/2020/10/30/tensorflow-coding-mnist-simple-classfication/</id>
    <published>2020-10-30T02:44:37.000Z</published>
    <updated>2020-10-30T06:29:33.796Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Original-Codes"><a href="#Original-Codes" class="headerlink" title="Original Codes"></a>Original Codes</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">#载入数据集</span><br><span class="line">mnist &#x3D; input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot&#x3D;True)</span><br><span class="line"></span><br><span class="line">#每个批次的大小</span><br><span class="line">batch_size &#x3D; 100</span><br><span class="line">#计算一共有多少个批次</span><br><span class="line">n_batch &#x3D; mnist.train.num_examples &#x2F;&#x2F; batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义两个placeholder</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,784])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32,[None,10])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#创建一个简单的神经网络</span><br><span class="line">w &#x3D; tf.Variable(tf.zeros([784,10]))</span><br><span class="line">b &#x3D; tf.Variable(tf.zeros([10]))</span><br><span class="line">prediction &#x3D; tf.nn.softmax(tf.matmul(x,w) + b)</span><br><span class="line"></span><br><span class="line">#二次代价函数</span><br><span class="line">loss &#x3D; tf.reduce_mean(tf.square(y - prediction))</span><br><span class="line"></span><br><span class="line">#使用梯度下降法</span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span><br><span class="line"></span><br><span class="line">#初始化变量</span><br><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">#定义求准确率的方法,结果存在一个布尔型列表中</span><br><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))</span><br><span class="line">#求准确率</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    for epoch in range(21):</span><br><span class="line">        for batch in range(n_batch):</span><br><span class="line">            batch_xs,batch_ys &#x3D; mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict&#x3D;&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">            </span><br><span class="line">        acc &#x3D; sess.run(accuracy,feed_dict&#x3D;&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line">        print(&quot;Iter&quot; + str(epoch) + &quot;,Testing Accuracy&quot; + str(acc))</span><br></pre></td></tr></table></figure><p>执行结果：20次迭代后，准确率为0.9138</p><h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><h2 id="Note1-tf-argmax"><a href="#Note1-tf-argmax" class="headerlink" title="Note1    tf.argmax()"></a>Note1    tf.argmax()</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.argmax(input,axis)</span><br></pre></td></tr></table></figure><p>含义：根据axis取值的不同返回每行或者每列最大值的索引。</p><div class="table-container"><table><thead><tr><th>属性</th><th>含义</th></tr></thead><tbody><tr><td>input</td><td>输入的array</td></tr><tr><td>axis</td><td>axis=0，将每一列最大元素的所在索引记录下来，最后输出每一列最大元素所在的索引数组。axis=1，将每一行最大元素所在的索引记录下来，最后返回每一行最大元素所在的索引数组。</td></tr></tbody></table></div><h2 id="Note2-tf-cast"><a href="#Note2-tf-cast" class="headerlink" title="Note2    tf.cast()"></a>Note2    tf.cast()</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.cast(x, dtype, name&#x3D;None)</span><br></pre></td></tr></table></figure><p>含义：tf.cast()函数的作用是执行 tensorflow 中张量数据类型转换。</p><div class="table-container"><table><thead><tr><th>属性</th><th>含义</th></tr></thead><tbody><tr><td>x</td><td>待转换的数据（张量）。</td></tr><tr><td>dtype</td><td>目标数据类型。</td></tr><tr><td>name</td><td>可选参数，定义操作的名称。</td></tr></tbody></table></div><p>Tensorflow中的数据类型列表：</p><div class="table-container"><table><thead><tr><th>数据类型</th><th>Python类型</th><th>描述</th></tr></thead><tbody><tr><td>DT_FLOAT</td><td>tf.float32</td><td>32位浮点数</td></tr><tr><td>DT_DOUBLE</td><td>tf.float64</td><td>64位浮点数</td></tr><tr><td>DT_INT64</td><td>tf.int64</td><td>64位有符号整型</td></tr><tr><td>DT_INT32</td><td>tf.int32</td><td>32位有符号整型</td></tr><tr><td>DT_INT16</td><td>tf.int16</td><td>16位有符号整型</td></tr><tr><td>DT_INT8</td><td>tf.int8</td><td>8位有符号整型</td></tr><tr><td>DT_UINT8</td><td>tf.uint8</td><td>8位无符号整型</td></tr><tr><td>DT_STRING</td><td>tf.string</td><td>可变长度的字节数组.每一个张量元素都是一个字节数组.</td></tr><tr><td>DT_BOOL</td><td>tf.bool</td><td>布尔型</td></tr><tr><td>DT_COMPLEX64</td><td>tf.complex64</td><td>由两个32位浮点数组成的复数:实数和虚数.</td></tr><tr><td>DT_QINT32</td><td>tf.qint32</td><td>用于量化Ops的32位有符号整型.</td></tr><tr><td>DT_QINT8</td><td>tf.qint8</td><td>用于量化Ops的8位有符号整型.</td></tr><tr><td>DT_QUINT8</td><td>tf.quint8</td><td>用于量化Ops的8位无符号整型.</td></tr></tbody></table></div><h1 id="超参数调整"><a href="#超参数调整" class="headerlink" title="超参数调整"></a>超参数调整</h1><h2 id="只有输入层和输出层"><a href="#只有输入层和输出层" class="headerlink" title="只有输入层和输出层"></a>只有输入层和输出层</h2><div class="table-container"><table><thead><tr><th>batch_size</th><th>learning_rate</th><th>accuracy</th></tr></thead><tbody><tr><td>32</td><td>0.2</td><td>0.9236</td></tr><tr><td>64</td><td>0.2</td><td>0.9185</td></tr><tr><td>128</td><td>0.2</td><td>0.9107</td></tr><tr><td>16</td><td>0.2</td><td>0.9272</td></tr><tr><td>8</td><td>0.2</td><td>0.9310</td></tr><tr><td>4</td><td>0.2</td><td>0.9308</td></tr><tr><td>8</td><td>0.1</td><td>0.9301</td></tr></tbody></table></div><h2 id="增加隐藏层"><a href="#增加隐藏层" class="headerlink" title="增加隐藏层"></a>增加隐藏层</h2><div class="table-container"><table><thead><tr><th>隐藏层数</th><th>隐藏层神经元数</th><th>激活函数</th><th>batch_size</th><th>w</th><th>learning_rate</th><th>accuracy</th></tr></thead><tbody><tr><td>1</td><td>10</td><td>tanh</td><td>64</td><td>0</td><td>0.3</td><td>0.937</td></tr><tr><td>1</td><td>20</td><td>tanh</td><td>64</td><td>0</td><td>0.3</td><td>0.853</td></tr><tr><td>1</td><td>30</td><td>tanh</td><td>64</td><td>0</td><td>0.3</td><td>0.9638</td></tr><tr><td>1</td><td>30</td><td>tanh</td><td>32</td><td>0</td><td>0.3</td><td>0.961</td></tr><tr><td>1</td><td>30</td><td>tanh</td><td>48</td><td>0</td><td>0.3</td><td>0.962</td></tr><tr><td>1</td><td>30</td><td>tanh</td><td>128</td><td>0</td><td>0.3</td><td>0.962</td></tr><tr><td>1</td><td>40</td><td>tanh</td><td>64</td><td>0</td><td>0.3</td><td>0.967</td></tr><tr><td>1</td><td>50</td><td>tanh</td><td>64</td><td>0</td><td>0.3</td><td>0.798</td></tr><tr><td>1</td><td>32</td><td>tanh</td><td>64</td><td>0</td><td>0.3</td><td>0.962</td></tr><tr><td>2</td><td>10 4</td><td>tanh tanh</td><td>64</td><td>0</td><td>0.3</td><td>0.868</td></tr><tr><td>2</td><td>20 10</td><td>tanh tanh</td><td>64</td><td>0</td><td>0.6</td><td>0.905</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><p><strong>Q1:weight初试化为0，网络由于对称性实际上可以再压缩</strong></p>]]></content>
    
    
    <summary type="html">Tesorflow实战：简单mnist分类任务。</summary>
    
    
    
    <category term="Tensorflow-Coding-Notes" scheme="http://blog.too.ink/categories/Tensorflow-Coding-Notes/"/>
    
    
    <category term="Tensorflow" scheme="http://blog.too.ink/tags/Tensorflow/"/>
    
    <category term="Deeplearning" scheme="http://blog.too.ink/tags/Deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow问题合集</title>
    <link href="http://blog.too.ink/2020/10/29/tensorflow-coding-questions/"/>
    <id>http://blog.too.ink/2020/10/29/tensorflow-coding-questions/</id>
    <published>2020-10-29T11:46:21.000Z</published>
    <updated>2020-11-02T01:48:57.557Z</updated>
    
    <content type="html"><![CDATA[<h6 id="Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。"><a href="#Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。" class="headerlink" title="Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。"></a>Q1、在不同的超参数情况下，损失函数和拟合结果有很大差别，为什么会出现损失函数为直线，参数不更新的情况。</h6><p><img src="/.ink//image-20201029195107148.png" alt="image-20201029195107148" style="zoom:50%;"></p><p><img src="/.ink//image-20201029195649017.png" alt="image-20201029195649017" style="zoom:50%;"></p><h6 id="A1："><a href="#A1：" class="headerlink" title="A1："></a>A1：</h6><h6 id="Q2：随着迭代次数的增加，准确率在下降（来源于Tensorflow实战：mnist简单分类任务）"><a href="#Q2：随着迭代次数的增加，准确率在下降（来源于Tensorflow实战：mnist简单分类任务）" class="headerlink" title="Q2：随着迭代次数的增加，准确率在下降（来源于Tensorflow实战：mnist简单分类任务）"></a>Q2：随着迭代次数的增加，准确率在下降（来源于Tensorflow实战：mnist简单分类任务）</h6><p><img src="/.ink//image-20201030115021987.png" alt="image-20201030115021987" style="zoom:50%;"></p><p>参数：     batch_size = 8    learning_rate=0.2 w=0 b=0</p>]]></content>
    
    
    <summary type="html">Tensorflow 问题合集。</summary>
    
    
    
    <category term="Tensorflow" scheme="http://blog.too.ink/categories/Tensorflow/"/>
    
    
    <category term="Tensorflow" scheme="http://blog.too.ink/tags/Tensorflow/"/>
    
    <category term="Deeplearning" scheme="http://blog.too.ink/tags/Deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow实战：非线性逻辑回归</title>
    <link href="http://blog.too.ink/2020/10/28/tensorflow-coding-nonlinear-logic-regression/"/>
    <id>http://blog.too.ink/2020/10/28/tensorflow-coding-nonlinear-logic-regression/</id>
    <published>2020-10-28T11:36:50.000Z</published>
    <updated>2020-10-30T03:24:15.608Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><h2 id="Note1-linspace-函数功能"><a href="#Note1-linspace-函数功能" class="headerlink" title="Note1    linspace()函数功能"></a>Note1    linspace()函数功能</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_data &#x3D; np.linspace(-0.5,0.5,10)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.linspace(start, stop, num&#x3D;50, endpoint&#x3D;True, retstep&#x3D;False, dtype&#x3D;None)</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>属性</th><th>说明</th></tr></thead><tbody><tr><td>start</td><td>队列的开始值</td></tr><tr><td>stop</td><td>队列的结束值</td></tr><tr><td>num</td><td>要生成的样本数，非负数，默认是50</td></tr><tr><td>endpoint</td><td>若为True，“stop”是最后的样本；否则“stop”将不会被包含。默认为True</td></tr><tr><td>retstop</td><td>若为False，返回等差数列；否则返回array([samples, step])。默认为False</td></tr></tbody></table></div><h2 id="Note2-newaxis-功能"><a href="#Note2-newaxis-功能" class="headerlink" title="Note2    [:,newaxis]功能"></a>Note2    [:,newaxis]功能</h2><p>执行代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_data &#x3D; np.linspace(-0.5,0.5,10)</span><br><span class="line">print(x_data.shape)</span><br><span class="line">x_data &#x3D; x_data[:,newaxis]</span><br><span class="line">print(x_data.shape)</span><br></pre></td></tr></table></figure><p>输出结果为</p><p>(10,)</p><p>(10,1)</p><p>x_data[:,newaxis]能将原来的数据扩充一个维度。</p><h2 id><a href="#" class="headerlink" title=" "></a> </h2><h2 id="Note3-正态分布生成函数"><a href="#Note3-正态分布生成函数" class="headerlink" title="Note3    正态分布生成函数"></a>Note3    正态分布生成函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.normal(loc&#x3D;0.0,scale&#x3D;1.0,size&#x3D;None)</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>属性</th><th>类型</th><th>说明</th></tr></thead><tbody><tr><td>loc</td><td>float</td><td>正态分布的均值</td></tr><tr><td>scale</td><td>float</td><td>正态分布的标准差，对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高</td></tr><tr><td>size</td><td>int or tuple of ints</td><td>输出的shape，默认为None，只输出一个值</td></tr></tbody></table></div><h2 id="Note4-y-np-square-x"><a href="#Note4-y-np-square-x" class="headerlink" title="Note4    y = np.square(x)"></a>Note4    y = np.square(x)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y &#x3D; np.square(x)</span><br></pre></td></tr></table></figure><p>返回的y与x有相同的shape，其中y的每个元素是x每个元素的平方。</p><h2 id="Note5-Tensorflow的占位符"><a href="#Note5-Tensorflow的占位符" class="headerlink" title="Note5 Tensorflow的占位符"></a>Note5 Tensorflow的占位符</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,1])</span><br></pre></td></tr></table></figure><p>定义了一个32位浮点型的占位符，shape为[None,1]，行数没有定义，列数定义为1</p><p>执行时使用feed_dict{}以字典的形式传入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(train_step,feed_dict&#x3D;&#123;x:x_data,y:y_data&#125;)</span><br></pre></td></tr></table></figure><p>前面定义了operation，我们运行的时候seess.run()最后一个operation就可以，前面的会自动执行（我的推断）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.placeholder(dtype,shape&#x3D;None,name&#x3D;None)</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>属性</th><th>说明</th></tr></thead><tbody><tr><td>dtype</td><td>数据类型。常用的是tf.float32,tf.float64等数值类型</td></tr><tr><td>shape</td><td>数据形状。默认是None，就是一维值，也可以是多维（比如[2,3], [None, 3]表示列是3，行不定）</td></tr><tr><td>name</td><td>名称</td></tr></tbody></table></div><h1 id="Codes"><a href="#Codes" class="headerlink" title="Codes"></a>Codes</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#使用numpy生成200个随机点</span><br><span class="line">x_data &#x3D; np.linspace(-0.5,0.5,200)[:,np.newaxis]</span><br><span class="line">noise &#x3D; np.random.normal(0,0.02,x_data.shape)</span><br><span class="line">y_data &#x3D; np.square(x_data) + noise</span><br><span class="line"></span><br><span class="line">#定义两个placeholder</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32,[None,1])</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32,[None,1])</span><br><span class="line"></span><br><span class="line">#定义神经网络中间层</span><br><span class="line">Weights_L1 &#x3D; tf.Variable(tf.random_normal([1,10]))</span><br><span class="line">biases_L1 &#x3D; tf.Variable(tf.zeros([1,10]))</span><br><span class="line">Wx_plus_b_L1 &#x3D; tf.matmul(x,Weights_L1) + biases_L1</span><br><span class="line">L1 &#x3D; tf.nn.tanh(Wx_plus_b_L1)</span><br><span class="line"></span><br><span class="line">#定义神经网络输出层</span><br><span class="line">Weights_L2 &#x3D; tf.Variable(tf.random_normal([10,1]))</span><br><span class="line">biases_L2 &#x3D; tf.Variable(tf.zeros([1,1]))</span><br><span class="line">Wx_plus_b_L2 &#x3D; tf.matmul(L1,Weights_L2) + biases_L2</span><br><span class="line">prediction &#x3D; tf.nn.tanh(Wx_plus_b_L2)</span><br><span class="line"></span><br><span class="line">#二次代价函数</span><br><span class="line">loss &#x3D; tf.reduce_mean(tf.square(y-prediction))</span><br><span class="line">#使用梯度下降法训练</span><br><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.1).minimize(loss)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    #变量初始化</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    for _ in range(2000):</span><br><span class="line">        sess.run(train_step,feed_dict&#x3D;&#123;x:x_data,y:y_data&#125;)</span><br><span class="line">        </span><br><span class="line">    #获得预测值</span><br><span class="line">    prediction_value &#x3D; sess.run(prediction,feed_dict&#x3D;&#123;x:x_data&#125;)</span><br><span class="line">    #画图</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(x_data,y_data)</span><br><span class="line">    plt.plot(x_data,prediction_value,&#39;r-&#39;,lw&#x3D;5)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Tensorflow实战：非线性逻辑回归</summary>
    
    
    
    <category term="Tensorflow-Coding-Notes" scheme="http://blog.too.ink/categories/Tensorflow-Coding-Notes/"/>
    
    
    <category term="Tensorflow" scheme="http://blog.too.ink/tags/Tensorflow/"/>
    
    <category term="Deeplearning" scheme="http://blog.too.ink/tags/Deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>关于激活函数前使用Batch Normalization的思考</title>
    <link href="http://blog.too.ink/2020/10/28/deeplearning-batch-normalization/"/>
    <id>http://blog.too.ink/2020/10/28/deeplearning-batch-normalization/</id>
    <published>2020-10-28T07:21:45.000Z</published>
    <updated>2020-10-28T08:36:53.343Z</updated>
    
    <content type="html"><![CDATA[<h1 id="没有BN时的网络流程"><a href="#没有BN时的网络流程" class="headerlink" title="没有BN时的网络流程"></a>没有BN时的网络流程</h1><p>在没有Batch Normalization时，$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$，$a^{[l]}=g^{[l]}(z^{[l]})$</p><p>注：$[l]$表示第$l$层网络的数据，$g^{[l]}(z)$为第$l$层的激活函数</p><p>注：Batch Normalization的目的是使参数搜索问题变得更容易，使神经网络对于超参数的选择更稳定</p><h1 id="在激活函数前Batch-Normalization"><a href="#在激活函数前Batch-Normalization" class="headerlink" title="在激活函数前Batch Normalization"></a>在激活函数前Batch Normalization</h1><p>①    $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$</p><p>②    $z_{BN}^{[l]}=\frac{z^{[l]}-u^{[l]}}{\sigma^{[l]}}=\frac{w^{[l]}a^{[l-1]}+b^{[l]}-u^{[l]}}{\sigma^{[l]}}$     则①可变为    $z^{[l]}=w^{[l]}a^{[l-1]}$，参数$b^{[l]}$失去了意义</p><p>③    由新加入的参数$\beta^{[l]}$和$\gamma^{[l]}$重新缩放    $\rightarrow$    $\overset{~}{z}^{[l]}=\beta^{[l]}z_{BN}^{[l]}+\gamma^{[l]}$</p><p>④    $\overset{~}{z}^{[l]}=\beta^{[l]}z_{BN}^{[l]}+\gamma^{[l]}=\beta^{[l]}\frac{z^{[l]}-u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}=\frac{\beta^{[l]}z^{[l]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}=\frac{\beta^{[l]}w^{[l]}a^{[l-1]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}$</p><h1 id="我的思考"><a href="#我的思考" class="headerlink" title="我的思考"></a>我的思考</h1><ol><li>​    $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$ 和 $z^{[l]}=\frac{\beta^{[l]}w^{[l]}a^{[l-1]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}$ 有多大的区别，后者是否同样是一个与前者一样的线性函数，如果是线性函数的话，这么做的意义何在？</li><li>​    可能的解释：$\sigma^{[l]}$ 和 $u^{[l]}$ 都是 $z^{[l]}$ 或者说 $a^{[l-1]}$ 的函数，并不能与①等同看成一个线性函数。</li><li>​    想法：$z^{[l]}=\frac{\beta^{[l]}w^{[l]}a^{[l-1]}}{\sigma^{[l]}}-\frac{\beta^{[l]}u^{[l]}}{\sigma^{[l]}}+\gamma^{[l]}$ 是否是一个参数更多的神经元，是否可以用更多的参数来构建次数更高的多项式然后来进行模拟？</li></ol>]]></content>
    
    
    <summary type="html">关于激活函数前使用Batch Normalization的思考，网络中激活函数前加入BN的公式推导以及自己的问题和想法。</summary>
    
    
    
    <category term="DeepLearning" scheme="http://blog.too.ink/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>2019刘晓燕救命班-comment</title>
    <link href="http://blog.too.ink/2020/10/25/postgraduate-exam-english/"/>
    <id>http://blog.too.ink/2020/10/25/postgraduate-exam-english/</id>
    <published>2020-10-25T00:03:10.000Z</published>
    <updated>2020-10-25T01:34:06.536Z</updated>
    
    <content type="html"><![CDATA[<center>`记笔记！！！然后学以致用！！！！`</center><p>（未完待续。。。2020/10/25 9:33）</p><h1 id="完型"><a href="#完型" class="headerlink" title="完型"></a>完型</h1><p>前面教你猜选项那个我觉得不靠谱，尽量还是自己做，但是最后真的考场上只剩一两分钟但是你还没做，那就大胆按照她的方法来蒙答案。</p><p>这个课我觉得还是值得看的，她里面讲到了按照文章的中心来做题，我觉得这个非常有用。</p><p>我以前看过王晟的课，他讲了逻辑关系词，我觉得那个对于完型、阅读、新题型都有好处，能够帮助理解文章（对于完型来说有利于解题，大概能够完型提升1-2分，对于阅读，文章理解好了提升也很大）</p><h1 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h1><p>翻译按照 <code>正确、通顺、完整</code> 进行，刘晓燕在她的课里面讲了“做一个勇敢的中国人”，她的意思是告诉你为了使翻译出来的中文要通顺，大胆对单词意思进行延伸。只要翻译的意思准确，信息传达完整，句子通顺就能得全分。（就算对单词逐个翻译，但是不通顺也不会得全分）。</p><p>另外刘晓燕还介绍了几种方法，我觉得都是能够比较快掌握又比较实用的。</p><h1 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h1><p>讲了一篇模板作文应该怎么写，也告诉你了如果你的英语能力好怎么做可以拿更高的分。在看视频的时候做好笔记，然后按照笔记来进行练习，学会写这个模板作文30分里面就已经可以稳稳拿到20分，对于自身的英语基础要求也不高，只需要会写简单句和几个句型就可以。</p><p>看完视频课后一定要自己写几篇，可以每天晚上写一篇然后持续一段时间。然后还有时间的话再看看其他的作文资料，没有时间的话这个已经足够了。考研英语作文要拿到25分我觉得需要一个比较好的英语基础，现在英语基础没那么好的话就把时间花在其他更容易提升的地方。</p><p>我英语76分，但是作文也是20分左右这一档（阅读+新题型得了大概40分，完型+翻译大概13分，完型翻译作文大家差距都不大，差距主要在阅读和新题型这里拉开，要考80分以上的话可能每个部分都得细致认真的复习）。</p><h1 id="阅读"><a href="#阅读" class="headerlink" title="阅读"></a>阅读</h1><p>思路比较重要。</p><p>一句话一句话的翻译一遍没有很大的作用，可能对翻译有点好处，但是对于阅读的提升并不大。阅读需要的基础积累是长期的，现在要找到正确的方法来利用已有的基础去更快更好的理解文章，然后更快更准确的做题。（应试比的不仅仅是基础）</p><h1 id="新题型"><a href="#新题型" class="headerlink" title="新题型"></a>新题型</h1><h1 id="Last-but-not-least"><a href="#Last-but-not-least" class="headerlink" title="Last but not least"></a>Last but not least</h1><p>英语这门课时间比较紧，经常会有同学在考场上做不完题，为了避免这种情况，需要在复习的时候就规划好每部分的做题时间，然后在每部分练习的时候就计时来做。</p><p>考试时间180分钟</p><p>完型-10分——10分钟</p><p>阅读-40分——70~80分钟</p><p>新题型-10分——20分钟</p><p>翻译-10分——20分钟</p><p>小作文-10分——10~20分钟</p><p>大作文-20分——30分钟</p><p>上边是各部分对应的分值和需要的大致时间，你应该按照自己的情况进行具体调整，在给自己设置计划的时候，把总时间当成170分钟来设置计划，留10分钟裕量确保考试的时候能够稳稳把题做完。</p><p>阅读大概每篇需要20分钟左右，但是每年真题阅读每篇文章的难度不一，有的只需要15分钟左右，有的需要可能20分钟多一点，我之前是每篇设置的20分钟，尤其是对于较难的文章，如果较难的文章都能在20分钟内完成好，那4篇文章70分钟应该不困难。</p><p>翻译自己先试一下翻译完5道题自己大概需要多长时间，做的时候不要拖拖拉拉死抠细解，因为要写的中文有点多，所以写字就得要很多时间。</p>]]></content>
    
    
    <summary type="html">我觉得的关于2019刘晓燕救命班的一些重点与非重点。</summary>
    
    
    
    <category term="考研" scheme="http://blog.too.ink/categories/%E8%80%83%E7%A0%94/"/>
    
    
  </entry>
  
  <entry>
    <title>如何新建一篇文章</title>
    <link href="http://blog.too.ink/2020/10/24/hexo-article-writing/"/>
    <id>http://blog.too.ink/2020/10/24/hexo-article-writing/</id>
    <published>2020-10-24T06:22:43.000Z</published>
    <updated>2020-10-24T06:49:10.132Z</updated>
    
    <content type="html"><![CDATA[<p>1.在博客文件夹目录下鼠标右键，点击Git Bash Here</p><p>2.输入代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new article</span><br></pre></td></tr></table></figure><p>回车新建文章，生成的.md文件在/source/_post/目录下</p><p>3.</p><p>清除之前的页面</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure><p>生成新的页面</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo generate</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure><p>部署新的页面</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo deploy</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>打开本地服务器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure><p>4.文章Front Matter</p><div class="table-container"><table><thead><tr><th>属性</th><th>含义</th></tr></thead><tbody><tr><td>title</td><td>文章的标题</td></tr><tr><td>date</td><td>文章的日期</td></tr><tr><td>categories</td><td>文章的所属类别，只有一个，填写多个时默认第一个</td></tr><tr><td>tags</td><td>标签，可以有多个</td></tr><tr><td>description</td><td>首页缩略显示内容</td></tr><tr><td>comments</td><td>是否支持评论</td></tr></tbody></table></div>]]></content>
    
    
    <summary type="html">新建文章的一些简单指令代码。</summary>
    
    
    
    <category term="Hexo" scheme="http://blog.too.ink/categories/Hexo/"/>
    
    
  </entry>
  
</feed>
